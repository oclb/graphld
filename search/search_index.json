{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"GraphLD","text":"<p>This repository implements the graphREML method described in:</p> <p>Hui Li, Tushar Kamath, Rahul Mazumder, Xihong Lin, &amp; Luke J. O'Connor (2024). Improved heritability partitioning and enrichment analyses using summary statistics with graphREML. medRxiv, 2024-11. DOI: 10.1101/2024.11.04.24316716</p> <p>and provides a Python API for computationally efficient linkage disequilibrium (LD) matrix operations with LD graphical models (LDGMs), described in:</p> <p>Pouria Salehi Nowbandegani, Anthony Wilder Wohns, Jenna L. Ballard, Eric S. Lander, Alex Bloemendal, Benjamin M. Neale, and Luke J. O'Connor (2023) Extremely sparse models of linkage disequilibrium in ancestrally diverse association studies. Nat Genet. DOI: 10.1038/s41588-023-01487-8</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Heritability Estimation: Run graphREML for heritability partitioning and enrichment analysis</li> <li>Enrichment Score Test: Fast score test for genomic annotations (no dependencies required)</li> <li>Matrix Operations: Efficient LD matrix operations using LDGM precision matrices</li> <li>Simulation: Simulate GWAS summary statistics from flexible mixture distributions</li> <li>LD Clumping: Fast LD-based pruning for polygenic score computation</li> <li>BLUP: Best Linear Unbiased Prediction for effect size estimation</li> </ul>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ul> <li>Installation: How to install GraphLD and its dependencies</li> <li>Command Line Interface: Using the <code>graphld</code> CLI</li> <li>Enrichment Score Test: Fast annotation enrichment testing</li> <li>API Reference: Python API documentation</li> <li>File Formats: Supported input/output formats</li> </ul>"},{"location":"REVIEW_NOTES/","title":"Code Review Notes for Release","text":"<p>This document tracks issues found during code review that require careful consideration before fixing. These are NOT simple linting fixes - they represent potential logic changes or architectural decisions.</p>"},{"location":"REVIEW_NOTES/#coverage-gaps","title":"Coverage Gaps","text":""},{"location":"REVIEW_NOTES/#modules-with-0-coverage-need-tests","title":"Modules with 0% Coverage (Need Tests)","text":"<ol> <li>graphld/genesets.py (88 statements) - Gene set annotation utilities</li> <li>graphld/genesets_gene_hdf5.py (207 statements) - Gene-level HDF5 operations  </li> <li>graphld/heritability_testing.py (509 statements) - Heritability testing module</li> </ol>"},{"location":"REVIEW_NOTES/#modules-with-low-coverage-50","title":"Modules with Low Coverage (&lt;50%)","text":"<ol> <li>graphld/cli.py (47%) - Command line interface; many branches untested</li> <li>score_test/convert_scores.py (38%) - Score conversion utilities</li> </ol>"},{"location":"REVIEW_NOTES/#potential-issues-to-review","title":"Potential Issues to Review","text":""},{"location":"REVIEW_NOTES/#graphld-package","title":"graphld package","text":""},{"location":"REVIEW_NOTES/#star-imports-heritabilitypy-heritability_testingpy","title":"Star Imports (heritability.py, heritability_testing.py)","text":"<ul> <li>Lines 8, 16, 17 in heritability.py: <code>from typing import *</code>, <code>from .io import *</code>, <code>from .likelihood import *</code></li> <li>This causes F403/F405 linting errors and makes the code harder to understand</li> <li>Decision needed: Replace with explicit imports (may require significant refactoring)</li> </ul>"},{"location":"REVIEW_NOTES/#undefined-name-references-iopy","title":"Undefined Name References (io.py)","text":"<ul> <li>Line 15: <code>PrecisionOperator</code> referenced before it's imported</li> <li>This is a forward reference issue that may cause runtime errors in some contexts</li> <li>Decision needed: Add proper TYPE_CHECKING import or restructure</li> </ul>"},{"location":"REVIEW_NOTES/#unused-variable-ldsc_iopy","title":"Unused Variable (ldsc_io.py)","text":"<ul> <li>Line 77: <code>columns_to_read</code> is assigned but never used</li> <li>Decision needed: Remove or use the variable</li> </ul>"},{"location":"REVIEW_NOTES/#score_test-package","title":"score_test package","text":""},{"location":"REVIEW_NOTES/#potential-logic-bug-score_test_iopy-lines-292-293","title":"Potential Logic Bug (score_test_io.py, lines 292-293)","text":"<p><pre><code>if not add_positions:\n    annotations = annotations.rename({\"BP\": \"POS\"})\n</code></pre> The logic appears inverted. The docstring says \"If True, rename BP to POS\" but the code renames when <code>add_positions=False</code>. - Decision needed: Verify intended behavior and fix if necessary</p>"},{"location":"REVIEW_NOTES/#incorrect-docstring-score_testpy-run_score_test","title":"Incorrect Docstring (score_test.py, run_score_test)","text":"<p>The docstring says \"Unlike run_score_test, this function does not adjust...\" but this IS <code>run_score_test</code>. Copy-paste error. - Decision needed: Update docstring</p>"},{"location":"REVIEW_NOTES/#type-mismatch-in-metaanalysis-meta_analysispy","title":"Type Mismatch in MetaAnalysis (meta_analysis.py)","text":"<p>Class attributes are declared as <code>np.ndarray</code> but <code>__init__</code> sets them to integers (<code>0</code>). - Decision needed: Initialize as empty arrays or change type hints</p>"},{"location":"REVIEW_NOTES/#dead-code-genomeannot-class-score_testpy-lines-266-280","title":"Dead Code: GenomeAnnot Class (score_test.py, lines 266-280)","text":"<p>Class only raises <code>NotImplementedError</code>.  - Decision needed: Remove or implement</p>"},{"location":"REVIEW_NOTES/#unused-function-parameters-score_test_iopy-load_gene_annotations","title":"Unused Function Parameters (score_test_io.py, load_gene_annotations)","text":"<p>Parameters <code>gene_table_path</code> and <code>nearest_weights</code> are passed in but the loaded <code>gene_table</code> is never used. - Decision needed: Review if these should be used or removed</p>"},{"location":"REVIEW_NOTES/#recommended-changes","title":"Recommended Changes","text":""},{"location":"REVIEW_NOTES/#high-priority-potential-bugs","title":"High Priority (Potential Bugs)","text":"<ol> <li>Fix inverted logic in <code>load_annotations</code> (<code>add_positions</code> parameter) in score_test_io.py</li> <li>Fix docstring in <code>run_score_test</code> that references itself incorrectly</li> <li>Add <code>Optional</code> to nullable dataclass fields in <code>TraitData</code></li> <li>Fix forward reference for <code>PrecisionOperator</code> in io.py</li> </ol>"},{"location":"REVIEW_NOTES/#medium-priority-code-quality","title":"Medium Priority (Code Quality)","text":"<ol> <li>Replace star imports in heritability.py and heritability_testing.py with explicit imports</li> <li>Add missing type annotations, especially in meta_analysis.py</li> <li>Standardize on Python 3.10+ style type hints (<code>list[str]</code> instead of <code>List[str]</code>)</li> <li>Refactor the <code>main()</code> function in score_test.py (143 lines) into smaller functions</li> <li>Remove or properly implement <code>GenomeAnnot</code> class</li> </ol>"},{"location":"REVIEW_NOTES/#low-priority-nice-to-have","title":"Low Priority / Nice to Have","text":"<ol> <li>Fix naming conventions (<code>noBlocks</code> -&gt; <code>num_blocks</code> in score_test.py)</li> <li>Remove unused imports across all files (already partially done with ruff --fix)</li> <li>Consolidate duplicate import try/except patterns across score_test modules</li> <li>Add tests for 0% coverage modules</li> </ol>"},{"location":"REVIEW_NOTES/#notes-on-specific-files","title":"Notes on Specific Files","text":""},{"location":"REVIEW_NOTES/#heritabilitypy-heritability_testingpy","title":"heritability.py / heritability_testing.py","text":"<p>These files are near-duplicates with substantial shared code. Consider: - Why are there two versions? - Should they be merged or one deprecated? - The <code>_testing</code> version appears to be a variant for score test output</p>"},{"location":"REVIEW_NOTES/#genesetspy-genesets_gene_hdf5py","title":"genesets.py / genesets_gene_hdf5.py","text":"<p>Both deal with gene sets but have different approaches: - genesets.py: Basic gene set utilities - genesets_gene_hdf5.py: HDF5-based storage for gene-level data Both have 0% test coverage and need review.</p>"},{"location":"REVIEW_NOTES/#multiprocessing_templatepy","title":"multiprocessing_template.py","text":"<p>Complex multiprocessing framework. The <code>ParallelProcessor</code> base class is well-designed but: - Documentation could be improved - Some methods have <code>**kwargs</code> without clear documentation of expected parameters</p>"},{"location":"REVIEW_NOTES/#clipy-both-packages","title":"cli.py (both packages)","text":"<p>Both CLI modules have low coverage and complex argument handling: - graphld/cli.py: 47% coverage, 1124 lines - score_test/cli.py: 65% coverage, 478 lines Consider integration tests for CLI commands.</p>"},{"location":"REVIEW_NOTES/#files-already-fixed-linting","title":"Files Already Fixed (Linting)","text":"<p>The following were automatically fixed with <code>ruff --fix</code>: - Trailing whitespace (W291) - Blank lines with whitespace (W293) - Missing newline at end of file (W292) - f-strings without placeholders (F541) - Some unused imports (F401)</p>"},{"location":"cli/","title":"Command Line Interface","text":"<p>The GraphLD CLI provides commands for various LD-based analyses. Use <code>uv run graphld &lt;command&gt; -h</code> to see usage for a specific command.</p> <p>For the enrichment score test CLI (<code>estest</code>), see Enrichment Score Test.</p>"},{"location":"cli/#available-commands","title":"Available Commands","text":"Command Description <code>reml</code> Run graphREML heritability estimation <code>blup</code> Compute best linear unbiased predictor weights <code>clump</code> P-value thresholding and LD-based pruning <code>simulate</code> Simulate GWAS summary statistics <code>surrogates</code> Precompute surrogate markers"},{"location":"cli/#heritability-estimation-reml","title":"Heritability Estimation (reml)","text":"<p>Run graphREML for heritability partitioning:</p> <pre><code>uv run graphld reml \\\n    /path/to/sumstats/file.sumstats \\\n    output_files_prefix \\\n    --annot-dir /directory/containing/annotation/files/\n</code></pre>"},{"location":"cli/#input-formats","title":"Input Formats","text":"<ul> <li>Summary statistics: VCF (<code>.vcf</code>), LDSC (<code>.sumstats</code>), or Parquet (<code>.parquet</code>) format</li> <li>Variant annotations: Per-chromosome LDSC <code>.annot</code> files, including <code>thin-annot</code> format</li> <li>BED files: UCSC <code>.bed</code> files (not stratified per-chromosome) for binary annotations based on GRCh38 coordinates</li> <li>Gene annotations: GMT files (<code>.gmt</code>) containing gene sets, converted to variant-level using nearest-gene weighting</li> </ul>"},{"location":"cli/#output-files","title":"Output Files","text":"<p>Default output:</p> <ul> <li><code>output_prefix.tall.csv</code>: Heritability, enrichment, and coefficient estimates for each annotation</li> <li><code>output_prefix.convergence.csv</code>: Information about the optimization process</li> </ul> <p>With <code>--alt-output</code> flag (useful for storing results from multiple traits or runs in a single file):</p> <ul> <li><code>output_prefix.heritability.csv</code>: Heritability estimates (one row per run)</li> <li><code>output_prefix.enrichment.csv</code>: Enrichment estimates (one row per run)</li> <li><code>output_prefix.parameters.csv</code>: Coefficient estimates (one row per run)</li> </ul> <p>Use <code>--name</code> to label each run when appending to these files.</p>"},{"location":"cli/#options-reference","title":"Options Reference","text":"<pre><code>uv run graphld reml --help\n</code></pre> <p>Required (one of):</p> Option Description <code>sumstats</code> Path to summary statistics file (.vcf, .sumstats, or .parquet) <code>-a, --annot-dir</code> Path to annotation directory containing .annot and/or .bed files <code>-g, --gene-annot-dir</code> Path to directory containing .gmt gene set files <p>Common options:</p> Option Default Description <code>out</code> None Output file path prefix <code>--intercept</code> 1.0 LD score regression intercept (recommended to estimate with LDSC first) <code>-n, --num-samples</code> auto Sample size (auto-detected from sumstats if available) <code>-p, --population</code> EUR Population for LDGM selection <code>-c, --chromosome</code> all Restrict analysis to specific chromosome <code>--name</code> None Label for this run (used in alt-output files and score test HDF5) <code>-v, --verbose</code> False Print detailed progress <code>-q, --quiet</code> False Suppress output except errors <p>Optimization options:</p> Option Default Description <code>--num-iterations</code> 50 Maximum optimization iterations <code>--convergence-tol</code> 0.01 Convergence tolerance <code>--convergence-window</code> 3 Iterations to consider for convergence <code>--num-jackknife-blocks</code> 100 Number of jackknife blocks for standard errors <code>--xtrace-num-samples</code> 100 Samples for stochastic gradient estimation <code>--reset-trust-region</code> False Reset trust region at each iteration <p>Processing options:</p> Option Default Description <code>--num-processes</code> auto Number of parallel processes <code>--run-in-serial</code> False Disable parallelization <code>--match-by-position</code> False Match variants by position instead of RSID <code>--maximum-missingness</code> 0.1 Maximum fraction of missing samples allowed <code>--max-chisq-threshold</code> None Exclude blocks with chi-squared above threshold <p>Annotation options:</p> Option Default Description <code>--annotation-columns</code> all Specific annotation columns to use <code>--binary-annotations-only</code> False Only include 0/1 valued annotations <p>Gene annotation options (used with <code>-g, --gene-annot-dir</code>):</p> Option Default Description <code>--gene-table</code> data/genes.tsv Path to gene table TSV file <code>--nearest-weights</code> 0.4,0.2,0.1,0.1,0.1,0.05,0.05 Weights for k-nearest genes <p>Advanced options:</p> Option Default Description <code>--surrogates</code> None Path to precomputed surrogate markers HDF5 <code>--initial-params</code> None Initial parameter values (comma-separated) <code>--metadata</code> data/ldgms/metadata.csv Path to LDGM metadata file <code>--score-test-filename</code> None Output HDF5 for score test precomputation <code>--alt-output</code> False Write separate files for heritability/enrichment/parameters <code>--no-save</code> False Do not save results"},{"location":"cli/#gene-set-annotations","title":"Gene Set Annotations","text":"<p>GraphREML can use gene-level annotations from GMT files (Gene Matrix Transposed format). Gene sets are converted to variant-level annotations using a nearest-gene weighting scheme.</p> <pre><code>uv run graphld reml \\\n    /path/to/sumstats.sumstats \\\n    output_prefix \\\n    --gene-annot-dir /path/to/gmt/files/ \\\n    --gene-table data/genes.tsv\n</code></pre> <p>GMT file format: <pre><code>gene_set_name&lt;TAB&gt;description&lt;TAB&gt;gene1&lt;TAB&gt;gene2&lt;TAB&gt;...\n</code></pre></p> <p>Each gene set becomes a variant-level annotation where each variant receives a weighted score based on its proximity to genes in the set. The <code>--nearest-weights</code> option controls how the k-nearest genes contribute to each variant's score.</p> <p>The gene table TSV must contain columns: <code>gene_id</code>, <code>gene_name</code>, <code>start</code>, <code>end</code>, <code>CHR</code>.</p>"},{"location":"cli/#surrogate-markers","title":"Surrogate Markers","text":"<p>If variants are missing from your GWAS summary statistics, graphREML automatically assigns surrogate markers in high LD. This is common when using HapMap3 SNPs only (~1.1M SNPs).</p> <p>To speed up repeated analyses, precompute surrogates:</p> <pre><code>uv run graphld surrogates /path/to/sumstats.sumstats\n</code></pre> <p>Then pass the cached file with <code>--surrogates</code>. Surrogates don't need to match exactly and can be reused across similar sumstats files.</p> <p>Precomputed surrogates for common reference panels are available for download from Zenodo (TODO: add direct link).</p>"},{"location":"cli/#parquet-multi-trait-files","title":"Parquet Multi-Trait Files","text":"<p>When using parquet files with multiple traits:</p> <pre><code># Process specific traits\nuv run graphld reml sumstats.parquet output --name height,bmi ...\n\n# Process all traits (omit --name)\nuv run graphld reml sumstats.parquet output ...\n</code></pre>"},{"location":"cli/#blup-weights-blup","title":"BLUP Weights (blup)","text":"<p>Compute best linear unbiased predictor effect size estimates:</p> <pre><code>uv run graphld blup \\\n    /path/to/sumstats.sumstats \\\n    output_prefix \\\n    --heritability 0.1\n</code></pre>"},{"location":"cli/#ld-clumping-clump","title":"LD Clumping (clump)","text":"<p>Perform p-value thresholding and LD-based pruning:</p> <pre><code>uv run graphld clump \\\n    /path/to/sumstats.sumstats \\\n    output_prefix \\\n    --rsq-threshold 0.1 \\\n    --chisq-threshold 30.0\n</code></pre>"},{"location":"cli/#simulation-simulate","title":"Simulation (simulate)","text":"<p>Simulate GWAS summary statistics:</p> <pre><code>uv run graphld simulate \\\n    output_prefix \\\n    --sample-size 10000 \\\n    --heritability 0.5\n</code></pre>"},{"location":"file_formats/","title":"File Formats","text":"<p>GraphLD supports various input and output file formats for summary statistics, annotations, and gene sets.</p>"},{"location":"file_formats/#summary-statistics","title":"Summary Statistics","text":""},{"location":"file_formats/#ldsc-format-sumstats","title":"LDSC Format (.sumstats)","text":"<p>The standard LDSC summary statistics format.</p> <p>Read with: <pre><code>import graphld as gld\nsumstats = gld.read_ldsc_sumstats(\"path/to/file.sumstats\")\n</code></pre></p>"},{"location":"file_formats/#gwas-vcf-format-vcf","title":"GWAS-VCF Format (.vcf)","text":"<p>The GWAS-VCF specification is supported. Required FORMAT fields:</p> Field Description <code>ES</code> Effect size estimate <code>SE</code> Standard error of effect size <code>LP</code> -log10 p-value <p>Read with: <pre><code>import graphld as gld\nsumstats = gld.read_gwas_vcf(\"path/to/file.vcf\")\n</code></pre></p>"},{"location":"file_formats/#parquet-format-parquet","title":"Parquet Format (.parquet)","text":"<p>Parquet files produced by kodama are supported. The format stores per-trait columns as <code>{trait}_BETA</code> and <code>{trait}_SE</code>, allowing multiple traits per file.</p> <p>Variant info columns are detected automatically:</p> <ul> <li><code>site_ids</code> or <code>SNP</code></li> <li><code>chrom</code> or <code>CHR</code></li> <li><code>position</code> or <code>POS</code></li> <li><code>ref</code> or <code>REF</code></li> <li><code>alt</code> or <code>ALT</code></li> </ul> <p>Read with: <pre><code>import graphld as gld\nsumstats = gld.read_parquet_sumstats(\"path/to/file.parquet\")\n</code></pre></p>"},{"location":"file_formats/#cli-usage-with-multi-trait-parquet","title":"CLI Usage with Multi-Trait Parquet","text":"<pre><code># Process specific traits\nuv run graphld reml sumstats.parquet output --name height,bmi\n\n# Process all traits\nuv run graphld reml sumstats.parquet output\n</code></pre>"},{"location":"file_formats/#annotations","title":"Annotations","text":""},{"location":"file_formats/#ldsc-format-annot","title":"LDSC Format (.annot)","text":"<p>Per-chromosome annotation files in LDSC format.</p> <p>Download BaselineLD model annotations (GRCh38) from the Price lab Google Cloud bucket.</p> <p>Both standard and <code>thin-annot</code> formats (without variant IDs) are supported.</p> <p>Read with: <pre><code>import graphld as gld\nannotations = gld.load_annotations(\"path/to/annot_dir/\", chromosome=1)\n</code></pre></p>"},{"location":"file_formats/#bed-format-bed","title":"BED Format (.bed)","text":"<p>UCSC BED files containing genomic regions. Each <code>.bed</code> file creates a binary annotation with <code>1</code> for variants whose GRCh38 coordinates match.</p> <p>BED files should not be stratified per-chromosome. Place them in the annotation directory and they will be processed automatically.</p> <p>Read with: <pre><code>import graphld as gld\nannotations = gld.load_annotations(\"path/to/annot_dir/\", chromosome=1)\n</code></pre></p>"},{"location":"file_formats/#gmt-format-gmt","title":"GMT Format (.gmt)","text":"<p>Gene Matrix Transposed format for gene sets. Tab-separated with:</p> <ol> <li>Gene set name</li> <li>Description</li> <li>Gene IDs or symbols (remaining columns)</li> </ol> <p>No header row.</p> <p>Example: <pre><code>PATHWAY_A    Description of pathway A    GENE1    GENE2    GENE3\nPATHWAY_B    Description of pathway B    GENE4    GENE5\n</code></pre></p> <p>Read with: <pre><code>from score_test.score_test_io import load_gene_annotations\ngene_annotations = load_gene_annotations(\"path/to/file.gmt\")\n</code></pre></p>"},{"location":"file_formats/#ldgm-files","title":"LDGM Files","text":""},{"location":"file_formats/#edge-list-format","title":"Edge List Format","text":"<p>LDGM precision matrices are stored as edge lists. Files are named like: <pre><code>1kg_chr1_16103_2888443.EAS.edgelist\n</code></pre></p>"},{"location":"file_formats/#snp-list-format","title":"SNP List Format","text":"<p>Associated SNP lists contain variant information: <pre><code>1kg_chr1_16103_2888443.snplist\n</code></pre></p>"},{"location":"file_formats/#metadata-csv","title":"Metadata CSV","text":"<p>The metadata file contains information about all LDGM blocks:</p> Column Description <code>chrom</code> Chromosome <code>chromStart</code> Block start position <code>chromEnd</code> Block end position <code>name</code> Edge list filename <code>snplistName</code> SNP list filename <code>population</code> Population code (e.g., EUR, EAS) <code>numVariants</code> Number of variants <code>numIndices</code> Number of matrix indices <code>numEntries</code> Number of non-zero entries <p>Read with: <pre><code>import graphld as gld\nmetadata = gld.read_ldgm_metadata(\"path/to/metadata.csv\", populations=[\"EUR\"])\n</code></pre></p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#enrichment-score-test-only","title":"Enrichment Score Test Only","text":"<p>If you only need the enrichment score test, no installation or dependencies are required. With <code>uv</code> installed, simply run:</p> <pre><code>uv run src/score_test/score_test.py --help\n</code></pre> <p>Or make the script executable: <pre><code>chmod +x src/score_test/score_test.py\n./src/score_test/score_test.py --help\n</code></pre></p>"},{"location":"installation/#full-installation","title":"Full Installation","text":"<p>The full <code>graphld</code> package (for graphREML, simulation, clumping, BLUP) requires SuiteSparse for sparse matrix operations.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>SuiteSparse</p> <p>On Mac: <pre><code>brew install suitesparse\n</code></pre></p> <p>On Ubuntu/Debian: <pre><code>sudo apt-get install libsuitesparse-dev\n</code></pre></p> <p>Intel MKL (Recommended)</p> <p>For users with Intel chips, Intel MKL can produce a 100x speedup with SuiteSparse vs. OpenBLAS (your likely default BLAS library). See Giulio Genovese's documentation.</p>"},{"location":"installation/#using-uv-recommended","title":"Using uv (Recommended)","text":"<p>Install uv if needed. In the repo directory:</p> <pre><code>uv venv &amp;&amp; uv sync\n</code></pre> <p>For development installation: <pre><code>uv venv &amp;&amp; uv sync --dev --extra dev  # editable with pytest dependencies\nuv run pytest  # tests will fail if you haven't run `make download`\n</code></pre></p>"},{"location":"installation/#using-conda-and-pip","title":"Using conda and pip","text":"<p>Conda has the advantage that you can <code>conda install</code> SuiteSparse directly.</p> <p>Create conda environment:</p> <pre><code>module load miniconda3/4.10.3  # if on a cluster\nconda create -n suitesparse conda-forge::suitesparse python=3.11.0\nconda activate suitesparse\n</code></pre> <p>You may need to revert or reinstall some Python packages: <pre><code>pip install numpy==1.26.4\n</code></pre></p> <p>Install scikit-sparse:</p> <pre><code>conda config --add channels conda-forge\nconda config --set channel_priority strict\nconda install scikit-sparse\n</code></pre> <p>Install graphld:</p> <pre><code>cd graphld &amp;&amp; pip install .\n</code></pre> <p>Test installation:</p> <pre><code>graphld -h\n</code></pre>"},{"location":"installation/#downloading-data","title":"Downloading Data","text":"<p>Pre-computed LDGMs and data files are available from Zenodo. Download using the provided Makefile:</p> <pre><code>cd data &amp;&amp; make download_all\n</code></pre> <p>The full download takes 30-60 minutes depending on connection speed.</p>"},{"location":"installation/#recommended-downloads","title":"Recommended Downloads","text":"Use Case Command Size Score test for gene sets only <code>make download_gene_scores</code> ~10 MB Score test for variant annotations <code>make download_scores</code> ~6.5 GB graphREML on European-ancestry data <code>make download_reml</code> ~2 GB All populations / all features <code>make download_all</code> ~25 GB <p>To try out graphREML or score test with example summary statistics, additionally run <code>make download_sumstats</code> (~7 GB).</p>"},{"location":"installation/#all-download-options","title":"All Download Options","text":"Command Description Size <code>make download_all</code> All data files ~25 GB <code>make download_reml</code> UKBB precision + annotations + surrogates ~2 GB <code>make download_ukbb_precision</code> UK Biobank LDGM precision matrices ~1.5 GB <code>make download_precision</code> All LDGM precision matrices (all populations) ~10 GB <code>make download_annotations</code> BaselineLD annotation files ~400 MB <code>make download_scores</code> Score statistics (variant + gene level) ~6.5 GB <code>make download_gene_scores</code> Gene-level score statistics only ~10 MB <code>make download_surrogates</code> Surrogate markers + gene table ~60 MB <code>make download_sumstats</code> GWAS summary statistics (Li et al. 2025) ~7 GB"},{"location":"installation/#data-sources","title":"Data Sources","text":"<ul> <li>Precision matrices: Zenodo 8157131 - LDGM precision matrices for 1000 Genomes populations</li> <li>Annotations &amp; sumstats: Zenodo 15085817 - BaselineLD annotations and UK Biobank summary statistics</li> <li>Score statistics: Zenodo 18102484 - Pre-computed score statistics for enrichment testing</li> </ul>"},{"location":"installation/#directory-structure","title":"Directory Structure","text":"<p>After downloading, the <code>data/</code> directory will contain:</p> <pre><code>data/\n\u251c\u2500\u2500 ldgms/              # LDGM precision matrices\n\u251c\u2500\u2500 baselineld/         # BaselineLD annotation files\n\u251c\u2500\u2500 scores/             # Score statistics (.h5 files)\n\u251c\u2500\u2500 surrogates/         # Surrogate marker files\n\u251c\u2500\u2500 genes.tsv           # Gene table (GRCh38)\n\u2514\u2500\u2500 rsid_position.csv   # SNP position mapping\n</code></pre>"},{"location":"score_test/","title":"Enrichment Score Test","text":"<p>The enrichment score test is a fast method to test genomic or gene annotations for heritability enrichment conditional upon a null model.</p>"},{"location":"score_test/#overview","title":"Overview","text":"<p>The test produces Z scores:</p> <ul> <li>Positive score: Heritability enrichment</li> <li>Negative score: Heritability depletion</li> </ul> <p>Enrichments are conditional upon the null model, similar to the <code>tau</code> parameter in S-LDSC. The test does not produce point estimates (for that, run graphREML).</p>"},{"location":"score_test/#requirements","title":"Requirements","text":"<p>You need a file containing precomputed derivatives for each trait being tested. This can be:</p> <ul> <li>Downloaded from Zenodo via the Makefile (<code>make download_scorestats</code>)</li> <li>Created by running graphREML with the <code>--score-test-filename</code> flag</li> </ul>"},{"location":"score_test/#supported-annotation-formats","title":"Supported Annotation Formats","text":"Format Description LDSC <code>.annot</code> Variant annotations UCSC <code>.bed</code> Genomic regions GMT Gene sets (symbols or IDs)"},{"location":"score_test/#basic-usage","title":"Basic Usage","text":""},{"location":"score_test/#view-available-traits","title":"View Available Traits","text":"<pre><code>uv run estest show path/to/precomputed/derivatives.h5\n</code></pre>"},{"location":"score_test/#test-variant-annotations","title":"Test Variant Annotations","text":"<pre><code>uv run estest \\\n    path/to/precomputed/derivatives.h5 \\\n    path/to/output/file/prefix \\\n    --variant-annot-dir /directory/containing/dot-annot/files/\n</code></pre>"},{"location":"score_test/#test-genomic-regions","title":"Test Genomic Regions","text":"<pre><code>uv run estest \\\n    path/to/precomputed/derivatives.h5 \\\n    path/to/output/file/prefix \\\n    --variant-annot-dir /directory/containing/dot-bed/files/\n</code></pre>"},{"location":"score_test/#test-gene-annotations","title":"Test Gene Annotations","text":"<pre><code>uv run estest \\\n    path/to/precomputed/derivatives.h5 \\\n    path/to/output/file/prefix \\\n    --gene-annot-dir /directory/containing/gmt/files/\n</code></pre>"},{"location":"score_test/#options-reference","title":"Options Reference","text":"<pre><code>uv run estest test --help\n</code></pre> Option Description <code>-a, --variant-annot-dir</code> Directory containing .annot files <code>-g, --gene-annot-dir</code> Directory containing .gmt files <code>--random-genes</code> Comma-separated probabilities for random gene annotations <code>--random-variants</code> Comma-separated probabilities for random variant annotations <code>--gene-table</code> Path to gene table TSV (required for gene-level options) <code>--nearest-weights</code> Comma-separated weights for k-nearest genes <code>--annotations</code> Specific annotation names to test <code>-n, --name</code> Specific trait to process (default: all traits) <code>-v, --verbose</code> Enable verbose output <code>--seed</code> Random seed for reproducibility"},{"location":"score_test/#random-annotations","title":"Random Annotations","text":"<p>Test random annotations to verify the null distribution:</p>"},{"location":"score_test/#random-variant-annotations","title":"Random Variant Annotations","text":"<pre><code>uv run estest \\\n    path/to/precomputed/derivatives.h5 \\\n    path/to/output/file/prefix \\\n    --random-variants 0.1,0.2,0.3\n</code></pre> <p>Creates random annotations with 10%, 20%, and 30% of variants.</p>"},{"location":"score_test/#random-gene-annotations","title":"Random Gene Annotations","text":"<pre><code>uv run estest \\\n    path/to/precomputed/derivatives.h5 \\\n    path/to/output/file/prefix \\\n    --random-genes 0.1,0.2,0.3\n</code></pre> <p>Creates random annotations with 10%, 20%, and 30% of genes.</p>"},{"location":"score_test/#perturb-existing-annotations","title":"Perturb Existing Annotations","text":"<pre><code>uv run estest \\\n    path/to/precomputed/derivatives.h5 \\\n    path/to/output/file/prefix \\\n    --variant-annot-dir /directory/containing/dot-annot/files/ \\\n    --perturb-annot 0.5  # 50% of annotation values sampled randomly\n</code></pre>"},{"location":"score_test/#gene-set-testing","title":"Gene Set Testing","text":"<p>Gene sets can be tested for heritability enrichment under the Abstract Mediation Model (AMM; Weiner et al. 2022 AJHG). This tests whether variants in proximity to genes in the gene set are enriched for heritability.</p>"},{"location":"score_test/#basic-approach","title":"Basic Approach","text":"<p>Supply a GMT file to <code>--gene-annot-dir</code>.</p>"},{"location":"score_test/#faster-approach","title":"Faster Approach","text":"<p>Convert variant-level to gene-level score statistics first:</p> <pre><code>uv run estest convert variant_statistics.h5 gene_statistics.h5\n</code></pre> <p>This requires a gene positions file (provided in <code>data/genes.tsv</code> after running the Makefile).</p> <p>Then run the test:</p> <pre><code>uv run estest \\\n    gene_statistics.h5 output_prefix \\\n    --gene-annot-dir /directory/containing/gmt/files/\n</code></pre> <p>Results are nearly identical to the variant-level test but much faster.</p>"},{"location":"score_test/#meta-analysis-across-traits","title":"Meta-Analysis Across Traits","text":"<p>Test whether an annotation is enriched across multiple traits:</p>"},{"location":"score_test/#add-meta-analysis","title":"Add Meta-Analysis","text":"<pre><code># Add all traits\nuv run estest add-meta statistics.h5 all_traits '*'\n\n# Add specific traits\nuv run estest add-meta statistics.h5 body_traits height bmi\n</code></pre> <p>Then run the score test as normal. The meta-analysis appears as a column in the output.</p> <p>The meta-analysis uses precision-weighted linear combination of score statistics with jackknife standard errors. Non-independence across traits causes power loss but not false positives.</p>"},{"location":"score_test/#manipulating-hdf5-files","title":"Manipulating HDF5 Files","text":"<p>The <code>estest</code> command provides utilities for managing traits and meta-analyses in HDF5 files.</p>"},{"location":"score_test/#show-contents","title":"Show Contents","text":"<p>Display all traits and meta-analyses in an HDF5 file:</p> <pre><code>uv run estest show statistics.h5\n</code></pre>"},{"location":"score_test/#rename-traits-or-meta-analyses","title":"Rename Traits or Meta-Analyses","text":"<p>Rename a trait or meta-analysis (auto-detects which):</p> <pre><code>uv run estest mv statistics.h5 old_name new_name\n</code></pre>"},{"location":"score_test/#remove-traits-or-meta-analyses","title":"Remove Traits or Meta-Analyses","text":"<p>Remove one or more traits or meta-analyses:</p> <pre><code># Remove a single item\nuv run estest rm statistics.h5 trait_name\n\n# Remove multiple items\nuv run estest rm statistics.h5 bmi height cancer\n\n# Use wildcards\nuv run estest rm statistics.h5 '*_EAS'\n\n# Force removal without confirmation\nuv run estest rm statistics.h5 'BMI*' -f\n</code></pre> <p>The command auto-detects whether names are traits or meta-analyses and supports wildcards (<code>*</code>).</p>"},{"location":"see_also/","title":"See Also","text":""},{"location":"see_also/#related-projects","title":"Related Projects","text":""},{"location":"see_also/#ldgm-repository","title":"LDGM Repository","text":"<p>Main LDGM repository with MATLAB API:</p> <ul> <li>https://github.com/awohns/ldgm</li> </ul>"},{"location":"see_also/#graphreml-matlab","title":"graphREML (MATLAB)","text":"<p>Original graphREML implementation in MATLAB:</p> <ul> <li>https://github.com/huilisabrina/graphREML</li> </ul> <p>We recommend using the Python implementation in this repository, which is much faster.</p>"},{"location":"see_also/#ld-score-regression","title":"LD Score Regression","text":"<p>Reference implementation of LD score regression:</p> <ul> <li>https://github.com/bulik/ldsc</li> </ul>"},{"location":"see_also/#ldgm-vcf-bcftools","title":"LDGM-VCF (bcftools)","text":"<p>Giulio Genovese's bcftools plugin with LDGM-VCF file format specification and C implementation:</p> <ul> <li>https://github.com/freeseek/score</li> </ul>"},{"location":"see_also/#suitesparse","title":"SuiteSparse","text":"<p>Sparse matrix operations used by graphld:</p> <ul> <li>https://github.com/DrTimothyAldenDavis/SuiteSparse</li> </ul>"},{"location":"see_also/#citations","title":"Citations","text":"<p>If you use this software, please cite:</p>"},{"location":"see_also/#graphreml","title":"graphREML","text":"<p>Hui Li, Tushar Kamath, Rahul Mazumder, Xihong Lin, &amp; Luke J. O'Connor (2024). Improved heritability partitioning and enrichment analyses using summary statistics with graphREML. medRxiv, 2024-11. DOI: 10.1101/2024.11.04.24316716</p>"},{"location":"see_also/#ldgm","title":"LDGM","text":"<p>Pouria Salehi Nowbandegani, Anthony Wilder Wohns, Jenna L. Ballard, Eric S. Lander, Alex Bloemendal, Benjamin M. Neale, and Luke J. O'Connor (2023). Extremely sparse models of linkage disequilibrium in ancestrally diverse association studies. Nat Genet. DOI: 10.1038/s41588-023-01487-8</p>"},{"location":"api/blup/","title":"graphld.blup","text":"<p>Best Linear Unbiased Prediction (BLUP) for effect size estimation.</p> <p>Under the infinitesimal model with per-s.d. effect sizes $\\beta \\sim N(0, D)$, the BLUP effect sizes are:</p> <p>$$E(\\beta) = \\sqrt{n} D (nD + R^{-1})^{-1} R^{-1}z$$</p> <p>where $R^{-1}$ is approximated with the LDGM precision matrix.</p>"},{"location":"api/blup/#graphld.blup","title":"blup","text":""},{"location":"api/blup/#graphld.blup.BLUP","title":"BLUP","text":"<p>               Bases: <code>ParallelProcessor</code></p> <p>Computes the best linear unbiased predictor using LDGMs and GWAS summary statistics.</p>"},{"location":"api/blup/#graphld.blup.BLUP.prepare_block_data","title":"prepare_block_data  <code>classmethod</code>","text":"<pre><code>prepare_block_data(metadata: DataFrame, **kwargs) -&gt; list[tuple]\n</code></pre> <p>Split summary statistics into blocks whose positions match the LDGMs.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>DataFrame</code> <p>DataFrame containing LDGM metadata</p> required <code>**kwargs</code> <p>Additional arguments from run(), including: annotations: Optional DataFrame containing variant annotations</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[tuple]</code> <p>List of block-specific annotation DataFrames, or None if no annotations</p> Source code in <code>src/graphld/blup.py</code> <pre><code>@classmethod\ndef prepare_block_data(cls, metadata: pl.DataFrame, **kwargs) -&gt; list[tuple]:\n    \"\"\"Split summary statistics into blocks whose positions match the LDGMs.\n\n    Args:\n        metadata: DataFrame containing LDGM metadata\n        **kwargs: Additional arguments from run(), including:\n            annotations: Optional DataFrame containing variant annotations\n\n    Returns:\n        List of block-specific annotation DataFrames, or None if no annotations\n    \"\"\"\n    sumstats = kwargs.get('sumstats')\n\n    # Partition annotations into blocks\n    sumstats_blocks: list[pl.DataFrame] = partition_variants(metadata, sumstats)\n\n    cumulative_num_variants = np.cumsum(np.array([len(df) for df in sumstats_blocks]))\n    cumulative_num_variants = [0] + list(cumulative_num_variants[:-1])\n\n    return list(zip(sumstats_blocks, cumulative_num_variants, strict=False))\n</code></pre>"},{"location":"api/blup/#graphld.blup.BLUP.create_shared_memory","title":"create_shared_memory  <code>staticmethod</code>","text":"<pre><code>create_shared_memory(metadata: DataFrame, block_data: list[tuple], **kwargs) -&gt; SharedData\n</code></pre> <p>Create output array with length number of variants in the summary statistics that  migtht match to one of the blocks.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>DataFrame</code> <p>Metadata DataFrame containing block information</p> required <code>block_data</code> <code>list[tuple]</code> <p>List of block-specific sumstats DataFrames</p> required <code>**kwargs</code> <p>Not used</p> <code>{}</code> Source code in <code>src/graphld/blup.py</code> <pre><code>@staticmethod\ndef create_shared_memory(metadata: pl.DataFrame, block_data: list[tuple], **kwargs) -&gt; SharedData:\n    \"\"\"Create output array with length number of variants in the summary statistics that \n    migtht match to one of the blocks.\n\n    Args:\n        metadata: Metadata DataFrame containing block information\n        block_data: List of block-specific sumstats DataFrames\n        **kwargs: Not used\n    \"\"\"\n    total_variants = sum([len(df) for df, _ in block_data])\n    return SharedData({\n        'beta': total_variants,    # BLUP effect sizes\n    })\n</code></pre>"},{"location":"api/blup/#graphld.blup.BLUP.process_block","title":"process_block  <code>classmethod</code>","text":"<pre><code>process_block(ldgm: PrecisionOperator, flag: Value, shared_data: SharedData, block_offset: int, block_data: tuple, worker_params: tuple) -&gt; None\n</code></pre> <p>Run BLUP on a single block.</p> Source code in <code>src/graphld/blup.py</code> <pre><code>@classmethod\ndef process_block(cls, ldgm: PrecisionOperator, flag: Value,\n                 shared_data: SharedData, block_offset: int,\n                 block_data: tuple,\n                 worker_params: tuple) -&gt; None:\n    \"\"\"Run BLUP on a single block.\"\"\"\n    sigmasq, sample_size, match_by_position = worker_params\n    assert isinstance(sigmasq, float), \"sigmasq parameter must be a float\"\n    assert isinstance(block_data, tuple), \"block_data must be a tuple\"\n    sumstats, variant_offset = block_data\n    num_variants = len(sumstats)\n\n    # Merge annotations with LDGM variant info and get indices of merged variants\n    from .io import merge_snplists\n    ldgm, sumstat_indices = merge_snplists(\n        ldgm, sumstats,\n        match_by_position=match_by_position,\n        pos_col='POS',\n        ref_allele_col='REF',\n        alt_allele_col='ALT',\n        add_allelic_cols=['Z'],\n    )\n\n    # Keep only first occurrence of each index\n    first_index_mask = ldgm.variant_info.select(pl.col('index').is_first_distinct()).to_numpy().flatten()\n    ldgm.variant_info = ldgm.variant_info.filter(first_index_mask)\n    sumstat_indices = sumstat_indices[first_index_mask]\n\n    # Get Z-scores from the merged variant info\n    z = ldgm.variant_info.select('Z').to_numpy()\n\n    # Compute the BLUP for this block\n    beta = ldgm @ z\n    ldgm.update_matrix(np.full(ldgm.shape[0], sample_size*sigmasq))\n    beta = np.sqrt(sample_size) * sigmasq * ldgm.solve(beta)\n    ldgm.del_factor()\n\n    # Store results for variants that were successfully merged\n    beta_reshaped = np.zeros((num_variants,1))\n    # Get indices of variants that were actually merged\n    beta_reshaped[sumstat_indices, 0] = beta.flatten()\n\n    # Update the shared memory array\n    block_slice = slice(variant_offset, variant_offset + num_variants)\n    shared_data['beta', block_slice] = beta_reshaped\n</code></pre>"},{"location":"api/blup/#graphld.blup.BLUP.supervise","title":"supervise  <code>classmethod</code>","text":"<pre><code>supervise(manager: WorkerManager, shared_data: Dict[str, Any], block_data: list, **kwargs) -&gt; pl.DataFrame\n</code></pre> <p>Supervise worker processes and collect results.</p> <p>Parameters:</p> Name Type Description Default <code>manager</code> <code>WorkerManager</code> <p>Worker manager</p> required <code>shared_data</code> <code>Dict[str, Any]</code> <p>Dictionary of shared memory arrays</p> required <code>**kwargs</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing simulated summary statistics</p> Source code in <code>src/graphld/blup.py</code> <pre><code>@classmethod\ndef supervise(cls, manager: WorkerManager, shared_data: Dict[str, Any], block_data: list, **kwargs) -&gt; pl.DataFrame:\n    \"\"\"Supervise worker processes and collect results.\n\n    Args:\n        manager: Worker manager\n        shared_data: Dictionary of shared memory arrays\n        **kwargs: Additional arguments\n\n    Returns:\n        DataFrame containing simulated summary statistics\n    \"\"\"\n\n    manager.start_workers()\n    manager.await_workers()\n    beta = shared_data['beta']\n    sumstats = pl.concat([df for df, _ in block_data])\n    return sumstats.with_columns(pl.Series('weight', beta))\n</code></pre>"},{"location":"api/blup/#graphld.blup.BLUP.compute_blup","title":"compute_blup  <code>classmethod</code>","text":"<pre><code>compute_blup(ldgm_metadata_path: str, sumstats: DataFrame, sigmasq: float, sample_size: float, populations: Optional[Union[str, List[str]]] = None, chromosomes: Optional[Union[int, List[int]]] = None, num_processes: Optional[int] = None, run_in_serial: bool = False, match_by_position: bool = False, verbose: bool = False) -&gt; pl.DataFrame\n</code></pre> <p>Simulate GWAS summary statistics for multiple LD blocks.</p> <p>Parameters:</p> Name Type Description Default <code>ldgm_metadata_path</code> <code>str</code> <p>Path to metadata CSV file</p> required <code>sumstats</code> <code>DataFrame</code> <p>Sumstats dataframe containing Z scores</p> required <code>populations</code> <code>Optional[Union[str, List[str]]]</code> <p>Optional population name</p> <code>None</code> <code>chromosomes</code> <code>Optional[Union[int, List[int]]]</code> <p>Optional chromosome or list of chromosomes</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Print additional information if True</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Array of BLUP effect sizes, same length as sumstats</p> Source code in <code>src/graphld/blup.py</code> <pre><code>@classmethod\ndef compute_blup(cls,\n        ldgm_metadata_path: str,\n        sumstats: pl.DataFrame,\n        sigmasq: float,\n        sample_size: float,\n        populations: Optional[Union[str, List[str]]] = None,\n        chromosomes: Optional[Union[int, List[int]]] = None,\n        num_processes: Optional[int] = None,\n        run_in_serial: bool = False,\n        match_by_position: bool = False,\n        verbose: bool = False,\n        ) -&gt; pl.DataFrame:\n    \"\"\"Simulate GWAS summary statistics for multiple LD blocks.\n\n    Args:\n        ldgm_metadata_path: Path to metadata CSV file\n        sumstats: Sumstats dataframe containing Z scores\n        populations: Optional population name\n        chromosomes: Optional chromosome or list of chromosomes\n        verbose: Print additional information if True\n\n    Returns:\n        Array of BLUP effect sizes, same length as sumstats\n    \"\"\"\n    run_fn = cls.run_serial if run_in_serial else cls.run\n    result = run_fn(\n        ldgm_metadata_path=ldgm_metadata_path,\n        populations=populations,\n        chromosomes=chromosomes,\n        num_processes=num_processes,\n        worker_params=(sigmasq, sample_size, match_by_position),\n        sumstats=sumstats\n    )\n\n    if verbose:\n        print(f\"Number of variants in summary statistics: {len(result)}\")\n        nonzero_count = (result['weight'] != 0).sum()\n        print(f\"Number of variants with nonzero weights: {nonzero_count}\")\n\n    return result\n</code></pre>"},{"location":"api/blup/#graphld.blup.run_blup","title":"run_blup","text":"<pre><code>run_blup(*args, **kwargs)\n</code></pre> <p>Compute Best Linear Unbiased Prediction (BLUP) weights.</p> <p>Parameters:</p> Name Type Description Default <code>ldgm_metadata_path</code> <code>str</code> <p>Path to LDGM metadata file</p> required <code>sumstats</code> <code>DataFrame</code> <p>Summary statistics DataFrame</p> required <code>heritability</code> <code>float</code> <p>Heritability parameter (between 0 and 1)</p> required <code>num_samples</code> <code>Optional[int]</code> <p>Number of samples. Defaults to None.</p> required <code>num_processes</code> <code>Optional[int]</code> <p>Number of processes for parallel computation. Defaults to None.</p> required <code>run_in_serial</code> <code>bool</code> <p>Whether to run in serial mode. Defaults to False.</p> required <code>chromosome</code> <code>Optional[int]</code> <p>Chromosome to filter. Defaults to None.</p> required <code>population</code> <code>Optional[str]</code> <p>Population to filter. Defaults to None.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print verbose output. Defaults to False.</p> required <code>quiet</code> <code>bool</code> <p>Whether to suppress all output except errors. Defaults to False.</p> required <p>Returns:</p> Type Description <p>pl.DataFrame: DataFrame with BLUP weights and associated statistics</p> Source code in <code>src/graphld/blup.py</code> <pre><code>def run_blup(*args, **kwargs):\n    \"\"\"\n    Compute Best Linear Unbiased Prediction (BLUP) weights.\n\n    Args:\n        ldgm_metadata_path (str): Path to LDGM metadata file\n        sumstats (pl.DataFrame): Summary statistics DataFrame\n        heritability (float): Heritability parameter (between 0 and 1)\n        num_samples (Optional[int], optional): Number of samples. Defaults to None.\n        num_processes (Optional[int], optional): Number of processes for parallel computation. Defaults to None.\n        run_in_serial (bool, optional): Whether to run in serial mode. Defaults to False.\n        chromosome (Optional[int], optional): Chromosome to filter. Defaults to None.\n        population (Optional[str], optional): Population to filter. Defaults to None.\n        verbose (bool, optional): Whether to print verbose output. Defaults to False.\n        quiet (bool, optional): Whether to suppress all output except errors. Defaults to False.\n\n    Returns:\n        pl.DataFrame: DataFrame with BLUP weights and associated statistics\n    \"\"\"\n    return BLUP.compute_blup(*args, **kwargs)\n</code></pre>"},{"location":"api/clumping/","title":"graphld.clumping","text":"<p>LD clumping for identifying independent variants.</p> <p>LD clumping identifies independent index variants by iteratively selecting the variant with the highest \u03c7\u00b2 statistic and pruning all variants in high LD with it. Clumping + thresholding is a popular (though suboptimal) way of computing polygenic scores.</p>"},{"location":"api/clumping/#graphld.clumping","title":"clumping","text":"<p>LD clumping implementation using ParallelProcessor framework.</p>"},{"location":"api/clumping/#graphld.clumping.LDClumper","title":"LDClumper","text":"<p>               Bases: <code>ParallelProcessor</code></p> <p>Fast LD clumping to find unlinked lead SNPs from GWAS summary statistics.</p>"},{"location":"api/clumping/#graphld.clumping.LDClumper.prepare_block_data","title":"prepare_block_data  <code>classmethod</code>","text":"<pre><code>prepare_block_data(metadata: DataFrame, **kwargs) -&gt; list[tuple]\n</code></pre> <p>Split summary statistics into blocks whose positions match the LDGMs.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>DataFrame</code> <p>DataFrame containing LDGM metadata</p> required <code>**kwargs</code> <p>Additional arguments from run(), including: sumstats: DataFrame containing summary statistics</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[tuple]</code> <p>List of block-specific sumstats DataFrames and their offsets</p> Source code in <code>src/graphld/clumping.py</code> <pre><code>@classmethod\ndef prepare_block_data(cls, metadata: pl.DataFrame, **kwargs) -&gt; list[tuple]:\n    \"\"\"Split summary statistics into blocks whose positions match the LDGMs.\n\n    Args:\n        metadata: DataFrame containing LDGM metadata\n        **kwargs: Additional arguments from run(), including:\n            sumstats: DataFrame containing summary statistics\n\n    Returns:\n        List of block-specific sumstats DataFrames and their offsets\n    \"\"\"\n    sumstats = kwargs.get('sumstats')\n\n    # Partition annotations into blocks\n    sumstats_blocks: list[pl.DataFrame] = partition_variants(metadata, sumstats)\n\n    cumulative_num_variants = np.cumsum(np.array([len(df) for df in sumstats_blocks]))\n    cumulative_num_variants = [0] + list(cumulative_num_variants[:-1])\n\n    return list(zip(sumstats_blocks, cumulative_num_variants, strict=False))\n</code></pre>"},{"location":"api/clumping/#graphld.clumping.LDClumper.create_shared_memory","title":"create_shared_memory  <code>staticmethod</code>","text":"<pre><code>create_shared_memory(metadata: DataFrame, block_data: list[tuple], **kwargs) -&gt; SharedData\n</code></pre> <p>Create output array with length number of variants in the summary statistics.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>DataFrame</code> <p>Metadata DataFrame containing block information</p> required <code>block_data</code> <code>list[tuple]</code> <p>List of block-specific sumstats DataFrames</p> required <code>**kwargs</code> <p>Not used</p> <code>{}</code> <p>Returns:</p> Type Description <code>SharedData</code> <p>SharedData containing arrays for clumping results</p> Source code in <code>src/graphld/clumping.py</code> <pre><code>@staticmethod\ndef create_shared_memory(metadata: pl.DataFrame, block_data: list[tuple], **kwargs) -&gt; SharedData:\n    \"\"\"Create output array with length number of variants in the summary statistics.\n\n    Args:\n        metadata: Metadata DataFrame containing block information\n        block_data: List of block-specific sumstats DataFrames\n        **kwargs: Not used\n\n    Returns:\n        SharedData containing arrays for clumping results\n    \"\"\"\n    total_variants = sum([len(df) for df, _ in block_data])\n    return SharedData({\n        'is_index': total_variants,  # Will be converted to boolean later\n    })\n</code></pre>"},{"location":"api/clumping/#graphld.clumping.LDClumper.process_block","title":"process_block  <code>classmethod</code>","text":"<pre><code>process_block(ldgm: PrecisionOperator, flag: Value, shared_data: SharedData, block_offset: int, block_data: tuple, worker_params: tuple) -&gt; None\n</code></pre> <p>Process single block for LD clumping.</p> <p>Parameters:</p> Name Type Description Default <code>ldgm</code> <code>PrecisionOperator</code> <p>LDGM object</p> required <code>flag</code> <code>Value</code> <p>Worker flag</p> required <code>shared_data</code> <code>SharedData</code> <p>Dictionary-like shared data object</p> required <code>block_offset</code> <code>int</code> <p>Offset for this block</p> required <code>block_data</code> <code>tuple</code> <p>Tuple of (sumstats DataFrame, variant_offset)</p> required <code>worker_params</code> <code>tuple</code> <p>Tuple of (rsq_threshold, chisq_threshold, z_col, match_by_position, variant_id_col)</p> required Source code in <code>src/graphld/clumping.py</code> <pre><code>@classmethod\ndef process_block(cls, ldgm: PrecisionOperator,\n                flag: Value,\n                shared_data: SharedData,\n                block_offset: int,\n                block_data: tuple,\n                worker_params: tuple) -&gt; None:\n    \"\"\"Process single block for LD clumping.\n\n    Args:\n        ldgm: LDGM object\n        flag: Worker flag\n        shared_data: Dictionary-like shared data object\n        block_offset: Offset for this block\n        block_data: Tuple of (sumstats DataFrame, variant_offset)\n        worker_params: Tuple of (rsq_threshold, chisq_threshold, z_col, match_by_position, variant_id_col)\n    \"\"\"\n    rsq_threshold, chisq_threshold, z_col, match_by_position, variant_id_col = worker_params\n    assert isinstance(block_data, tuple), \"block_data must be a tuple\"\n    sumstats, variant_offset = block_data\n    num_variants = len(sumstats)\n\n    # Merge variants with LDGM variant info and get indices of merged variants\n    ldgm, sumstat_indices = merge_snplists(\n        ldgm, sumstats,\n        match_by_position=match_by_position,\n        pos_col='POS',\n        variant_id_col=variant_id_col,\n        ref_allele_col='REF',\n        alt_allele_col='ALT',\n        add_allelic_cols=[z_col],  # Z score column\n    )\n\n    # Keep only first occurrence of each index\n    first_index_mask = pl.Series(ldgm.variant_info.select(pl.col('is_representative')).to_numpy().flatten().astype(bool))\n    ldgm.variant_info = ldgm.variant_info.filter(first_index_mask)\n    sumstat_indices = sumstat_indices[first_index_mask]\n\n    # Get Z scores and compute chi-square statistics\n    z_scores = ldgm.variant_info.select(z_col).to_numpy().flatten()  # Z score column\n    chisq = z_scores ** 2\n\n    # Check original sumstats chi-square values\n    original_z = sumstats.select(z_col).to_numpy().flatten()  # Z score column\n    original_chisq = original_z ** 2\n    assert np.allclose(original_chisq[sumstat_indices.flatten()], chisq), \"Chi-square values changed after merging\"\n\n    # Sort variants by chi-square statistic\n    sort_idx = np.argsort(chisq)[::-1]  # Descending order\n\n    # Initialize arrays for tracking pruned and index variants\n    n = len(z_scores)\n    was_pruned = np.zeros(n, dtype=bool)\n    is_index = np.zeros(n, dtype=bool)\n\n    # Iterate through variants in order of decreasing chi-square\n    for i in sort_idx:\n        # Stop if we reach variants below threshold\n        if chisq[i] &lt; chisq_threshold:\n            break\n\n        # Skip if this variant was already pruned\n        if was_pruned[i]:\n            continue\n\n        # This is an index variant\n        is_index[i] = True\n\n        # Compute LD with all other variants\n        indicator = np.zeros(n)\n        indicator[i] = 1\n        ld = ldgm.solve(indicator)\n\n        # Mark variants in high LD for pruning\n        to_prune = (ld ** 2 &gt;= rsq_threshold)\n        assert to_prune[i]  # Lead SNP should be in LD with itself\n        was_pruned[to_prune] = True\n\n    # Initialize results array with all False\n    results = np.zeros(num_variants, dtype=float)  # Use float for shared memory\n\n    # Map results back to original variants using sumstat_indices\n    # Only set results for variants that were successfully merged\n    results[sumstat_indices.flatten()] = is_index.astype(float)  # Convert to float for shared memory\n\n    # Store results\n    block_slice = slice(variant_offset, variant_offset + num_variants)\n    shared_data['is_index', block_slice] = results\n</code></pre>"},{"location":"api/clumping/#graphld.clumping.LDClumper.supervise","title":"supervise  <code>classmethod</code>","text":"<pre><code>supervise(manager: Union[WorkerManager, SerialManager], shared_data: SharedData, block_data: list, **kwargs) -&gt; pl.DataFrame\n</code></pre> <p>Monitor workers and process results.</p> <p>Parameters:</p> Name Type Description Default <code>manager</code> <code>Union[WorkerManager, SerialManager]</code> <p>Worker manager for controlling processes</p> required <code>shared_data</code> <code>SharedData</code> <p>Shared memory data</p> required <code>block_data</code> <code>list</code> <p>List of block-specific data</p> required <code>**kwargs</code> <p>Additional arguments passed from run()</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with clumping results</p> Source code in <code>src/graphld/clumping.py</code> <pre><code>@classmethod\ndef supervise(cls, manager: Union[WorkerManager, SerialManager],\n            shared_data: SharedData,\n            block_data: list, **kwargs) -&gt; pl.DataFrame:\n    \"\"\"Monitor workers and process results.\n\n    Args:\n        manager: Worker manager for controlling processes\n        shared_data: Shared memory data\n        block_data: List of block-specific data\n        **kwargs: Additional arguments passed from run()\n\n    Returns:\n        DataFrame with clumping results\n    \"\"\"\n    manager.start_workers()\n    manager.await_workers()\n    is_index = shared_data['is_index']\n\n    # Concatenate the original sumstats DataFrames in order\n    sumstats = pl.concat([df for df, _ in block_data])\n\n    # Add is_index column with results, converting back to boolean\n    return sumstats.with_columns(pl.Series('is_index', is_index.astype(bool)))\n</code></pre>"},{"location":"api/clumping/#graphld.clumping.LDClumper.clump","title":"clump  <code>classmethod</code>","text":"<pre><code>clump(sumstats: DataFrame, ldgm_metadata_path: str = 'data/ldgms/metadata.csv', rsq_threshold: float = 0.1, chisq_threshold: float = 30.0, populations: Optional[Union[str, List[str]]] = None, chromosomes: Optional[Union[int, List[int]]] = None, run_in_serial: bool = False, num_processes: Optional[int] = None, z_col: str = 'Z', match_by_position: bool = True, variant_id_col: str = 'SNP', verbose: bool = False) -&gt; pl.DataFrame\n</code></pre> <p>Perform LD clumping on summary statistics.</p> <p>Parameters:</p> Name Type Description Default <code>sumstats</code> <code>DataFrame</code> <p>Summary statistics DataFrame containing Z scores</p> required <code>ldgm_metadata_path</code> <code>str</code> <p>Path to metadata CSV file (default 'data/ldgms/metadata.csv')</p> <code>'data/ldgms/metadata.csv'</code> <code>rsq_threshold</code> <code>float</code> <p>r\u00b2 threshold for clumping (default 0.1)</p> <code>0.1</code> <code>chisq_threshold</code> <code>float</code> <p>\u03c7\u00b2 threshold for significance (default 30.0)</p> <code>30.0</code> <code>populations</code> <code>Optional[Union[str, List[str]]]</code> <p>Optional population name(s)</p> <code>None</code> <code>chromosomes</code> <code>Optional[Union[int, List[int]]]</code> <p>Optional chromosome(s)</p> <code>None</code> <code>num_processes</code> <code>Optional[int]</code> <p>Optional number of processes</p> <code>None</code> <code>run_in_serial</code> <code>bool</code> <p>Whether to run in serial mode</p> <code>False</code> <code>z_col</code> <code>str</code> <p>Name of column containing Z scores</p> <code>'Z'</code> <code>match_by_position</code> <code>bool</code> <p>Whether to match SNPs by position instead of ID</p> <code>True</code> <code>variant_id_col</code> <code>str</code> <p>Name of column containing variant IDs if not matching by position</p> <code>'SNP'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with additional column 'is_index' indicating index variants</p> Source code in <code>src/graphld/clumping.py</code> <pre><code>@classmethod\ndef clump(cls,\n        sumstats: pl.DataFrame,\n        ldgm_metadata_path: str = 'data/ldgms/metadata.csv',\n        rsq_threshold: float = 0.1,\n        chisq_threshold: float = 30.0,\n        populations: Optional[Union[str, List[str]]] = None,\n        chromosomes: Optional[Union[int, List[int]]] = None,\n        run_in_serial: bool = False,\n        num_processes: Optional[int] = None,\n        z_col: str = 'Z',\n        match_by_position: bool = True,\n        variant_id_col: str = 'SNP',\n        verbose: bool = False,\n        ) -&gt; pl.DataFrame:\n    \"\"\"Perform LD clumping on summary statistics.\n\n    Args:\n        sumstats: Summary statistics DataFrame containing Z scores\n        ldgm_metadata_path: Path to metadata CSV file (default 'data/ldgms/metadata.csv')\n        rsq_threshold: r\u00b2 threshold for clumping (default 0.1)\n        chisq_threshold: \u03c7\u00b2 threshold for significance (default 30.0)\n        populations: Optional population name(s)\n        chromosomes: Optional chromosome(s)\n        num_processes: Optional number of processes\n        run_in_serial: Whether to run in serial mode\n        z_col: Name of column containing Z scores\n        match_by_position: Whether to match SNPs by position instead of ID\n        variant_id_col: Name of column containing variant IDs if not matching by position\n\n    Returns:\n        DataFrame with additional column 'is_index' indicating index variants\n    \"\"\"\n\n    run_fn = cls.run_serial if run_in_serial else cls.run\n    result = run_fn(ldgm_metadata_path=ldgm_metadata_path,\n            populations=populations,\n            chromosomes=chromosomes,\n            num_processes=num_processes,\n            worker_params=(rsq_threshold, chisq_threshold, z_col, match_by_position, variant_id_col),\n            sumstats=sumstats)\n\n    if verbose:\n        print(f\"Number of variants in summary statistics: {len(result)}\")\n        nonzero_count = (result['is_index']).sum()\n        print(f\"Number of index variants: {nonzero_count}\")\n\n    return result\n</code></pre>"},{"location":"api/clumping/#graphld.clumping.run_clump","title":"run_clump","text":"<pre><code>run_clump(*args, **kwargs)\n</code></pre> <p>Perform LD-based clumping on summary statistics.</p> <p>Parameters:</p> Name Type Description Default <code>ldgm_metadata_path</code> <code>str</code> <p>Path to LDGM metadata file</p> required <code>sumstats</code> <code>DataFrame</code> <p>Summary statistics DataFrame</p> required <code>min_chisq</code> <code>float</code> <p>Minimum chi-squared threshold. Defaults to 5.0.</p> required <code>max_rsq</code> <code>float</code> <p>Maximum R-squared threshold. Defaults to 0.1.</p> required <code>num_processes</code> <code>Optional[int]</code> <p>Number of processes for parallel computation. Defaults to None.</p> required <code>run_in_serial</code> <code>bool</code> <p>Whether to run in serial mode. Defaults to False.</p> required <code>chromosome</code> <code>Optional[int]</code> <p>Chromosome to filter. Defaults to None.</p> required <code>population</code> <code>Optional[str]</code> <p>Population to filter. Defaults to None.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print verbose output. Defaults to False.</p> required <code>quiet</code> <code>bool</code> <p>Whether to suppress all output except errors. Defaults to False.</p> required <p>Returns:</p> Type Description <p>pl.DataFrame: DataFrame with clumped summary statistics and index variant information</p> Source code in <code>src/graphld/clumping.py</code> <pre><code>def run_clump(*args, **kwargs):\n    \"\"\"\n    Perform LD-based clumping on summary statistics.\n\n    Args:\n        ldgm_metadata_path (str): Path to LDGM metadata file\n        sumstats (pl.DataFrame): Summary statistics DataFrame\n        min_chisq (float, optional): Minimum chi-squared threshold. Defaults to 5.0.\n        max_rsq (float, optional): Maximum R-squared threshold. Defaults to 0.1.\n        num_processes (Optional[int], optional): Number of processes for parallel computation. Defaults to None.\n        run_in_serial (bool, optional): Whether to run in serial mode. Defaults to False.\n        chromosome (Optional[int], optional): Chromosome to filter. Defaults to None.\n        population (Optional[str], optional): Population to filter. Defaults to None.\n        verbose (bool, optional): Whether to print verbose output. Defaults to False.\n        quiet (bool, optional): Whether to suppress all output except errors. Defaults to False.\n\n    Returns:\n        pl.DataFrame: DataFrame with clumped summary statistics and index variant information\n    \"\"\"\n    return LDClumper.clump(*args, **kwargs)\n</code></pre>"},{"location":"api/genesets/","title":"graphld.genesets","text":"<p>Gene set annotation utilities.</p>"},{"location":"api/genesets/#graphld.genesets","title":"genesets","text":"<p>Gene set annotation utilities for graphREML.</p> <p>This module provides functions to load gene sets from GMT files and convert gene-level annotations to variant-level annotations for use with graphREML.</p>"},{"location":"api/genesets/#graphld.genesets.load_gene_table","title":"load_gene_table","text":"<pre><code>load_gene_table(gene_table_path: str, chromosomes: Optional[list[int]] = None) -&gt; pl.DataFrame\n</code></pre> <p>Load gene table and optionally filter to specific chromosomes.</p> <p>Parameters:</p> Name Type Description Default <code>gene_table_path</code> <code>str</code> <p>Path to gene table TSV file with columns: gene_id, gene_id_version, gene_name, start, end, CHR</p> required <code>chromosomes</code> <code>Optional[list[int]]</code> <p>Optional list of chromosome numbers to filter to</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Gene table DataFrame with POS column set to gene midpoint</p> Source code in <code>src/graphld/genesets.py</code> <pre><code>def load_gene_table(\n    gene_table_path: str, chromosomes: Optional[list[int]] = None\n) -&gt; pl.DataFrame:\n    \"\"\"Load gene table and optionally filter to specific chromosomes.\n\n    Args:\n        gene_table_path: Path to gene table TSV file with columns:\n            gene_id, gene_id_version, gene_name, start, end, CHR\n        chromosomes: Optional list of chromosome numbers to filter to\n\n    Returns:\n        Gene table DataFrame with POS column set to gene midpoint\n    \"\"\"\n    schema = {\n        \"gene_id\": pl.Utf8,\n        \"gene_id_version\": pl.Utf8,\n        \"gene_name\": pl.Utf8,\n        \"start\": pl.Int64,\n        \"end\": pl.Int64,\n        \"CHR\": pl.Utf8,\n    }\n    gene_table = (\n        pl.scan_csv(gene_table_path, schema=schema, separator=\"\\t\", has_header=True)\n        .filter(pl.col(\"CHR\").is_in([str(i) for i in range(1, 23)]))\n        .filter(pl.col(\"gene_id\").is_not_null())\n        .with_columns(pl.col(\"gene_name\").fill_null(\"NA\"))\n        .with_columns(((pl.col(\"start\") + pl.col(\"end\")) / 2).alias(\"midpoint\"))\n        .with_columns(pl.col(\"midpoint\").alias(\"POS\"))\n        .sort(pl.col(\"CHR\").cast(pl.Int64), \"midpoint\")\n        .collect()\n    )\n\n    if chromosomes:\n        # Convert chromosomes to integers if they're strings\n        if isinstance(chromosomes[0], str):\n            chromosomes = [int(c) for c in chromosomes if c.isdigit()]\n        gene_table = gene_table.filter(pl.col(\"CHR\").cast(pl.Int64).is_in(chromosomes))\n\n    # Add POS column (using midpoint) for compatibility with position-based functions\n    gene_table = gene_table.with_columns(pl.col(\"midpoint\").cast(pl.Int64).alias(\"POS\"))\n\n    return gene_table\n</code></pre>"},{"location":"api/genesets/#graphld.genesets.load_gene_sets_from_gmt","title":"load_gene_sets_from_gmt","text":"<pre><code>load_gene_sets_from_gmt(gene_annot_dir: str) -&gt; dict[str, list[str]]\n</code></pre> <p>Load gene sets from GMT files in a directory.</p> <p>GMT format: set_namedescriptiongene1gene2... <p>Parameters:</p> Name Type Description Default <code>gene_annot_dir</code> <code>str</code> <p>Directory containing .gmt files</p> required <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>Dictionary mapping set names to lists of genes</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If no .gmt files are found in the directory</p> Source code in <code>src/graphld/genesets.py</code> <pre><code>def load_gene_sets_from_gmt(gene_annot_dir: str) -&gt; dict[str, list[str]]:\n    \"\"\"Load gene sets from GMT files in a directory.\n\n    GMT format: set_name&lt;tab&gt;description&lt;tab&gt;gene1&lt;tab&gt;gene2&lt;tab&gt;...\n\n    Args:\n        gene_annot_dir: Directory containing .gmt files\n\n    Returns:\n        Dictionary mapping set names to lists of genes\n\n    Raises:\n        FileNotFoundError: If no .gmt files are found in the directory\n    \"\"\"\n    gmt_files = glob.glob(str(Path(gene_annot_dir) / \"*.gmt\"))\n    if not gmt_files:\n        raise FileNotFoundError(f\"No .gmt files found in {gene_annot_dir}\")\n\n    gene_sets = {}\n    for gmt_file in gmt_files:\n        with open(gmt_file) as f:\n            for line in f:\n                parts = line.strip().split(\"\\t\")\n                if len(parts) &gt;= 3:\n                    set_name = parts[0]\n                    genes = parts[2:]  # Skip description\n                    gene_sets[set_name] = genes\n\n    return gene_sets\n</code></pre>"},{"location":"api/genesets/#graphld.genesets.convert_gene_sets_to_variant_annotations","title":"convert_gene_sets_to_variant_annotations","text":"<pre><code>convert_gene_sets_to_variant_annotations(gene_sets: dict[str, list[str]], variant_table: DataFrame, gene_table: DataFrame, nearest_weights: ndarray) -&gt; pl.DataFrame\n</code></pre> <p>Convert gene sets to variant-level annotations.</p> <p>Parameters:</p> Name Type Description Default <code>gene_sets</code> <code>dict[str, list[str]]</code> <p>Dictionary mapping gene set names to lists of genes (symbols or IDs)</p> required <code>variant_table</code> <code>DataFrame</code> <p>Variant table DataFrame with CHR, POS, SNP columns</p> required <code>gene_table</code> <code>DataFrame</code> <p>Gene table DataFrame with CHR, POS, gene_id, gene_name columns</p> required <code>nearest_weights</code> <code>ndarray</code> <p>Weights for k-nearest genes (e.g., [0.4, 0.2, 0.1, 0.1, 0.1, 0.05, 0.05])</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with variant-level annotations in LDSC format (CHR, BP, SNP, CM, ...)</p> Source code in <code>src/graphld/genesets.py</code> <pre><code>def convert_gene_sets_to_variant_annotations(\n    gene_sets: dict[str, list[str]],\n    variant_table: pl.DataFrame,\n    gene_table: pl.DataFrame,\n    nearest_weights: np.ndarray,\n) -&gt; pl.DataFrame:\n    \"\"\"Convert gene sets to variant-level annotations.\n\n    Args:\n        gene_sets: Dictionary mapping gene set names to lists of genes (symbols or IDs)\n        variant_table: Variant table DataFrame with CHR, POS, SNP columns\n        gene_table: Gene table DataFrame with CHR, POS, gene_id, gene_name columns\n        nearest_weights: Weights for k-nearest genes (e.g., [0.4, 0.2, 0.1, 0.1, 0.1, 0.05, 0.05])\n\n    Returns:\n        DataFrame with variant-level annotations in LDSC format (CHR, BP, SNP, CM, ...)\n    \"\"\"\n    # Determine if using gene IDs or symbols\n    first_set = next(iter(gene_sets.values()))\n    use_gene_id = _is_gene_id(first_set[0]) if first_set else False\n    gene_key = \"gene_id\" if use_gene_id else \"gene_name\"\n\n    # Get gene-variant matrix\n    variant_positions = _compute_positions(variant_table)\n    gene_positions = _compute_positions(gene_table)\n    gv_matrix = _get_gene_variant_matrix(variant_positions, gene_positions, nearest_weights)\n\n    # Convert each gene set to variant-level annotation\n    variant_annots = {}\n    gene_identifiers = gene_table[gene_key].to_list()\n\n    for set_name, genes in gene_sets.items():\n        gene_set = set(genes)\n        gene_values = np.array(\n            [1.0 if gene in gene_set else 0.0 for gene in gene_identifiers], dtype=np.float64\n        )\n        variant_values = (gv_matrix @ gene_values.reshape(-1, 1)).ravel()\n        variant_annots[set_name] = variant_values\n\n    # Create output DataFrame in LDSC format\n    df_annot = pl.DataFrame(\n        {\n            \"CHR\": variant_table[\"CHR\"],\n            \"BP\": variant_table[\"POS\"],\n            \"SNP\": variant_table[\"SNP\"],\n            \"CM\": pl.Series([0.0] * len(variant_table)),\n            **variant_annots,\n        }\n    )\n\n    return df_annot\n</code></pre>"},{"location":"api/genesets/#graphld.genesets.load_gene_annotations","title":"load_gene_annotations","text":"<pre><code>load_gene_annotations(gene_annot_dir: str, variant_table: DataFrame, gene_table_path: str, nearest_weights: ndarray, annot_names: Optional[list[str]] = None) -&gt; pl.DataFrame\n</code></pre> <p>Load gene-level annotations and convert to variant-level.</p> <p>This is the main entry point for loading GMT-based gene annotations for use with graphREML.</p> <p>Parameters:</p> Name Type Description Default <code>gene_annot_dir</code> <code>str</code> <p>Directory containing GMT files with gene sets</p> required <code>variant_table</code> <code>DataFrame</code> <p>Variant table DataFrame with CHR, POS, SNP columns</p> required <code>gene_table_path</code> <code>str</code> <p>Path to gene table TSV file</p> required <code>nearest_weights</code> <code>ndarray</code> <p>Weights for k-nearest genes</p> required <code>annot_names</code> <code>Optional[list[str]]</code> <p>Optional list of specific annotation names to load</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with variant-level annotations</p> Source code in <code>src/graphld/genesets.py</code> <pre><code>def load_gene_annotations(\n    gene_annot_dir: str,\n    variant_table: pl.DataFrame,\n    gene_table_path: str,\n    nearest_weights: np.ndarray,\n    annot_names: Optional[list[str]] = None,\n) -&gt; pl.DataFrame:\n    \"\"\"Load gene-level annotations and convert to variant-level.\n\n    This is the main entry point for loading GMT-based gene annotations\n    for use with graphREML.\n\n    Args:\n        gene_annot_dir: Directory containing GMT files with gene sets\n        variant_table: Variant table DataFrame with CHR, POS, SNP columns\n        gene_table_path: Path to gene table TSV file\n        nearest_weights: Weights for k-nearest genes\n        annot_names: Optional list of specific annotation names to load\n\n    Returns:\n        DataFrame with variant-level annotations\n    \"\"\"\n    chromosomes = variant_table[\"CHR\"].unique().sort().to_list()\n    gene_table = load_gene_table(gene_table_path, chromosomes)\n    gene_sets = load_gene_sets_from_gmt(gene_annot_dir)\n\n    if annot_names:\n        gene_sets = {name: genes for name, genes in gene_sets.items() if name in annot_names}\n\n    return convert_gene_sets_to_variant_annotations(\n        gene_sets, variant_table, gene_table, nearest_weights\n    )\n</code></pre>"},{"location":"api/graphld/","title":"graphld","text":"<p>Main module providing the public API for GraphLD.</p>"},{"location":"api/graphld/#graphld","title":"graphld","text":"<p>GraphLD: Graph-based linkage disequilibrium matrix operations.</p> <p>This package provides efficient LD matrix operations using LDGM precision matrices, heritability estimation with graphREML, and utilities for GWAS analysis.</p> Main Features <ul> <li>Heritability estimation with graphREML</li> <li>Efficient LD matrix operations via precision matrices</li> <li>GWAS summary statistics simulation</li> <li>LD clumping for polygenic scores</li> <li>BLUP effect size estimation</li> </ul> <p>Example::</p> <pre><code>import graphld as gld\n\nsumstats = gld.read_ldsc_sumstats(\"path/to/sumstats.sumstats\")\nresults = gld.run_graphREML(\n    model_options=gld.ModelOptions(),\n    method_options=gld.MethodOptions(),\n    summary_stats=sumstats,\n    annotation_data=gld.load_annotations(\"path/to/annot/\", chromosome=1),\n    ldgm_metadata_path=\"path/to/metadata.csv\",\n    populations=\"EUR\"\n)\n</code></pre>"},{"location":"api/graphld/#graphld.BLUP","title":"BLUP","text":"<p>               Bases: <code>ParallelProcessor</code></p> <p>Computes the best linear unbiased predictor using LDGMs and GWAS summary statistics.</p>"},{"location":"api/graphld/#graphld.BLUP.compute_blup","title":"compute_blup  <code>classmethod</code>","text":"<pre><code>compute_blup(ldgm_metadata_path: str, sumstats: DataFrame, sigmasq: float, sample_size: float, populations: Optional[Union[str, List[str]]] = None, chromosomes: Optional[Union[int, List[int]]] = None, num_processes: Optional[int] = None, run_in_serial: bool = False, match_by_position: bool = False, verbose: bool = False) -&gt; pl.DataFrame\n</code></pre> <p>Simulate GWAS summary statistics for multiple LD blocks.</p> <p>Parameters:</p> Name Type Description Default <code>ldgm_metadata_path</code> <code>str</code> <p>Path to metadata CSV file</p> required <code>sumstats</code> <code>DataFrame</code> <p>Sumstats dataframe containing Z scores</p> required <code>populations</code> <code>Optional[Union[str, List[str]]]</code> <p>Optional population name</p> <code>None</code> <code>chromosomes</code> <code>Optional[Union[int, List[int]]]</code> <p>Optional chromosome or list of chromosomes</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Print additional information if True</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Array of BLUP effect sizes, same length as sumstats</p>"},{"location":"api/graphld/#graphld.BLUP.create_shared_memory","title":"create_shared_memory  <code>staticmethod</code>","text":"<pre><code>create_shared_memory(metadata: DataFrame, block_data: list[tuple], **kwargs) -&gt; SharedData\n</code></pre> <p>Create output array with length number of variants in the summary statistics that  migtht match to one of the blocks.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>DataFrame</code> <p>Metadata DataFrame containing block information</p> required <code>block_data</code> <code>list[tuple]</code> <p>List of block-specific sumstats DataFrames</p> required <code>**kwargs</code> <p>Not used</p> <code>{}</code>"},{"location":"api/graphld/#graphld.BLUP.prepare_block_data","title":"prepare_block_data  <code>classmethod</code>","text":"<pre><code>prepare_block_data(metadata: DataFrame, **kwargs) -&gt; list[tuple]\n</code></pre> <p>Split summary statistics into blocks whose positions match the LDGMs.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>DataFrame</code> <p>DataFrame containing LDGM metadata</p> required <code>**kwargs</code> <p>Additional arguments from run(), including: annotations: Optional DataFrame containing variant annotations</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[tuple]</code> <p>List of block-specific annotation DataFrames, or None if no annotations</p>"},{"location":"api/graphld/#graphld.BLUP.process_block","title":"process_block  <code>classmethod</code>","text":"<pre><code>process_block(ldgm: PrecisionOperator, flag: Value, shared_data: SharedData, block_offset: int, block_data: tuple, worker_params: tuple) -&gt; None\n</code></pre> <p>Run BLUP on a single block.</p>"},{"location":"api/graphld/#graphld.BLUP.supervise","title":"supervise  <code>classmethod</code>","text":"<pre><code>supervise(manager: WorkerManager, shared_data: Dict[str, Any], block_data: list, **kwargs) -&gt; pl.DataFrame\n</code></pre> <p>Supervise worker processes and collect results.</p> <p>Parameters:</p> Name Type Description Default <code>manager</code> <code>WorkerManager</code> <p>Worker manager</p> required <code>shared_data</code> <code>Dict[str, Any]</code> <p>Dictionary of shared memory arrays</p> required <code>**kwargs</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing simulated summary statistics</p>"},{"location":"api/graphld/#graphld.LDClumper","title":"LDClumper","text":"<p>               Bases: <code>ParallelProcessor</code></p> <p>Fast LD clumping to find unlinked lead SNPs from GWAS summary statistics.</p>"},{"location":"api/graphld/#graphld.LDClumper.clump","title":"clump  <code>classmethod</code>","text":"<pre><code>clump(sumstats: DataFrame, ldgm_metadata_path: str = 'data/ldgms/metadata.csv', rsq_threshold: float = 0.1, chisq_threshold: float = 30.0, populations: Optional[Union[str, List[str]]] = None, chromosomes: Optional[Union[int, List[int]]] = None, run_in_serial: bool = False, num_processes: Optional[int] = None, z_col: str = 'Z', match_by_position: bool = True, variant_id_col: str = 'SNP', verbose: bool = False) -&gt; pl.DataFrame\n</code></pre> <p>Perform LD clumping on summary statistics.</p> <p>Parameters:</p> Name Type Description Default <code>sumstats</code> <code>DataFrame</code> <p>Summary statistics DataFrame containing Z scores</p> required <code>ldgm_metadata_path</code> <code>str</code> <p>Path to metadata CSV file (default 'data/ldgms/metadata.csv')</p> <code>'data/ldgms/metadata.csv'</code> <code>rsq_threshold</code> <code>float</code> <p>r\u00b2 threshold for clumping (default 0.1)</p> <code>0.1</code> <code>chisq_threshold</code> <code>float</code> <p>\u03c7\u00b2 threshold for significance (default 30.0)</p> <code>30.0</code> <code>populations</code> <code>Optional[Union[str, List[str]]]</code> <p>Optional population name(s)</p> <code>None</code> <code>chromosomes</code> <code>Optional[Union[int, List[int]]]</code> <p>Optional chromosome(s)</p> <code>None</code> <code>num_processes</code> <code>Optional[int]</code> <p>Optional number of processes</p> <code>None</code> <code>run_in_serial</code> <code>bool</code> <p>Whether to run in serial mode</p> <code>False</code> <code>z_col</code> <code>str</code> <p>Name of column containing Z scores</p> <code>'Z'</code> <code>match_by_position</code> <code>bool</code> <p>Whether to match SNPs by position instead of ID</p> <code>True</code> <code>variant_id_col</code> <code>str</code> <p>Name of column containing variant IDs if not matching by position</p> <code>'SNP'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with additional column 'is_index' indicating index variants</p>"},{"location":"api/graphld/#graphld.LDClumper.create_shared_memory","title":"create_shared_memory  <code>staticmethod</code>","text":"<pre><code>create_shared_memory(metadata: DataFrame, block_data: list[tuple], **kwargs) -&gt; SharedData\n</code></pre> <p>Create output array with length number of variants in the summary statistics.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>DataFrame</code> <p>Metadata DataFrame containing block information</p> required <code>block_data</code> <code>list[tuple]</code> <p>List of block-specific sumstats DataFrames</p> required <code>**kwargs</code> <p>Not used</p> <code>{}</code> <p>Returns:</p> Type Description <code>SharedData</code> <p>SharedData containing arrays for clumping results</p>"},{"location":"api/graphld/#graphld.LDClumper.prepare_block_data","title":"prepare_block_data  <code>classmethod</code>","text":"<pre><code>prepare_block_data(metadata: DataFrame, **kwargs) -&gt; list[tuple]\n</code></pre> <p>Split summary statistics into blocks whose positions match the LDGMs.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>DataFrame</code> <p>DataFrame containing LDGM metadata</p> required <code>**kwargs</code> <p>Additional arguments from run(), including: sumstats: DataFrame containing summary statistics</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[tuple]</code> <p>List of block-specific sumstats DataFrames and their offsets</p>"},{"location":"api/graphld/#graphld.LDClumper.process_block","title":"process_block  <code>classmethod</code>","text":"<pre><code>process_block(ldgm: PrecisionOperator, flag: Value, shared_data: SharedData, block_offset: int, block_data: tuple, worker_params: tuple) -&gt; None\n</code></pre> <p>Process single block for LD clumping.</p> <p>Parameters:</p> Name Type Description Default <code>ldgm</code> <code>PrecisionOperator</code> <p>LDGM object</p> required <code>flag</code> <code>Value</code> <p>Worker flag</p> required <code>shared_data</code> <code>SharedData</code> <p>Dictionary-like shared data object</p> required <code>block_offset</code> <code>int</code> <p>Offset for this block</p> required <code>block_data</code> <code>tuple</code> <p>Tuple of (sumstats DataFrame, variant_offset)</p> required <code>worker_params</code> <code>tuple</code> <p>Tuple of (rsq_threshold, chisq_threshold, z_col, match_by_position, variant_id_col)</p> required"},{"location":"api/graphld/#graphld.LDClumper.supervise","title":"supervise  <code>classmethod</code>","text":"<pre><code>supervise(manager: Union[WorkerManager, SerialManager], shared_data: SharedData, block_data: list, **kwargs) -&gt; pl.DataFrame\n</code></pre> <p>Monitor workers and process results.</p> <p>Parameters:</p> Name Type Description Default <code>manager</code> <code>Union[WorkerManager, SerialManager]</code> <p>Worker manager for controlling processes</p> required <code>shared_data</code> <code>SharedData</code> <p>Shared memory data</p> required <code>block_data</code> <code>list</code> <p>List of block-specific data</p> required <code>**kwargs</code> <p>Additional arguments passed from run()</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with clumping results</p>"},{"location":"api/graphld/#graphld.MethodOptions","title":"MethodOptions  <code>dataclass</code>","text":"<pre><code>MethodOptions(gradient_num_samples: int = 100, gradient_seed: Optional[int] = 123, match_by_position: bool = False, num_iterations: int = 50, convergence_tol: float = 0.01, run_serial: bool = False, num_processes: Optional[int] = None, verbose: bool = False, use_surrogate_markers: bool = True, trust_region_size: float = 0.1, trust_region_rho_lb: float = 0.0001, trust_region_rho_ub: float = 0.99, trust_region_scalar: float = 5, max_trust_iterations: int = 100, minimum_likelihood_increase: float = 1e-06, convergence_window: int = 3, reset_trust_region: bool = False, num_jackknife_blocks: int = 100, max_chisq_threshold: Optional[float] = None, score_test_hdf5_file_name: Optional[str] = None, score_test_hdf5_trait_name: Optional[str] = None, surrogate_markers_path: Optional[str] = None)\n</code></pre> <p>Stores method parameters for graphREML.</p> <p>Attributes:</p> Name Type Description <code>gradient_num_samples</code> <code>int</code> <p>Number of samples for gradient estimation</p> <code>match_by_position</code> <code>bool</code> <p>Use position/allele instead of RSID for merging</p> <code>num_iterations</code> <code>int</code> <p>Optimization steps</p> <code>convergence_tol</code> <code>float</code> <p>Convergence tolerance</p> <code>run_serial</code> <code>bool</code> <p>Run in serial rather than parallel</p> <code>num_processes</code> <code>Optional[int]</code> <p>If None, autodetect</p> <code>verbose</code> <code>bool</code> <p>Flag for verbose output</p> <code>use_surrogate_markers</code> <code>bool</code> <p>Whether to use surrogate markers for missing variants</p> <code>trust_region_size</code> <code>float</code> <p>Initial trust region size parameter</p> <code>trust_region_rho_lb</code> <code>float</code> <p>Lower bound for trust region ratio</p> <code>trust_region_rho_ub</code> <code>float</code> <p>Upper bound for trust region ratio</p> <code>trust_region_scalar</code> <code>float</code> <p>Scaling factor for trust region updates</p> <code>max_trust_iterations</code> <code>int</code> <p>Maximum number of trust region iterations</p> <code>reset_trust_region</code> <code>bool</code> <p>Whether to reset trust region size at each iteration</p> <code>num_jackknife_blocks</code> <code>int</code> <p>Number of blocks to use for jackknife estimation</p> <code>max_chisq_threshold</code> <code>Optional[float]</code> <p>Maximum allowed chi^2 value in a block. Blocks with chi^2 &gt; threshold are excluded.</p> <code>score_test_hdf5_file_name</code> <code>Optional[str]</code> <p>Optional file name to create or append to an hdf5 file with pre-computed derivatives for the score test.</p> <code>score_test_hdf5_trait_name</code> <code>Optional[str]</code> <p>Name of the trait's subdirectory within the score test HDF5 file.</p> <code>surrogate_markers_path</code> <code>Optional[str]</code> <p>Optional path to an HDF5 file with per-block surrogate mappings.</p>"},{"location":"api/graphld/#graphld.ModelOptions","title":"ModelOptions  <code>dataclass</code>","text":"<pre><code>ModelOptions(annotation_columns: Optional[List[str]] = None, params: Optional[ndarray] = None, sample_size: Optional[float] = None, intercept: float = 1.0, link_fn_denominator: float = 6000000.0, binary_annotations_only: bool = False)\n</code></pre> <p>Stores model parameters for graphREML.</p> <p>Attributes:</p> Name Type Description <code>annotation_columns</code> <code>Optional[List[str]]</code> <p>names of columns to be used as annotations</p> <code>params</code> <code>Optional[ndarray]</code> <p>Starting parameter values</p> <code>sample_size</code> <code>Optional[float]</code> <p>GWAS sample size, only needed for heritability scaling</p> <code>intercept</code> <code>float</code> <p>LDSC intercept or 1</p> <code>link_fn_denominator</code> <code>float</code> <p>Scalar denominator for link function.</p>"},{"location":"api/graphld/#graphld.ParallelProcessor","title":"ParallelProcessor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for parallel processing applications.</p> <p>This class provides a framework for parallel processing of LDGM files. Subclasses must implement the following methods:     - initialize: Set up shared memory arrays and data structures     - supervise: Monitor and control worker processes     - process_block: Process a single LDGM block</p>"},{"location":"api/graphld/#graphld.ParallelProcessor.create_shared_memory","title":"create_shared_memory  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>create_shared_memory(metadata: DataFrame, block_data: list, **kwargs) -&gt; SharedData\n</code></pre> <p>Initialize shared memory and data structures.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>DataFrame</code> <p>polars dataframe continaing LDGM metadata for each LD block</p> required <code>block_data</code> <code>list</code> <p>List of block-specific data</p> required <code>**kwargs</code> <p>Additional arguments passed from run()</p> <code>{}</code> <p>Returns:</p> Type Description <code>SharedData</code> <p>SharedData object containing shared memory arrays</p>"},{"location":"api/graphld/#graphld.ParallelProcessor.prepare_block_data","title":"prepare_block_data  <code>classmethod</code>","text":"<pre><code>prepare_block_data(metadata: DataFrame, **kwargs) -&gt; list\n</code></pre> <p>Prepare data specific to each block for processing.</p> <p>This method should return a list of length equal to the number of blocks, where each element contains any block-specific data needed by process_block. The base implementation returns None for each block.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>DataFrame</code> <p>Metadata DataFrame containing block information</p> required <code>**kwargs</code> <p>Additional arguments passed from run()</p> <code>{}</code> <p>Returns:</p> Type Description <code>list</code> <p>List of block-specific data, length equal to number of blocks</p>"},{"location":"api/graphld/#graphld.ParallelProcessor.process_block","title":"process_block  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>process_block(ldgm: PrecisionOperator, flag: Value, shared_data: SharedData, block_offset: int, block_data: Any = None, worker_params: Any = None) -&gt; None\n</code></pre> <p>Process single block.</p> <p>Parameters:</p> Name Type Description Default <code>ldgm</code> <code>PrecisionOperator</code> <p>LDGM object</p> required <code>flag</code> <code>Value</code> <p>Worker flag</p> required <code>shared_data</code> <code>SharedData</code> <p>Dictionary-like shared data object</p> required <code>block_offset</code> <code>int</code> <p>Offset for this block</p> required <code>block_data</code> <code>Any</code> <p>Optional block-specific data from prepare_block_data</p> <code>None</code> <code>worker_params</code> <code>Any</code> <p>Optional parameters passed to each worker process</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api/graphld/#graphld.ParallelProcessor.run","title":"run  <code>classmethod</code>","text":"<pre><code>run(ldgm_metadata_path: str, populations: Optional[Union[str, List[str]]] = None, chromosomes: Optional[Union[int, List[int]]] = None, num_processes: Optional[int] = None, worker_params: Any = None, **kwargs) -&gt; Any\n</code></pre> <p>Run parallel computation.</p> <p>Parameters:</p> Name Type Description Default <code>ldgm_metadata_path</code> <code>str</code> <p>Path to metadata file</p> required <code>populations</code> <code>Optional[Union[str, List[str]]]</code> <p>Populations to process; None -&gt; all</p> <code>None</code> <code>chromosomes</code> <code>Optional[Union[int, List[int]]]</code> <p>Chromosomes to process; None -&gt; all</p> <code>None</code> <code>num_processes</code> <code>Optional[int]</code> <p>Number of processes to use</p> <code>None</code> <code>worker_params</code> <code>Any</code> <p>Optional parameters passed to each worker process</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Results of the parallel computation</p>"},{"location":"api/graphld/#graphld.ParallelProcessor.run_serial","title":"run_serial  <code>classmethod</code>","text":"<pre><code>run_serial(ldgm_metadata_path: str, populations: Optional[Union[str, List[str]]] = None, chromosomes: Optional[Union[int, List[int]]] = None, num_processes: Optional[int] = None, worker_params: Any = None, **kwargs) -&gt; Any\n</code></pre> <p>Run parallel computation.</p> <p>Parameters:</p> Name Type Description Default <code>ldgm_metadata_path</code> <code>str</code> <p>Path to metadata file</p> required <code>populations</code> <code>Optional[Union[str, List[str]]]</code> <p>Populations to process; None -&gt; all</p> <code>None</code> <code>chromosomes</code> <code>Optional[Union[int, List[int]]]</code> <p>Chromosomes to process; None -&gt; all</p> <code>None</code> <code>num_processes</code> <code>Optional[int]</code> <p>Number of processes to use</p> <code>None</code> <code>worker_params</code> <code>Any</code> <p>Optional parameters passed to each worker process</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Results of the parallel computation</p>"},{"location":"api/graphld/#graphld.ParallelProcessor.serial_worker","title":"serial_worker  <code>classmethod</code>","text":"<pre><code>serial_worker(ldgm: Optional[PrecisionOperator], offset: int, file: str, data: list, flag: Value, shared_data: SharedData, worker_params: Any) -&gt; PrecisionOperator\n</code></pre> <p>Worker process that loads LDGMs and processes blocks in serial.</p> <p>Parameters:</p> Name Type Description Default <code>ldgm</code> <code>Optional[PrecisionOperator]</code> <p>the LDGM that was previously loaded, or None</p> required <code>offset</code> <code>int</code> <p>In shared data, where to start processing</p> required <code>files</code> <p>List of LDGM files to process</p> required <code>data</code> <code>list</code> <p>List of block-specific data</p> required <code>flag</code> <code>Value</code> <p>Shared flag for worker control</p> required <code>shared_data</code> <code>SharedData</code> <p>Shared memory data</p> required <code>worker_params</code> <code>Any</code> <p>Optional parameters passed to process_block</p> required"},{"location":"api/graphld/#graphld.ParallelProcessor.supervise","title":"supervise  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>supervise(manager: Union[WorkerManager, SerialManager], shared_data: SharedData, block_data: list, **kwargs) -&gt; Any\n</code></pre> <p>Monitor workers and process results.</p> <p>Parameters:</p> Name Type Description Default <code>manager</code> <code>Union[WorkerManager, SerialManager]</code> <p>Worker manager for controlling processes</p> required <code>shared_data</code> <code>SharedData</code> <p>Shared memory data</p> required <code>**kwargs</code> <p>Additional arguments passed from run()</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Results of the parallel computation</p>"},{"location":"api/graphld/#graphld.ParallelProcessor.worker","title":"worker  <code>classmethod</code>","text":"<pre><code>worker(files: list, block_data: list, flag: Value, shared_data: SharedData, offset: int, worker_params: Any = None) -&gt; None\n</code></pre> <p>Worker process that loads LDGMs and processes blocks.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>list</code> <p>List of LDGM files to process</p> required <code>block_data</code> <code>list</code> <p>List of block-specific data</p> required <code>flag</code> <code>Value</code> <p>Shared flag for worker control</p> required <code>shared_data</code> <code>SharedData</code> <p>Shared memory data</p> required <code>offset</code> <code>int</code> <p>In shared data, where to start processing</p> required <code>worker_params</code> <code>Any</code> <p>Optional parameters passed to process_block</p> <code>None</code>"},{"location":"api/graphld/#graphld.PrecisionOperator","title":"PrecisionOperator  <code>dataclass</code>","text":"<pre><code>PrecisionOperator(_matrix: csc_matrix, variant_info: DataFrame, _which_indices: Optional[ndarray] = None, _solver: Optional[cholesky] = None, _cholesky_is_up_to_date: bool = False)\n</code></pre> <p>               Bases: <code>LinearOperator</code></p> <p>LDGM precision matrix class implementing the LinearOperator interface.</p> <p>This class provides an efficient implementation of precision matrix operations using the scipy.sparse.linalg.LinearOperator interface. It supports matrix-vector multiplication and other essential operations for working with LDGM precision matrices.</p> <p>Attributes:</p> Name Type Description <code>_matrix</code> <code>csc_matrix</code> <p>The precision matrix in sparse format</p> <code>variant_info</code> <code>DataFrame</code> <p>Polars DataFrame containing variant information</p> <code>_which_indices</code> <code>Optional[ndarray]</code> <p>Array of indices for current selection</p> <code>_solver</code> <code>Optional[cholesky]</code> <p>Previously computed Cholesky factorization</p> <code>_cholesky_is_up_to_date</code> <code>bool</code> <p>Flag indicating whether the Cholesky factorization is up to date</p>"},{"location":"api/graphld/#graphld.PrecisionOperator.diagonal_indices","title":"diagonal_indices  <code>cached</code> <code>property</code>","text":"<pre><code>diagonal_indices: ndarray\n</code></pre> <p>Get indices of diagonal elements corresponding to _which_indices in _matrix.data.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of indices into self._matrix.data where diagonal elements are stored</p>"},{"location":"api/graphld/#graphld.PrecisionOperator.dtype","title":"dtype  <code>property</code>","text":"<pre><code>dtype: dtype\n</code></pre> <p>Return the dtype of the precision matrix.</p>"},{"location":"api/graphld/#graphld.PrecisionOperator.inv","title":"inv  <code>property</code>","text":"<pre><code>inv: LinearOperator\n</code></pre> <p>A LinearOperator representing the LD correlation matrix.</p>"},{"location":"api/graphld/#graphld.PrecisionOperator.matrix","title":"matrix  <code>property</code>","text":"<pre><code>matrix\n</code></pre> <p>Get the precision matrix.</p>"},{"location":"api/graphld/#graphld.PrecisionOperator.nbytes","title":"nbytes  <code>property</code>","text":"<pre><code>nbytes: int\n</code></pre> <p>Return the total memory usage in bytes.</p>"},{"location":"api/graphld/#graphld.PrecisionOperator.shape","title":"shape  <code>property</code>","text":"<pre><code>shape\n</code></pre> <p>Get the shape of the matrix, accounting for partial indexing.</p>"},{"location":"api/graphld/#graphld.PrecisionOperator.variant_indices","title":"variant_indices  <code>property</code>","text":"<pre><code>variant_indices\n</code></pre> <p>Get the indices of the variants in the precision matrix.</p>"},{"location":"api/graphld/#graphld.PrecisionOperator.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key: Union[list, slice, ndarray]) -&gt; PrecisionOperator\n</code></pre> <p>Sets the _which_indices class attribute.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Union[list, slice, ndarray]</code> <p>Index, slice, or tuple of indices/slices to access. Can be: - List of indices - Array of indices - Boolean mask array - Slice object</p> required <p>Returns:</p> Type Description <code>PrecisionOperator</code> <p>New PrecisionOperator instance that shares the underlying matrix</p>"},{"location":"api/graphld/#graphld.PrecisionOperator.copy","title":"copy","text":"<pre><code>copy()\n</code></pre> <p>Copy the current LDGM instance.</p>"},{"location":"api/graphld/#graphld.PrecisionOperator.del_factor","title":"del_factor","text":"<pre><code>del_factor() -&gt; None\n</code></pre> <p>Free the memory used by the Cholesky factorization.</p>"},{"location":"api/graphld/#graphld.PrecisionOperator.factor","title":"factor","text":"<pre><code>factor() -&gt; None\n</code></pre> <p>Update the Cholesky factorization of the precision matrix.</p>"},{"location":"api/graphld/#graphld.PrecisionOperator.inverse_diagonal","title":"inverse_diagonal","text":"<pre><code>inverse_diagonal(method: str = 'xdiag', initialization: Optional[Tuple[ndarray, ndarray]] = None, n_samples: int = 100, seed: Optional[int] = None) -&gt; np.ndarray\n</code></pre> <p>Compute the diagonal elements of the inverse of the precision matrix.</p> <p>Parameters:</p> Name Type Description Default <code>initialization</code> <code>Optional[Tuple[ndarray, ndarray]]</code> <p>Optional tuple of matrices containing: - Initial guess for the diagonal elements - Initial guess for the off-diagonal elements</p> <code>None</code> <code>method</code> <code>str</code> <p>Method to use for computing diagonal elements ('exact', 'hutchinson', or 'xdiag')</p> <code>'xdiag'</code> <code>n_samples</code> <code>int</code> <p>Number of probe vectors for Hutchinson's method or xdiag</p> <code>100</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for generating probe vectors</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of diagonal elements of the inverse</p>"},{"location":"api/graphld/#graphld.PrecisionOperator.logdet","title":"logdet","text":"<pre><code>logdet() -&gt; float\n</code></pre> <p>Compute log determinant of the Schur complement.</p> <p>Returns:</p> Type Description <code>float</code> <p>Log determinant of the Schur complement</p>"},{"location":"api/graphld/#graphld.PrecisionOperator.set_which_indices","title":"set_which_indices","text":"<pre><code>set_which_indices(key: Union[list, slice, ndarray, int]) -&gt; None\n</code></pre> <p>Sets the _which_indices class attribute.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Union[list, slice, ndarray, int]</code> <p>Index, slice, or tuple of indices/slices to access. Can be: - List of indices - Array of indices - Boolean mask array - Slice object - Integer index</p> required"},{"location":"api/graphld/#graphld.PrecisionOperator.solve","title":"solve","text":"<pre><code>solve(b: ndarray, method: str = 'direct', tol: float = 1e-05, callback: Optional[Callable] = None, initialization: Optional[ndarray] = None) -&gt; np.ndarray\n</code></pre> <p>Solve the linear system Px = b.</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>ndarray</code> <p>Right-hand side vector or matrix</p> required <code>method</code> <code>str</code> <p>Solver method ('direct' or 'pcg')</p> <code>'direct'</code> <code>tol</code> <code>float</code> <p>Tolerance for PCG solver</p> <code>1e-05</code> <code>callback</code> <code>Optional[Callable]</code> <p>Optional callback for PCG solver</p> <code>None</code> <code>initialization</code> <code>Optional[ndarray]</code> <p>Optional initial guess for pcg</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Solution vector x</p>"},{"location":"api/graphld/#graphld.PrecisionOperator.solve_Lt","title":"solve_Lt","text":"<pre><code>solve_Lt(b: ndarray) -&gt; np.ndarray\n</code></pre> <p>Solve the triangular system L'x = b, where LL' = _matrix. If b ~ MVN(0, I), then x ~ MVN(0, R).</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>ndarray</code> <p>Right-hand side vector</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Solution vector x</p>"},{"location":"api/graphld/#graphld.PrecisionOperator.times_scalar","title":"times_scalar","text":"<pre><code>times_scalar(multiplier: float) -&gt; None\n</code></pre> <p>Multiply the precision matrix by a scalar in place.</p>"},{"location":"api/graphld/#graphld.PrecisionOperator.update_element","title":"update_element","text":"<pre><code>update_element(index: int, value: float) -&gt; None\n</code></pre> <p>Update a single diagonal element of the precision matrix.</p> <p>If which_indices is set, only updates the corresponding element.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The updated diagonal element is self._which_indices[index]</p> required <code>value</code> <code>float</code> <p>Value to add to the diagonal element</p> required Note <p>Updates the Cholesky factorization if it exists using a rank-1 update/downdate.</p>"},{"location":"api/graphld/#graphld.PrecisionOperator.update_matrix","title":"update_matrix","text":"<pre><code>update_matrix(update: ndarray) -&gt; None\n</code></pre> <p>Update the precision matrix by adding values to its diagonal.</p> <p>If which_indices is set, only updates the corresponding diagonal elements.</p> <p>Parameters:</p> Name Type Description Default <code>update</code> <code>ndarray</code> <p>Vector of values to add to the diagonal elements</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If update shape doesn't match or would make diagonal non-positive</p>"},{"location":"api/graphld/#graphld.PrecisionOperator.variant_solve","title":"variant_solve","text":"<pre><code>variant_solve(b: ndarray) -&gt; np.ndarray\n</code></pre> <p>Computes correlation_matrix @ b where the dimension is number of variants, not number of indices. If two variants i,j have the same index (and are in perfect LD), b[i] and b[j] are summed, then solve() is called, then the solution vector is assigned the same values in entries i,j.</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>ndarray</code> <p>Right-hand side vector</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Solution vector z</p>"},{"location":"api/graphld/#graphld.SharedData","title":"SharedData","text":"<pre><code>SharedData(sizes: Dict[str, Union[int, None]])\n</code></pre> <p>Wrapper for shared memory data structures.</p> <p>Attributes:</p> Name Type Description <code>_data_dict</code> <p>Dictionary mapping keys to shared memory objects</p> <p>Initialize shared memory objects.</p> <p>Parameters:</p> Name Type Description Default <code>sizes</code> <code>Dict[str, Union[int, None]]</code> <p>Dictionary mapping keys to array sizes.   If size is None, creates a shared float value.</p> required"},{"location":"api/graphld/#graphld.SharedData.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key: Union[str, Tuple[str, slice]]) -&gt; Union[np.ndarray, float]\n</code></pre> <p>Get numpy array view or float value.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Union[str, Tuple[str, slice]]</code> <p>Key in data dictionary or tuple of (key, slice)</p> required <p>Returns:</p> Type Description <code>Union[ndarray, float]</code> <p>numpy array view for Array, float for Value</p>"},{"location":"api/graphld/#graphld.SharedData.__setitem__","title":"__setitem__","text":"<pre><code>__setitem__(key: Union[str, Tuple[str, slice]], value: Union[ndarray, float])\n</code></pre> <p>Set array contents or float value.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Union[str, Tuple[str, slice]]</code> <p>Key in data dictionary or tuple of (key, slice)</p> required <code>value</code> <code>Union[ndarray, float]</code> <p>Array or float to set</p> required"},{"location":"api/graphld/#graphld.Simulate","title":"Simulate  <code>dataclass</code>","text":"<pre><code>Simulate(sample_size: int, heritability: float = 0.5, component_variance: Union[ndarray, List[float]] = None, component_weight: Union[ndarray, List[float]] = None, alpha_param: float = -1, annotation_dependent_polygenicity: bool = False, link_fn: Callable[[ndarray], ndarray] = _default_link_fn, random_seed: Optional[int] = None, annotation_columns: Optional[List[str]] = None)\n</code></pre> <p>               Bases: <code>ParallelProcessor</code>, <code>_SimulationSpecification</code></p> <p>Parallel processor for simulating GWAS summary statistics.</p>"},{"location":"api/graphld/#graphld.Simulate.create_shared_memory","title":"create_shared_memory  <code>staticmethod</code>","text":"<pre><code>create_shared_memory(metadata: DataFrame, block_data: list[tuple], **kwargs) -&gt; SharedData\n</code></pre> <p>Create shared memory arrays for simulation.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>DataFrame</code> <p>Metadata DataFrame containing block information</p> required <code>block_data</code> <code>list[tuple]</code> <p>List of tuples containing block-specific annotation DataFrames</p> required <code>**kwargs</code> <p>Additional keyword arguments</p> <code>{}</code>"},{"location":"api/graphld/#graphld.Simulate.prepare_block_data","title":"prepare_block_data  <code>classmethod</code>","text":"<pre><code>prepare_block_data(metadata: DataFrame, **kwargs) -&gt; list[tuple]\n</code></pre> <p>Prepare block-specific data for processing.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>DataFrame</code> <p>DataFrame containing LDGM metadata</p> required <code>**kwargs</code> <p>Additional arguments from run(), including: annotations: Optional DataFrame containing variant annotations</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[tuple]</code> <p>List of block-specific annotation DataFrames, or None if no annotations</p>"},{"location":"api/graphld/#graphld.Simulate.process_block","title":"process_block  <code>classmethod</code>","text":"<pre><code>process_block(ldgm: PrecisionOperator, flag: Value, shared_data: SharedData, block_offset: int, block_data: Optional[tuple] = None, worker_params: Optional[Dict] = None) -&gt; None\n</code></pre> <p>Process a single block.</p>"},{"location":"api/graphld/#graphld.Simulate.simulate","title":"simulate","text":"<pre><code>simulate(ldgm_metadata_path: str = 'data/ldgms/metadata.csv', populations: Optional[Union[str, List[str]]] = 'EUR', chromosomes: Optional[Union[int, List[int]]] = None, run_in_serial: bool = False, num_processes: Optional[int] = None, annotations: Optional[DataFrame] = None, verbose: bool = False) -&gt; pl.DataFrame\n</code></pre> <p>Simulate genetic data.</p> <p>Parameters:</p> Name Type Description Default <code>ldgm_metadata_path</code> <code>str</code> <p>Path to LDGM metadata file</p> <code>'data/ldgms/metadata.csv'</code> <code>populations</code> <code>Optional[Union[str, List[str]]]</code> <p>Population(s) to filter</p> <code>'EUR'</code> <code>chromosomes</code> <code>Optional[Union[int, List[int]]]</code> <p>Chromosome(s) to filter</p> <code>None</code> <code>run_in_serial</code> <code>bool</code> <p>Whether to run in serial mode</p> <code>False</code> <code>annotations</code> <code>Optional[DataFrame]</code> <p>Optional variant annotations</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Simulated genetic data DataFrame</p>"},{"location":"api/graphld/#graphld.Simulate.supervise","title":"supervise  <code>classmethod</code>","text":"<pre><code>supervise(manager: WorkerManager, shared_data: Dict[str, Any], block_data: list, **kwargs) -&gt; pl.DataFrame\n</code></pre> <p>Supervise worker processes and collect results.</p> <p>Parameters:</p> Name Type Description Default <code>manager</code> <code>WorkerManager</code> <p>Worker manager</p> required <code>shared_data</code> <code>Dict[str, Any]</code> <p>Dictionary of shared memory arrays</p> required <code>**kwargs</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing simulated summary statistics</p>"},{"location":"api/graphld/#graphld.WorkerManager","title":"WorkerManager","text":"<pre><code>WorkerManager(num_processes: int)\n</code></pre> <p>Manager for coordinating parallel worker processes.</p> <p>Attributes:</p> Name Type Description <code>flags</code> <p>List of shared flags for worker control</p> <code>processes</code> <code>List[Process]</code> <p>List of worker processes</p> <p>Initialize worker manager.</p> <p>Parameters:</p> Name Type Description Default <code>num_processes</code> <code>int</code> <p>Number of worker processes to manage</p> required"},{"location":"api/graphld/#graphld.WorkerManager.add_process","title":"add_process","text":"<pre><code>add_process(target: Callable, args: Tuple) -&gt; None\n</code></pre> <p>Add a worker process.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Callable</code> <p>Function to run in process</p> required <code>args</code> <code>Tuple</code> <p>Arguments to pass to function</p> required"},{"location":"api/graphld/#graphld.WorkerManager.await_workers","title":"await_workers","text":"<pre><code>await_workers() -&gt; None\n</code></pre> <p>Wait for all workers to finish current task; abort if a worker crashes.</p>"},{"location":"api/graphld/#graphld.WorkerManager.shutdown","title":"shutdown","text":"<pre><code>shutdown() -&gt; None\n</code></pre> <p>Shutdown all worker processes.</p>"},{"location":"api/graphld/#graphld.WorkerManager.start_workers","title":"start_workers","text":"<pre><code>start_workers(flag: Optional[int] = None) -&gt; None\n</code></pre> <p>Signal workers to start processing.</p> <p>Parameters:</p> Name Type Description Default <code>flag</code> <code>Optional[int]</code> <p>Optional flag value to set (default: 1)</p> <code>None</code>"},{"location":"api/graphld/#graphld.gaussian_likelihood","title":"gaussian_likelihood","text":"<pre><code>gaussian_likelihood(pz: ndarray, M: PrecisionOperator) -&gt; float\n</code></pre> <p>Compute log-likelihood of GWAS summary statistics under a Gaussian model.</p> The model is <p>beta ~ MVN(0, D) z|beta ~ MVN(sqrt(n)Rbeta, R) where R is the LD matrix, n the sample size pz = inv(R) * z / sqrt(n) M = cov(pz) = D + inv(R)/n</p> <p>Parameters:</p> Name Type Description Default <code>pz</code> <code>ndarray</code> <p>Array of precision-premultiplied GWAS effect size estimates</p> required <code>M</code> <code>PrecisionOperator</code> <p>PrecisionOperator. This should be the covariance of pz.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Log-likelihood value</p>"},{"location":"api/graphld/#graphld.gaussian_likelihood_gradient","title":"gaussian_likelihood_gradient","text":"<pre><code>gaussian_likelihood_gradient(pz: ndarray, M: PrecisionOperator, del_M_del_a: Optional[ndarray] = None, n_samples: int = 10, seed: Optional[int] = None, trace_estimator: Optional[str] = 'xdiag') -&gt; np.ndarray\n</code></pre> <p>Computes the score under a Gaussian model.</p> <pre><code>The model is:\nbeta ~ MVN(0, D)\nz|beta ~ MVN(sqrt(n)*R*beta, R) where R is the LD matrix, n the sample size\npz = inv(R) * z / sqrt(n)\nM = cov(pz) = D + inv(R)/n\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pz</code> <code>ndarray</code> <p>Array of precision-premultiplied GWAS effect size estimates</p> required <code>M</code> <code>PrecisionOperator</code> <p>PrecisionOperator. This should be the covariance of pz.</p> required <code>del_M_del_a</code> <code>Optional[ndarray]</code> <p>Matrix of derivatives of M's diagonal elements wrt parameters a</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>Number of probe vectors for Hutchinson's method or xdiag</p> <code>10</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for generating probe vectors</p> <code>None</code> <code>trace_estimator</code> <code>Optional[str]</code> <p>Method for computing the trace estimator. Options: \"exact\", \"hutchinson\", \"xdiag\"</p> <code>'xdiag'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of diagonal elements of the gradient wrt M's diagonal elements,</p> <code>ndarray</code> <p>or with respect to parameters a if del_M_del_a is provided</p>"},{"location":"api/graphld/#graphld.gaussian_likelihood_hessian","title":"gaussian_likelihood_hessian","text":"<pre><code>gaussian_likelihood_hessian(pz: ndarray, M: PrecisionOperator, del_M_del_a: Optional[ndarray] = None, diagonal_method: Optional[str] = None, n_samples: int = 100, seed: Optional[int] = None) -&gt; np.ndarray\n</code></pre> <p>Computes the average information matrix of the Gaussian log-likelihood.</p> The model is <p>beta ~ MVN(0, D) z|beta ~ MVN(sqrt(n)Rbeta, R) where R is the LD matrix, n the sample size pz = inv(R) * z / sqrt(n) M = cov(pz) = D + inv(R)/n</p> <p>Parameters:</p> Name Type Description Default <code>pz</code> <code>ndarray</code> <p>Array of precision-premultiplied GWAS effect size estimates</p> required <code>M</code> <code>PrecisionOperator</code> <p>PrecisionOperator. This should be the covariance of pz.</p> required <code>del_M_del_a</code> <code>Optional[ndarray]</code> <p>Matrix of derivatives of M's diagonal elements wrt parameters a. If None, only the diagonal elements are computed.</p> <code>None</code> <code>diagonal_method</code> <code>Optional[str]</code> <p>Method for computing the diagonal of the Hessian. Options: \"exact\", \"hutchinson\", \"xdiag\", None (default)</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>Number of probe vectors for Hutchinson's method or xdiag</p> <code>100</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for generating probe vectors</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Matrix of second derivatives wrt parameters a, or array of diagonal elements</p> <code>ndarray</code> <p>if del_M_del_a is None</p>"},{"location":"api/graphld/#graphld.get_parquet_traits","title":"get_parquet_traits","text":"<pre><code>get_parquet_traits(file: Union[str, Path]) -&gt; list[str]\n</code></pre> <p>Get list of trait names from a parquet summary statistics file.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Union[str, Path]</code> <p>Path to parquet file</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of trait names found in the file</p>"},{"location":"api/graphld/#graphld.load_annotations","title":"load_annotations","text":"<pre><code>load_annotations(annot_path: str, chromosome: Optional[int] = None, infer_schema_length: int = 100000, add_alleles: bool = False, add_positions: bool = True, positions_file: str = POSITIONS_FILE, file_pattern: str = 'baselineLD.{chrom}.annot', exclude_bed: bool = False) -&gt; pl.DataFrame\n</code></pre> <p>Load annotation data for specified chromosome(s) and merge with LDGMs data.</p> <p>Parameters:</p> Name Type Description Default <code>annot_path</code> <code>str</code> <p>Path to directory containing annotation files</p> required <code>chromosome</code> <code>Optional[int]</code> <p>Specific chromosome number, or None for all chromosomes</p> <code>None</code> <code>infer_schema_length</code> <code>int</code> <p>Number of rows to infer schema from. Runs faster if this is smaller</p> <code>100000</code> <code>file_pattern</code> <code>str</code> <p>Filename pattern to match, with {chrom} as a placeholder for chromosome number</p> <code>'baselineLD.{chrom}.annot'</code> <code>exclude_bed</code> <code>bool</code> <p>If True, skip loading .bed files from the annotations directory</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing annotations</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no matching annotation files are found</p>"},{"location":"api/graphld/#graphld.load_ldgm","title":"load_ldgm","text":"<pre><code>load_ldgm(filepath: str, snplist_path: Optional[str] = None, population: Optional[str] = 'EUR', snps_only: bool = False) -&gt; Union['PrecisionOperator', List['PrecisionOperator']]\n</code></pre> <p>Load an LDGM from a single LD block's edgelist and snplist files.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the .edgelist file or directory containing it</p> required <code>snplist_path</code> <code>Optional[str]</code> <p>Optional path to .snplist file or directory. If None, uses filepath</p> <code>None</code> <code>population</code> <code>Optional[str]</code> <p>Optional population name to filter files and set allele frequency column. Defaults to \"EUR\"</p> <code>'EUR'</code> <code>snps_only</code> <code>bool</code> <p>Import snplist data for SNPs only (smaller memory usage)</p> <code>False</code> <p>Returns:</p> Type Description <code>Union['PrecisionOperator', List['PrecisionOperator']]</code> <p>If filepath is a directory: List of PrecisionOperator instances, one for each edgelist file</p> <code>Union['PrecisionOperator', List['PrecisionOperator']]</code> <p>If filepath is a file: Single PrecisionOperator instance with loaded precision matrix and variant info</p>"},{"location":"api/graphld/#graphld.merge_snplists","title":"merge_snplists","text":"<pre><code>merge_snplists(precision_op: 'PrecisionOperator', sumstats: DataFrame, *, variant_id_col: str = 'SNP', ref_allele_col: str = 'REF', alt_allele_col: str = 'ALT', match_by_position: bool = False, pos_col: str = 'POS', table_format: str = '', add_cols: list[str] = None, add_allelic_cols: list[str] = None, representatives_only: bool = False, modify_in_place: bool = False) -&gt; Tuple['PrecisionOperator', np.ndarray]\n</code></pre> <p>Merge a PrecisionOperator instance with summary statistics DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>precision_op</code> <code>'PrecisionOperator'</code> <p>PrecisionOperator instance</p> required <code>sumstats</code> <code>DataFrame</code> <p>Summary statistics DataFrame</p> required <code>variant_id_col</code> <code>str</code> <p>Column name containing variant IDs</p> <code>'SNP'</code> <code>ref_allele_col</code> <code>str</code> <p>Column name containing reference allele</p> <code>'REF'</code> <code>alt_allele_col</code> <code>str</code> <p>Column name containing alternative allele</p> <code>'ALT'</code> <code>match_by_position</code> <code>bool</code> <p>Whether to match SNPs by position instead of ID</p> <code>False</code> <code>pos_col</code> <code>str</code> <p>Column name containing position</p> <code>'POS'</code> <code>table_format</code> <code>str</code> <p>Optional file format specification (e.g., 'vcf')</p> <code>''</code> <code>add_cols</code> <code>list[str]</code> <p>Optional list of column names from sumstats to append to variant_info</p> <code>None</code> <code>add_allelic_cols</code> <code>list[str]</code> <p>Optional list of column names from sumstats to append to variant_info, multiplied by the phase (-1 or 1) to align with ancestral/derived alleles. If no alleles are provided, these are added without sign-flipping.</p> <code>None</code> <code>modify_in_place</code> <code>bool</code> <p>Whether to modify the PrecisionOperator in place</p> <code>False</code> <p>Returns:</p> Type Description <code>'PrecisionOperator'</code> <p>Tuple containing:</p> <code>ndarray</code> <ul> <li>Modified PrecisionOperator with merged variant info and appended columns</li> </ul> <code>Tuple['PrecisionOperator', ndarray]</code> <ul> <li>Array of indices into sumstats DataFrame indicating which rows were successfully merged</li> </ul>"},{"location":"api/graphld/#graphld.partition_variants","title":"partition_variants","text":"<pre><code>partition_variants(ldgm_metadata: DataFrame, variant_data: DataFrame, *, chrom_col: Optional[str] = None, pos_col: Optional[str] = None) -&gt; List[pl.DataFrame]\n</code></pre> <p>Partition variant data according to LDGM blocks.</p> <p>Parameters:</p> Name Type Description Default <code>ldgm_metadata</code> <code>DataFrame</code> <p>DataFrame from read_ldgm_metadata containing block info</p> required <code>variant_data</code> <code>DataFrame</code> <p>DataFrame containing variant information</p> required <code>chrom_col</code> <code>Optional[str]</code> <p>Optional name of chromosome column. If None, tries common names</p> <code>None</code> <code>pos_col</code> <code>Optional[str]</code> <p>Optional name of position column. If None, tries common names</p> <code>None</code> <p>Returns:</p> Type Description <code>List[DataFrame]</code> <p>List of DataFrames, one per row in ldgm_metadata, containing variants</p> <code>List[DataFrame]</code> <p>that fall within each block's coordinates</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the file does not have the expected columns: variant_id, chromosome, position, ...</p>"},{"location":"api/graphld/#graphld.read_gwas_vcf","title":"read_gwas_vcf","text":"<pre><code>read_gwas_vcf(file_path: str, num_rows: Optional[int] = None, maximum_missingness: float = 0.1, verbose: bool = False) -&gt; pl.DataFrame\n</code></pre> <p>Reads a GWAS-VCF file using Polars and returns a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the VCF file.</p> required <code>num_rows</code> <code>Optional[int]</code> <p>Number of rows to read. Defaults to None.</p> <code>None</code> <code>maximum_missingness</code> <code>float</code> <p>Maximum fraction of missing samples allowed. Defaults to 0.1.</p> <code>0.1</code> <code>verbose</code> <code>bool</code> <p>Print detailed information about FORMAT columns. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: DataFrame containing the VCF data.</p>"},{"location":"api/graphld/#graphld.read_ldgm_metadata","title":"read_ldgm_metadata","text":"<pre><code>read_ldgm_metadata(filepath: Union[str, Path], *, populations: Optional[Union[str, List[str]]] = None, chromosomes: Optional[Union[int, List[int]]] = None, max_blocks: Optional[int] = None) -&gt; pl.DataFrame\n</code></pre> <p>Read LDGM metadata from CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>Union[str, Path]</code> <p>Path to metadata CSV file</p> required <code>populations</code> <code>Optional[Union[str, List[str]]]</code> <p>Optional population(s) to filter by</p> <code>None</code> <code>chromosomes</code> <code>Optional[Union[int, List[int]]]</code> <p>Optional chromosome(s) to filter by</p> <code>None</code> <code>max_blocks</code> <code>Optional[int]</code> <p>Optional maximum number of blocks to return</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Polars DataFrame containing LDGM metadata, filtered by population and chromosome</p> <code>DataFrame</code> <p>if specified, and limited to max_blocks if specified</p>"},{"location":"api/graphld/#graphld.read_ldsc_sumstats","title":"read_ldsc_sumstats","text":"<pre><code>read_ldsc_sumstats(file: Union[str, Path], add_positions: bool = True, positions_file: str = POSITIONS_FILE, maximum_missingness: float = 1.0) -&gt; pl.DataFrame\n</code></pre> <p>Read LDSC sumstats file format.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Union[str, Path]</code> <p>Path to LDSC sumstats file</p> required <code>add_positions</code> <code>bool</code> <p>If True, merge with external file to add positions</p> <code>True</code> <code>positions_file</code> <code>str</code> <p>File containing RSIDs and positions, defaults to data/rsid_position.csv</p> <code>POSITIONS_FILE</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with columns: SNP, N, Z, A1, A2</p> <code>DataFrame</code> <p>If add_positions=True, also includes: CHR, POS</p>"},{"location":"api/graphld/#graphld.read_parquet_sumstats","title":"read_parquet_sumstats","text":"<pre><code>read_parquet_sumstats(file: Union[str, Path], trait: Optional[str] = None, maximum_missingness: float = 1.0) -&gt; pl.DataFrame\n</code></pre> <p>Read parquet summary statistics file and return a single-trait DataFrame.</p> <p>The parquet format stores columns as {trait}_BETA and {trait}_SE for each trait. This function extracts a single trait and converts to a standard format with columns: SNP, CHR, POS, REF, ALT, N, Z.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Union[str, Path]</code> <p>Path to parquet file</p> required <code>trait</code> <code>Optional[str]</code> <p>Name of trait to extract. If None, uses the first trait found.</p> <code>None</code> <code>maximum_missingness</code> <code>float</code> <p>Maximum fraction of missing samples allowed (based on N column if present)</p> <code>1.0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with columns: SNP, CHR, POS, REF, ALT, N, Z</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified trait is not found in the file</p>"},{"location":"api/graphld/#graphld.run_blup","title":"run_blup","text":"<pre><code>run_blup(*args, **kwargs)\n</code></pre> <p>Compute Best Linear Unbiased Prediction (BLUP) weights.</p> <p>Parameters:</p> Name Type Description Default <code>ldgm_metadata_path</code> <code>str</code> <p>Path to LDGM metadata file</p> required <code>sumstats</code> <code>DataFrame</code> <p>Summary statistics DataFrame</p> required <code>heritability</code> <code>float</code> <p>Heritability parameter (between 0 and 1)</p> required <code>num_samples</code> <code>Optional[int]</code> <p>Number of samples. Defaults to None.</p> required <code>num_processes</code> <code>Optional[int]</code> <p>Number of processes for parallel computation. Defaults to None.</p> required <code>run_in_serial</code> <code>bool</code> <p>Whether to run in serial mode. Defaults to False.</p> required <code>chromosome</code> <code>Optional[int]</code> <p>Chromosome to filter. Defaults to None.</p> required <code>population</code> <code>Optional[str]</code> <p>Population to filter. Defaults to None.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print verbose output. Defaults to False.</p> required <code>quiet</code> <code>bool</code> <p>Whether to suppress all output except errors. Defaults to False.</p> required <p>Returns:</p> Type Description <p>pl.DataFrame: DataFrame with BLUP weights and associated statistics</p>"},{"location":"api/graphld/#graphld.run_clump","title":"run_clump","text":"<pre><code>run_clump(*args, **kwargs)\n</code></pre> <p>Perform LD-based clumping on summary statistics.</p> <p>Parameters:</p> Name Type Description Default <code>ldgm_metadata_path</code> <code>str</code> <p>Path to LDGM metadata file</p> required <code>sumstats</code> <code>DataFrame</code> <p>Summary statistics DataFrame</p> required <code>min_chisq</code> <code>float</code> <p>Minimum chi-squared threshold. Defaults to 5.0.</p> required <code>max_rsq</code> <code>float</code> <p>Maximum R-squared threshold. Defaults to 0.1.</p> required <code>num_processes</code> <code>Optional[int]</code> <p>Number of processes for parallel computation. Defaults to None.</p> required <code>run_in_serial</code> <code>bool</code> <p>Whether to run in serial mode. Defaults to False.</p> required <code>chromosome</code> <code>Optional[int]</code> <p>Chromosome to filter. Defaults to None.</p> required <code>population</code> <code>Optional[str]</code> <p>Population to filter. Defaults to None.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print verbose output. Defaults to False.</p> required <code>quiet</code> <code>bool</code> <p>Whether to suppress all output except errors. Defaults to False.</p> required <p>Returns:</p> Type Description <p>pl.DataFrame: DataFrame with clumped summary statistics and index variant information</p>"},{"location":"api/graphld/#graphld.run_graphREML","title":"run_graphREML","text":"<pre><code>run_graphREML(model_options: ModelOptions, method_options: MethodOptions, summary_stats: DataFrame, annotation_data: DataFrame, ldgm_metadata_path: str, populations: Union[str, List[str]] = None, chromosomes: Optional[Union[int, List[int]]] = None)\n</code></pre> <p>Wrapper function for GraphREML.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>Model instance containing parameters and settings</p> required <code>summary_stats</code> <code>DataFrame</code> <p>DataFrame containing GWAS summary statistics</p> required <code>annotation_data</code> <code>DataFrame</code> <p>DataFrame containing variant annotations</p> required <code>ldgm_metadata_path</code> <code>str</code> <p>Path to LDGM metadata file</p> required <code>populations</code> <code>Union[str, List[str]]</code> <p>Optional list of populations to include</p> <code>None</code> <code>chromosomes</code> <code>Optional[Union[int, List[int]]]</code> <p>Optional list of chromosomes to include</p> <code>None</code> <code>num_processes</code> <p>Optional number of processes for parallel computation</p> required <code>max_iterations</code> <p>Maximum number of iterations</p> required <code>convergence_tol</code> <p>Convergence tolerance</p> required <code>match_by_position</code> <p>If True, match variants by position instead of variant ID</p> required <code>run_in_serial</code> <p>If True, run in serial instead of parallel</p> required <code>num_iterations</code> <p>Number of repetitions</p> required <p>Returns:</p> Type Description <p>Dictionary containing:</p> <ul> <li>estimated parameters</li> </ul> <ul> <li>heritability estimates</li> </ul> <ul> <li>standard errors</li> </ul> <ul> <li>convergence diagnostics</li> </ul>"},{"location":"api/graphld/#graphld.run_simulate","title":"run_simulate","text":"<pre><code>run_simulate(sample_size: int, heritability: float = 0.5, component_variance: Optional[Union[ndarray, List[float]]] = None, component_weight: Optional[Union[ndarray, List[float]]] = None, alpha_param: float = -1, annotation_dependent_polygenicity: bool = False, link_fn: Callable[[ndarray], ndarray] = _default_link_fn, random_seed: Optional[int] = None, annotation_columns: Optional[List[str]] = None, ldgm_metadata_path: str = 'data/ldgms/metadata.csv', populations: Optional[Union[str, List[str]]] = 'EUR', chromosomes: Optional[Union[int, List[int]]] = None, run_in_serial: bool = False, num_processes: Optional[int] = None, annotations: Optional[DataFrame] = None, verbose: bool = False) -&gt; pl.DataFrame\n</code></pre> <p>Run GWAS summary statistics simulation with specified parameters.</p> <p>Parameters:</p> Name Type Description Default <code>sample_size</code> <code>int</code> <p>Sample size for the population</p> required <code>heritability</code> <code>float</code> <p>Total heritability (h2) for the trait</p> <code>0.5</code> <code>component_variance</code> <code>Optional[Union[ndarray, List[float]]]</code> <p>Per-allele effect size variance for each mixture component</p> <code>None</code> <code>component_weight</code> <code>Optional[Union[ndarray, List[float]]]</code> <p>Mixture weight for each component (must sum to \u2264 1)</p> <code>None</code> <code>alpha_param</code> <code>float</code> <p>Alpha parameter for allele frequency-dependent architecture</p> <code>-1</code> <code>annotation_dependent_polygenicity</code> <code>bool</code> <p>If True, use annotations to modify proportion of causal variants instead of effect size magnitude</p> <code>False</code> <code>link_fn</code> <code>Callable[[ndarray], ndarray]</code> <p>Function mapping annotation vector to relative per-variant heritability. Default is softmax: x -&gt; log(1 + exp(sum(x))). Must be defined at module level (not as lambda or nested function) to work with multiprocessing</p> <code>_default_link_fn</code> <code>random_seed</code> <code>Optional[int]</code> <p>Random seed for reproducibility</p> <code>None</code> <code>annotation_columns</code> <code>Optional[List[str]]</code> <p>List of column names to use as annotations</p> <code>None</code> <code>ldgm_metadata_path</code> <code>str</code> <p>Path to LDGM metadata file</p> <code>'data/ldgms/metadata.csv'</code> <code>populations</code> <code>Optional[Union[str, List[str]]]</code> <p>Population(s) to filter</p> <code>'EUR'</code> <code>chromosomes</code> <code>Optional[Union[int, List[int]]]</code> <p>Chromosome(s) to filter</p> <code>None</code> <code>run_in_serial</code> <code>bool</code> <p>Whether to run in serial mode</p> <code>False</code> <code>num_processes</code> <code>Optional[int]</code> <p>Number of processes for parallel execution</p> <code>None</code> <code>annotations</code> <code>Optional[DataFrame]</code> <p>Optional variant annotations DataFrame</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print progress information</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing simulated summary statistics</p>"},{"location":"api/heritability/","title":"graphld.heritability","text":"<p>Heritability estimation using graphREML.</p> <p>This module implements the graphREML method for heritability partitioning and enrichment analysis using GWAS summary statistics and LDGM precision matrices.</p>"},{"location":"api/heritability/#graphld.heritability","title":"heritability","text":"<p>GraphREML</p>"},{"location":"api/heritability/#graphld.heritability.ModelOptions","title":"ModelOptions  <code>dataclass</code>","text":"<pre><code>ModelOptions(annotation_columns: Optional[List[str]] = None, params: Optional[ndarray] = None, sample_size: Optional[float] = None, intercept: float = 1.0, link_fn_denominator: float = 6000000.0, binary_annotations_only: bool = False)\n</code></pre> <p>Stores model parameters for graphREML.</p> <p>Attributes:</p> Name Type Description <code>annotation_columns</code> <code>Optional[List[str]]</code> <p>names of columns to be used as annotations</p> <code>params</code> <code>Optional[ndarray]</code> <p>Starting parameter values</p> <code>sample_size</code> <code>Optional[float]</code> <p>GWAS sample size, only needed for heritability scaling</p> <code>intercept</code> <code>float</code> <p>LDSC intercept or 1</p> <code>link_fn_denominator</code> <code>float</code> <p>Scalar denominator for link function.</p>"},{"location":"api/heritability/#graphld.heritability.MethodOptions","title":"MethodOptions  <code>dataclass</code>","text":"<pre><code>MethodOptions(gradient_num_samples: int = 100, gradient_seed: Optional[int] = 123, match_by_position: bool = False, num_iterations: int = 50, convergence_tol: float = 0.01, run_serial: bool = False, num_processes: Optional[int] = None, verbose: bool = False, use_surrogate_markers: bool = True, trust_region_size: float = 0.1, trust_region_rho_lb: float = 0.0001, trust_region_rho_ub: float = 0.99, trust_region_scalar: float = 5, max_trust_iterations: int = 100, minimum_likelihood_increase: float = 1e-06, convergence_window: int = 3, reset_trust_region: bool = False, num_jackknife_blocks: int = 100, max_chisq_threshold: Optional[float] = None, score_test_hdf5_file_name: Optional[str] = None, score_test_hdf5_trait_name: Optional[str] = None, surrogate_markers_path: Optional[str] = None)\n</code></pre> <p>Stores method parameters for graphREML.</p> <p>Attributes:</p> Name Type Description <code>gradient_num_samples</code> <code>int</code> <p>Number of samples for gradient estimation</p> <code>match_by_position</code> <code>bool</code> <p>Use position/allele instead of RSID for merging</p> <code>num_iterations</code> <code>int</code> <p>Optimization steps</p> <code>convergence_tol</code> <code>float</code> <p>Convergence tolerance</p> <code>run_serial</code> <code>bool</code> <p>Run in serial rather than parallel</p> <code>num_processes</code> <code>Optional[int]</code> <p>If None, autodetect</p> <code>verbose</code> <code>bool</code> <p>Flag for verbose output</p> <code>use_surrogate_markers</code> <code>bool</code> <p>Whether to use surrogate markers for missing variants</p> <code>trust_region_size</code> <code>float</code> <p>Initial trust region size parameter</p> <code>trust_region_rho_lb</code> <code>float</code> <p>Lower bound for trust region ratio</p> <code>trust_region_rho_ub</code> <code>float</code> <p>Upper bound for trust region ratio</p> <code>trust_region_scalar</code> <code>float</code> <p>Scaling factor for trust region updates</p> <code>max_trust_iterations</code> <code>int</code> <p>Maximum number of trust region iterations</p> <code>reset_trust_region</code> <code>bool</code> <p>Whether to reset trust region size at each iteration</p> <code>num_jackknife_blocks</code> <code>int</code> <p>Number of blocks to use for jackknife estimation</p> <code>max_chisq_threshold</code> <code>Optional[float]</code> <p>Maximum allowed chi^2 value in a block. Blocks with chi^2 &gt; threshold are excluded.</p> <code>score_test_hdf5_file_name</code> <code>Optional[str]</code> <p>Optional file name to create or append to an hdf5 file with pre-computed derivatives for the score test.</p> <code>score_test_hdf5_trait_name</code> <code>Optional[str]</code> <p>Name of the trait's subdirectory within the score test HDF5 file.</p> <code>surrogate_markers_path</code> <code>Optional[str]</code> <p>Optional path to an HDF5 file with per-block surrogate mappings.</p>"},{"location":"api/heritability/#graphld.heritability.GraphREML","title":"GraphREML","text":"<p>               Bases: <code>ParallelProcessor</code></p>"},{"location":"api/heritability/#graphld.heritability.GraphREML.prepare_block_data","title":"prepare_block_data  <code>classmethod</code>","text":"<pre><code>prepare_block_data(metadata: DataFrame, **kwargs) -&gt; list[tuple]\n</code></pre> <p>Prepare block-specific data for processing.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>DataFrame</code> <p>DataFrame containing LDGM metadata</p> required <code>**kwargs</code> <p>Additional arguments from run(), including: sumstats: DataFrame containing Z scores and variant info, optionally annotations annotation_columns: list of column names for the annotations in sumstats method: MethodOptions instance containing method parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[tuple]</code> <p>List of dictionaries containing block-specific data with keys: sumstats: DataFrame for this block, or None if max Z\u00b2 exceeds threshold variant_offset: Cumulative number of variants before this block block_index: Index of this block Pz: Pre-computed precision-premultiplied Z scores for this block</p> Source code in <code>src/graphld/heritability.py</code> <pre><code>@classmethod\ndef prepare_block_data(cls, metadata: pl.DataFrame, **kwargs) -&gt; list[tuple]:\n    \"\"\"Prepare block-specific data for processing.\n\n    Args:\n        metadata: DataFrame containing LDGM metadata\n        **kwargs: Additional arguments from run(), including:\n            sumstats: DataFrame containing Z scores and variant info, optionally annotations\n            annotation_columns: list of column names for the annotations in sumstats\n            method: MethodOptions instance containing method parameters\n\n    Returns:\n        List of dictionaries containing block-specific data with keys:\n            sumstats: DataFrame for this block, or None if max Z\u00b2 exceeds threshold\n            variant_offset: Cumulative number of variants before this block\n            block_index: Index of this block\n            Pz: Pre-computed precision-premultiplied Z scores for this block\n    \"\"\"\n    sumstats: pl.DataFrame = kwargs.get(\"sumstats\")\n    method: MethodOptions = kwargs.get(\"method\")\n    sumstats_blocks: list[pl.DataFrame] = partition_variants(metadata, sumstats)\n    num_block_variants = sum(len(block) for block in sumstats_blocks)\n    assert num_block_variants &lt;= sumstats.shape[0], (\n        f\"Too many variants in blocks: {num_block_variants} &gt; {sumstats.shape[0]}\"\n    )\n    if method.verbose:\n        print(\n            f\"{sum(len(block) for block in sumstats_blocks)} variants in blocks, {sumstats.shape[0]} initially\"\n        )\n        print(\n            f\"Smallest block size: {min(len(block) for block in sumstats_blocks)}\"\n        )\n        print(f\"Largest block size: {max(len(block) for block in sumstats_blocks)}\")\n\n    # Filter blocks based on max Z\u00b2 threshold\n    if method.max_chisq_threshold is not None:\n        max_z2s = [\n            float(np.nanmax(block.select(\"Z\").to_numpy() ** 2))\n            for block in sumstats_blocks\n        ]\n        keep_block = [max_z2 &lt;= method.max_chisq_threshold for max_z2 in max_z2s]\n        sumstats_blocks = [\n            block if keep_block else pl.DataFrame([])\n            for block, max_z2 in zip(sumstats_blocks, max_z2s, strict=False)\n        ]\n        if method.verbose and not all(keep_block):\n            print(\n                f\"{len(sumstats_blocks) - sum(keep_block)} out of {len(sumstats_blocks)} blocks discarded due to\\n\"\n                f\"max chisq threshold of {method.max_chisq_threshold}\"\n            )\n\n    cumulative_num_variants = np.cumsum(\n        np.array([len(df) for df in sumstats_blocks])\n    )\n    cumulative_num_variants = [0] + list(cumulative_num_variants[:-1])\n    block_indices = list(range(len(sumstats_blocks)))\n    block_Pz = [None for _ in block_indices]\n    block_names = metadata.get_column(\"name\").to_list() if len(metadata) &gt; 0 else []\n\n    return [\n        {\n            \"sumstats\": block,\n            \"variant_offset\": offset,\n            \"block_index\": index,\n            \"Pz\": Pz,\n            \"block_name\": block_names[index] if index &lt; len(block_names) else None,\n        }\n        for block, offset, index, Pz in zip(\n            sumstats_blocks,\n            cumulative_num_variants,\n            block_indices,\n            block_Pz,\n            strict=False,\n        )\n    ]\n</code></pre>"},{"location":"api/heritability/#graphld.heritability.GraphREML.create_shared_memory","title":"create_shared_memory  <code>staticmethod</code>","text":"<pre><code>create_shared_memory(metadata: DataFrame, block_data: list[tuple], **kwargs) -&gt; SharedData\n</code></pre> <p>Create output array.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>DataFrame</code> <p>Metadata DataFrame containing block information</p> required <code>block_data</code> <code>list[tuple]</code> <p>List of block-specific sumstats DataFrames</p> required <code>**kwargs</code> <p>Not used</p> <code>{}</code> Source code in <code>src/graphld/heritability.py</code> <pre><code>@staticmethod\ndef create_shared_memory(\n    metadata: pl.DataFrame, block_data: list[tuple], **kwargs\n) -&gt; SharedData:\n    \"\"\"Create output array.\n\n    Args:\n        metadata: Metadata DataFrame containing block information\n        block_data: List of block-specific sumstats DataFrames\n        **kwargs: Not used\n    \"\"\"\n    num_params = kwargs.get(\"num_params\")\n    num_blocks = len(metadata)\n    num_variants = sum(\n        [len(d[\"sumstats\"]) for d in block_data if d[\"sumstats\"] is not None]\n    )\n\n    result = SharedData(\n        {\n            \"params\": num_params,\n            \"variant_data\": num_variants,\n            \"likelihood\": num_blocks,\n            \"gradient\": num_blocks * num_params,\n            \"hessian\": num_blocks * num_params**2,\n        }\n    )\n\n    return result\n</code></pre>"},{"location":"api/heritability/#graphld.heritability.GraphREML.process_block","title":"process_block  <code>classmethod</code>","text":"<pre><code>process_block(ldgm: PrecisionOperator, flag: Value, shared_data: SharedData, block_offset: int, block_data: Any = None, worker_params: Tuple[ModelOptions, MethodOptions] = None)\n</code></pre> <p>Computes likelihood, gradient, and hessian for a single block.</p> Source code in <code>src/graphld/heritability.py</code> <pre><code>@classmethod\ndef process_block(\n    cls,\n    ldgm: PrecisionOperator,\n    flag: Value,\n    shared_data: SharedData,\n    block_offset: int,\n    block_data: Any = None,\n    worker_params: Tuple[ModelOptions, MethodOptions] = None,\n):\n    \"\"\"Computes likelihood, gradient, and hessian for a single block.\"\"\"\n    model_options: ModelOptions\n    method_options: MethodOptions\n    model_options, method_options = worker_params\n    seed = None\n    if method_options.gradient_seed is not None:\n        seed = method_options.gradient_seed + block_data[\"block_index\"]\n        np.random.seed(seed)\n\n    sumstats: pl.DataFrame = block_data[\"sumstats\"]\n\n    Pz: np.ndarray\n    if flag.value == FLAGS[\"INITIALIZE\"]:\n        surrogate_map = None\n        if method_options.surrogate_markers_path is not None:\n            surrogate_map = cls._load_block_surrogate_map(\n                method_options.surrogate_markers_path, block_data.get(\"block_name\")\n            )\n\n        ldgm, Pz = cls._initialize_block_zscores(\n            ldgm,\n            sumstats,\n            model_options.annotation_columns,\n            method_options.match_by_position,\n            method_options.verbose,\n            surrogate_map=surrogate_map,\n        )\n\n        # Work in effect-size as opposed to Z score units\n        if Pz is not None:\n            Pz /= np.sqrt(model_options.sample_size)\n            ldgm.times_scalar(model_options.intercept / model_options.sample_size)\n        block_data[\"Pz\"] = Pz\n    # ldgm is modified in place and re-used in subsequent iterations\n\n    else:\n        Pz = block_data[\"Pz\"]\n\n    if Pz is None:\n        return\n\n    annot_indices: np.ndarray = (\n        ldgm.variant_info.select(\"annot_indices\").to_numpy().flatten()\n    )\n    max_index: int = np.max(annot_indices) + 1 if len(annot_indices) &gt; 0 else 0\n    variant_offset: int = block_data[\"variant_offset\"]\n    block_variants: slice = slice(variant_offset, variant_offset + max_index)\n    annot: np.ndarray = ldgm.variant_info.select(\n        model_options.annotation_columns\n    ).to_numpy()\n    params: np.ndarray = shared_data[\"params\"].reshape(-1, 1)\n\n    # Handle variant-specific gradient and Hessian computation\n    if flag.value == FLAGS[\"COMPUTE_VARIANT_SCORE\"]:\n        del_h2i_del_xi, _ = cls._link_fn_derivatives(\n            annot, params, model_options.link_fn_denominator\n        )\n        result = cls._compute_variant_grad(ldgm, Pz, del_h2i_del_xi, seed=seed)\n        result_padded = np.zeros(max_index)\n        result_padded[annot_indices] = result\n        shared_data[\"variant_data\", block_variants] = result_padded\n        return\n\n    if flag.value == FLAGS[\"COMPUTE_VARIANT_HESSIAN\"]:\n        del_h2i_del_xi, del2_h2i_del_xi2 = cls._link_fn_derivatives(\n            annot, params, model_options.link_fn_denominator\n        )\n        del_L_del_xi = shared_data[\"variant_data\", block_variants][\n            annot_indices\n        ].ravel()\n        result = cls._compute_variant_hessian_diag(\n            ldgm, Pz, del_L_del_xi, del_h2i_del_xi, del2_h2i_del_xi2, seed=seed\n        )\n        result_padded = np.zeros(max_index)\n        result_padded[annot_indices] = result.ravel()\n        shared_data[\"variant_data\", block_variants] = result_padded\n        return\n\n    block_index: int = block_data[\"block_index\"]\n    num_annot = len(model_options.annotation_columns)\n\n    old_variant_h2: np.ndarray = shared_data[\"variant_data\", block_variants][\n        annot_indices\n    ]\n\n    likelihood_only = flag.value == FLAGS[\"COMPUTE_LIKELIHOOD_ONLY\"]\n    likelihood, gradient, hessian, variant_h2 = cls._compute_block_likelihood(\n        ldgm=ldgm,\n        Pz=Pz,\n        annotations=annot,\n        params=params,\n        link_fn_denominator=model_options.link_fn_denominator,\n        old_variant_h2=old_variant_h2,\n        num_samples=method_options.gradient_num_samples,\n        likelihood_only=likelihood_only,\n        seed=seed,\n    )\n\n    shared_data[\"likelihood\", block_index] = likelihood\n    variant_h2_padded = np.zeros(max_index)\n    variant_h2_padded[annot_indices] = variant_h2.ravel()\n    shared_data[\"variant_data\", block_variants] = variant_h2_padded\n    if likelihood_only:\n        return\n\n    gradient_slice = slice(block_index * num_annot, (block_index + 1) * num_annot)\n    shared_data[\"gradient\", gradient_slice] = gradient.flatten()\n\n    hessian_slice = slice(\n        block_index * num_annot**2, (block_index + 1) * num_annot**2\n    )\n    shared_data[\"hessian\", hessian_slice] = hessian.flatten()\n</code></pre>"},{"location":"api/heritability/#graphld.heritability.GraphREML.supervise","title":"supervise  <code>classmethod</code>","text":"<pre><code>supervise(manager: WorkerManager, shared_data: SharedData, block_data: list, **kwargs)\n</code></pre> <p>Runs graphREML. Args:     manager: used to start parallel workers     shared_data: used to communicate with workers     block_data: annotation + gwas data passed to workers     **kwargs: Additional arguments</p> Source code in <code>src/graphld/heritability.py</code> <pre><code>@classmethod\ndef supervise(\n    cls, manager: WorkerManager, shared_data: SharedData, block_data: list, **kwargs\n):\n    \"\"\"Runs graphREML.\n    Args:\n        manager: used to start parallel workers\n        shared_data: used to communicate with workers\n        block_data: annotation + gwas data passed to workers\n        **kwargs: Additional arguments\n    \"\"\"\n\n    num_iterations = kwargs.get(\"num_iterations\")\n    num_params = kwargs.get(\"num_params\")\n    verbose = kwargs.get(\"verbose\")\n    method: MethodOptions = kwargs.get(\"method\")\n    model: ModelOptions = kwargs.get(\"model\")\n    trust_region_lambda = method.trust_region_size\n    log_likelihood_history = []\n    trust_region_history = []\n\n    def _trust_region_step(\n        gradient: np.ndarray, hessian: np.ndarray, trust_region_lambda: float\n    ) -&gt; np.ndarray:\n        \"\"\"Compute trust region step by solving (H + \u03bbD)x = -g.\"\"\"\n        hess_mod = hessian + trust_region_lambda * np.diag(\n            np.diag(hessian) - np.finfo(float).eps\n        )\n        step = np.linalg.solve(hess_mod, -gradient)\n        # predicted_increase = step.T @ gradient + 0.5 * step.T @ (hess_mod @ step)\n        predicted_increase = step.T @ gradient + 0.5 * step.T @ (hessian @ step)\n        assert predicted_increase &gt; -1e-6, (\n            f\"Predicted increase must be greater than -epsilon but is {predicted_increase}.\"\n        )\n\n        return step, predicted_increase\n\n    if model.params is not None:\n        shared_data[\"params\"] = model.params.flatten()\n    else:\n        shared_data[\"params\"] = np.full(num_params, 0)\n\n    last_step_bad = True\n    for rep in range(num_iterations):\n        if verbose:\n            print(f\"\\n\\tStarting iteration {rep}...\")\n\n        # Calculate likelihood, gradient, and hessian for each block\n        flag = FLAGS[\"INITIALIZE\"] if rep == 0 else FLAGS[\"COMPUTE_ALL\"]\n        manager.start_workers(flag)\n        manager.await_workers()\n\n        likelihood = cls._sum_blocks(shared_data[\"likelihood\"], (1,))[0]\n        gradient = cls._sum_blocks(shared_data[\"gradient\"], (num_params,))\n        hessian = cls._sum_blocks(shared_data[\"hessian\"], (num_params, num_params))\n\n        old_params = shared_data[\"params\"].copy()\n        old_likelihood = likelihood\n        if verbose and rep == 0:\n            print(f\"Initial log likelihood: {likelihood}\")\n\n        # Reset trust region size if specified\n        if method.reset_trust_region or last_step_bad:\n            trust_region_lambda = method.trust_region_size\n\n        # Trust region optimization loop\n        previous_lambda = trust_region_lambda\n        for trust_iter in range(method.max_trust_iterations):\n            # Compute proposed step\n            step, predicted_increase = _trust_region_step(\n                gradient, hessian, trust_region_lambda\n            )\n            shared_data[\"params\"] = old_params + step\n\n            # Evaluate proposed step\n            manager.start_workers(FLAGS[\"COMPUTE_LIKELIHOOD_ONLY\"])\n            manager.await_workers()\n            new_likelihood = cls._sum_blocks(shared_data[\"likelihood\"], (1,))[0]\n\n            # Compute actual vs predicted increase\n            actual_increase = new_likelihood - old_likelihood\n            if verbose:\n                print(\n                    f\"\\tIncrease in log-likelihood: {actual_increase}, predicted increase: {predicted_increase}\"\n                )\n\n            # Check if step is acceptable and update trust region size if needed\n            rho = actual_increase / predicted_increase\n            if rho &lt; method.trust_region_rho_lb:\n                if (\n                    predicted_increase &lt; method.minimum_likelihood_increase\n                    and rho &gt;= 0\n                ):\n                    if verbose:\n                        print(\n                            f\"\\tTerminated trust region size search with predicted likelihood increase {predicted_increase}.\"\n                        )\n                    break\n                # Reset trust region size to initial value if its below that\n                trust_region_lambda = max(\n                    method.trust_region_size,\n                    trust_region_lambda * method.trust_region_scalar,\n                )\n                shared_data[\"params\"] = old_params  # Revert step\n            elif rho &gt; method.trust_region_rho_ub:\n                trust_region_lambda /= method.trust_region_scalar\n                break  # Accept step and continue to next iteration\n            else:\n                break  # Accept step with current trust region size\n\n            if trust_iter == method.max_trust_iterations - 1:\n                if verbose:\n                    print(\"Warning: Maximum trust region iterations reached\")\n\n        last_step_bad = (\n            trust_region_lambda &gt; previous_lambda * method.trust_region_scalar\n        )\n\n        log_likelihood_history.append(new_likelihood)\n        trust_region_history.append(trust_region_lambda)\n\n        if verbose:\n            print(f\"log likelihood: {new_likelihood}\")\n            print(f\"Trust region lambda: {trust_region_lambda}\")\n            if len(log_likelihood_history) &gt;= 2:\n                print(\n                    f\"Change in likelihood: {log_likelihood_history[-1] - log_likelihood_history[-2]}\"\n                )\n            total_h2 = np.sum(shared_data[\"variant_data\"])\n            print(f\"Total h2 at current iteration: {total_h2}\")\n\n        # Check convergence\n        converged = False\n        if len(log_likelihood_history) &gt;= 1 + method.convergence_window:\n            if abs(\n                log_likelihood_history[-1]\n                - log_likelihood_history[-method.convergence_window]\n            ) &lt; (method.convergence_window * method.convergence_tol):\n                converged = True\n                break\n\n    if verbose:\n        print(\n            f\"-----Finished optimization after {rep + 1} out of {num_iterations} steps-----\"\n        )\n\n    # Point estimates\n    variant_h2 = shared_data[\"variant_data\"].copy()\n    annotations = pl.concat(\n        [\n            dict[\"sumstats\"].select(model.annotation_columns + SPECIAL_COLNAMES)\n            for dict in block_data\n            if len(dict[\"sumstats\"]) &gt; 0\n        ]\n    )\n    ref_col = 0  # Maybe TODO\n    annotation_heritability, annotation_enrichment = cls._annotation_heritability(\n        variant_h2, annotations.select(model.annotation_columns), ref_col\n    )\n\n    # Get block-wise gradients and Hessians for jackknife\n    num_blocks = len(block_data)\n    gradient_blocks = shared_data[\"gradient\"].reshape((num_blocks, num_params))\n    hessian_blocks = shared_data[\"hessian\"].reshape(\n        (num_blocks, num_params, num_params)\n    )\n\n    # Group blocks for jackknife\n    num_jackknife_blocks = min(method.num_jackknife_blocks, num_blocks)\n    jk_gradient_blocks = cls._group_blocks(gradient_blocks, num_jackknife_blocks)\n    jk_hessian_blocks = cls._group_blocks(hessian_blocks, num_jackknife_blocks)\n\n    # Compute jackknife estimates using the grouped blocks\n    params = shared_data[\"params\"].copy()\n    jackknife_params = cls._compute_pseudojackknife(\n        jk_gradient_blocks, jk_hessian_blocks, params\n    )\n\n    # Compute jackknife heritability estimates and standard errors\n    jackknife_h2, jackknife_annot_sums = cls._compute_jackknife_heritability(\n        block_data, jackknife_params, model\n    )\n\n    if method.score_test_hdf5_file_name is not None:\n        if verbose:\n            print(\"\\n\\tComputing variant scores...\")\n        manager.start_workers(FLAGS[\"COMPUTE_VARIANT_SCORE\"])\n        manager.await_workers()\n        variant_score = shared_data[\"variant_data\"].copy()\n\n        # Jackknife block to which each variant belongs\n        if verbose:\n            print(\"\\n\\tComputing jackknife variant assignments...\")\n        block_indptrs = [dict[\"variant_offset\"] for dict in block_data] + [\n            len(variant_score)\n        ]\n        jackknife_variant_assignments = cls._get_variant_jackknife_assignments(\n            block_indptrs, num_jackknife_blocks\n        )\n\n        # Project the annotations out of the score\n        annotations_matrix = annotations.select(model.annotation_columns).to_numpy()\n        _project_out(variant_score, annotations_matrix)\n\n        if verbose:\n            print(\"\\n\\tWriting variant data...\")\n        cls._write_variant_data(\n            method.score_test_hdf5_file_name,\n            annotations.select(SPECIAL_COLNAMES),\n            jackknife_variant_assignments,\n        )\n\n        cls._write_trait_stats(method, variant_score)\n\n    # Compute standard errors using jackknife formula: SE = sqrt((n-1) * var(estimates))\n    n_blocks = jackknife_params.shape[0]\n    params_se = np.sqrt((n_blocks - 1) * np.var(jackknife_params, axis=0, ddof=1))\n    h2_se = np.sqrt((n_blocks - 1) * np.var(jackknife_h2, axis=0, ddof=1))\n\n    # Compute normalized heritability for each jackknife estimate\n    jackknife_h2_normalized = jackknife_h2 / jackknife_annot_sums\n\n    # Compute quotient for point estimates and SE\n    jackknife_enrichment_quotient = (\n        jackknife_h2_normalized / jackknife_h2_normalized[:, [0]]\n    )\n    enrichment_se = np.sqrt(\n        (n_blocks - 1) * np.var(jackknife_enrichment_quotient, axis=0, ddof=1)\n    )\n\n    # Compute difference for p-values\n    jackknife_enrichment_diff = (\n        jackknife_h2_normalized - jackknife_h2_normalized[:, [0]]\n    )\n\n    # Two-tailed log10(p-values) using jackknife estimates\n    annotation_heritability_p = np.array(\n        [\n            cls._wald_log10pvalue(jackknife_h2[:, i])\n            for i in range(jackknife_h2.shape[1])\n        ]\n    )\n    # Use differences as opposed to quotients for log10(p-values)\n    annotation_enrichment_p = np.array(\n        [\n            cls._wald_log10pvalue(jackknife_enrichment_diff[:, i])\n            for i in range(jackknife_enrichment_diff.shape[1])\n        ]\n    )\n    params_p = np.array(\n        [\n            cls._wald_log10pvalue(jackknife_params[:, i])\n            for i in range(jackknife_params.shape[1])\n        ]\n    )\n\n    likelihood_changes = [\n        a - b\n        for a, b in zip(\n            log_likelihood_history[1:], log_likelihood_history[:-1], strict=False\n        )\n    ]\n\n    if verbose:\n        num_annotations = len(annotation_heritability)\n        print(f\"Heritability: {annotation_heritability[: min(5, num_annotations)]}\")\n        print(f\"Enrichment: {annotation_enrichment[: min(5, num_annotations)]}\")\n        print(\n            f\"Enrichment -log10(p-values): {-annotation_enrichment_p[: min(5, num_annotations)]}\"\n        )\n\n    return {\n        \"parameters\": params,\n        \"parameters_se\": params_se,\n        \"parameters_log10pval\": params_p,\n        \"heritability\": annotation_heritability,\n        \"heritability_se\": h2_se,\n        \"heritability_log10pval\": annotation_heritability_p,\n        \"enrichment\": annotation_enrichment,\n        \"enrichment_se\": enrichment_se,\n        \"enrichment_log10pval\": annotation_enrichment_p,\n        \"likelihood_history\": log_likelihood_history,\n        \"jackknife_h2\": jackknife_h2,\n        \"jackknife_params\": jackknife_params,\n        \"jackknife_enrichment\": jackknife_enrichment_quotient,\n        \"variant_h2\": variant_h2,\n        \"log\": {\n            \"converged\": converged,\n            \"num_iterations\": rep + 1,\n            \"likelihood_changes\": likelihood_changes,\n            \"final_likelihood\": log_likelihood_history[-1],\n            \"trust_region_lambdas\": trust_region_history,\n        },\n    }\n</code></pre>"},{"location":"api/heritability/#graphld.heritability.load_ldgm","title":"load_ldgm","text":"<pre><code>load_ldgm(filepath: str, snplist_path: Optional[str] = None, population: Optional[str] = 'EUR', snps_only: bool = False) -&gt; Union['PrecisionOperator', List['PrecisionOperator']]\n</code></pre> <p>Load an LDGM from a single LD block's edgelist and snplist files.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the .edgelist file or directory containing it</p> required <code>snplist_path</code> <code>Optional[str]</code> <p>Optional path to .snplist file or directory. If None, uses filepath</p> <code>None</code> <code>population</code> <code>Optional[str]</code> <p>Optional population name to filter files and set allele frequency column. Defaults to \"EUR\"</p> <code>'EUR'</code> <code>snps_only</code> <code>bool</code> <p>Import snplist data for SNPs only (smaller memory usage)</p> <code>False</code> <p>Returns:</p> Type Description <code>Union['PrecisionOperator', List['PrecisionOperator']]</code> <p>If filepath is a directory: List of PrecisionOperator instances, one for each edgelist file</p> <code>Union['PrecisionOperator', List['PrecisionOperator']]</code> <p>If filepath is a file: Single PrecisionOperator instance with loaded precision matrix and variant info</p> Source code in <code>src/graphld/io.py</code> <pre><code>def load_ldgm(filepath: str, snplist_path: Optional[str] = None, population: Optional[str] = \"EUR\",\n              snps_only: bool = False) -&gt; Union[\"PrecisionOperator\", List[\"PrecisionOperator\"]]:\n    \"\"\"\n    Load an LDGM from a single LD block's edgelist and snplist files.\n\n    Args:\n        filepath: Path to the .edgelist file or directory containing it\n        snplist_path: Optional path to .snplist file or directory. If None, uses filepath\n        population: Optional population name to filter files and set allele frequency\n            column. Defaults to \"EUR\"\n        snps_only: Import snplist data for SNPs only (smaller memory usage)\n\n    Returns:\n        If filepath is a directory:\n            List of PrecisionOperator instances, one for each edgelist file\n        If filepath is a file:\n            Single PrecisionOperator instance with loaded precision matrix and variant info\n    \"\"\"\n    from scipy.sparse import csc_matrix\n\n    from .precision import PrecisionOperator\n\n    # Handle directory vs file input\n    filepath = Path(filepath)\n    if filepath.is_dir():\n        pattern = \"*.edgelist\"\n        if population:\n            pattern = f\"*{population}*.edgelist\"\n        edgelist_files = list(filepath.glob(pattern))\n        if not edgelist_files:\n            raise FileNotFoundError(f\"No edgelist files found in {filepath}\")\n\n        # Load each file and return a list of PrecisionOperators\n        operators = []\n        for edgelist_file in edgelist_files:\n            operator = load_ldgm(edgelist_file, snplist_path, population, snps_only)\n            operators.append(operator)\n        return operators\n\n    # Use provided snplist path or find corresponding snplist file\n    if snplist_path is None:\n        snplist_path = filepath.parent\n        pattern = filepath.stem.split('.')[0]  # Remove all extensions\n        if pattern.endswith(f\".{population}\"):\n            pattern = pattern[:-len(f\".{population}\")]\n        snplist_files = list(Path(snplist_path).glob(f\"{pattern}*.snplist\"))\n        if not snplist_files:\n            raise FileNotFoundError(f\"No matching snplist file found for {filepath}\")\n        snplist_file = snplist_files[0]\n    else:\n        snplist_file = Path(snplist_path)\n        if not snplist_file.exists():\n            raise FileNotFoundError(f\"Snplist file not found: {snplist_file}\")\n\n    # Load edgelist data\n    edgelist = pl.read_csv(filepath, separator=',', has_header=False,\n                          new_columns=['i', 'j', 'value'])\n\n    # Create sparse matrix\n    matrix = csc_matrix(\n        (edgelist['value'].to_numpy(),\n         (edgelist['i'].to_numpy(), edgelist['j'].to_numpy()))\n    )\n\n    # Make matrix symmetric\n    matrix_t = matrix.T\n    diag_vals = matrix.diagonal().copy()\n    matrix = matrix + matrix_t\n    matrix.setdiag(diag_vals, k=0)\n\n    # Verify diagonal values\n    assert np.allclose(matrix.diagonal(), diag_vals), \"Diagonal values not set correctly\"\n\n    # Create mask for rows/cols with nonzeros on diagonal\n    diag = matrix.diagonal()\n    nonzero_where = np.where(diag != 0)[0]\n    n_nonzero = len(nonzero_where)\n\n    # Load variant info\n    variant_info = pl.read_csv(snplist_file, separator=',')\n    num_rows = variant_info['index'].max() + 1\n\n    # Create mapping from old indices to new indices\n    rows = np.full(num_rows, -1)\n    rows[nonzero_where] = np.arange(n_nonzero)\n\n    # If population is specified and exists as a column, rename it to 'af'\n    if population and population in variant_info.columns:\n        variant_info = variant_info.rename({population: 'af'})\n    elif 'af' not in variant_info.columns:\n        available_cols = \", \".join(variant_info.columns)\n        raise ValueError(\n            f\"Neither 'af' column nor '{population}' column found in snplist. \"\n            f\"Available columns: {available_cols}\"\n        )\n\n    # Store original indices and update with new mapping\n    variant_info = variant_info.with_columns([\n        pl.col('index').alias('original_index'),\n        pl.col('index').map_elements(lambda x: rows[x], return_dtype=pl.Int64).alias('index')\n    ])\n\n    # Filter out variants with no corresponding matrix row\n    variant_info = variant_info.filter(pl.col('index') &gt;= 0)\n\n    # Subset matrix to rows/cols with nonzero diagonal\n    matrix = matrix[nonzero_where][:, nonzero_where]\n\n    return PrecisionOperator(matrix, variant_info)\n</code></pre>"},{"location":"api/heritability/#graphld.heritability.merge_alleles","title":"merge_alleles","text":"<pre><code>merge_alleles(anc_alleles: Series, deriv_alleles: Series, ref_alleles: Series, alt_alleles: Series) -&gt; pl.Series\n</code></pre> <p>Compare alleles between two sources and return phase information.</p> <p>Parameters:</p> Name Type Description Default <code>anc_alleles</code> <code>Series</code> <p>Ancestral alleles from PrecisionOperator</p> required <code>deriv_alleles</code> <code>Series</code> <p>Derived alleles from PrecisionOperator</p> required <code>ref_alleles</code> <code>Series</code> <p>Reference alleles from summary statistics</p> required <code>alt_alleles</code> <code>Series</code> <p>Alternative alleles from summary statistics</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Series of integers indicating phase: 1: Alleles match exactly</p> <code>Series</code> <p>-1: Alleles match but are swapped 0: Alleles do not match</p> Source code in <code>src/graphld/io.py</code> <pre><code>def merge_alleles(anc_alleles: pl.Series, deriv_alleles: pl.Series,\n                  ref_alleles: pl.Series, alt_alleles: pl.Series) -&gt; pl.Series:\n    \"\"\"Compare alleles between two sources and return phase information.\n\n    Args:\n        anc_alleles: Ancestral alleles from PrecisionOperator\n        deriv_alleles: Derived alleles from PrecisionOperator\n        ref_alleles: Reference alleles from summary statistics\n        alt_alleles: Alternative alleles from summary statistics\n\n    Returns:\n        Series of integers indicating phase:\n         1: Alleles match exactly\n        -1: Alleles match but are swapped\n         0: Alleles do not match\n    \"\"\"\n    # Convert to numpy arrays for faster comparison\n    anc = anc_alleles.to_numpy()\n    der = deriv_alleles.to_numpy()\n    ref = ref_alleles.to_numpy()\n    alt = alt_alleles.to_numpy()\n\n    # Make case-insensitive\n    anc = np.char.lower(anc.astype(str))\n    der = np.char.lower(der.astype(str))\n    ref = np.char.lower(ref.astype(str))\n    alt = np.char.lower(alt.astype(str))\n\n    # Check matches\n    exact_match = (anc == ref) &amp; (der == alt)\n    flipped_match = (anc == alt) &amp; (der == ref)\n\n    # Null alleles are given NaN phase so that corresponding Z scores will be NaN\n    null_match = (ref_alleles.is_null().to_numpy() &amp; alt_alleles.is_null().to_numpy())\n\n    # Convert to phase\n    phase = np.zeros(len(anc), dtype=np.float32)\n    phase[exact_match] = 1\n    phase[flipped_match] = -1\n    phase[null_match] = np.nan\n\n    return pl.Series(phase)\n</code></pre>"},{"location":"api/heritability/#graphld.heritability.merge_snplists","title":"merge_snplists","text":"<pre><code>merge_snplists(precision_op: 'PrecisionOperator', sumstats: DataFrame, *, variant_id_col: str = 'SNP', ref_allele_col: str = 'REF', alt_allele_col: str = 'ALT', match_by_position: bool = False, pos_col: str = 'POS', table_format: str = '', add_cols: list[str] = None, add_allelic_cols: list[str] = None, representatives_only: bool = False, modify_in_place: bool = False) -&gt; Tuple['PrecisionOperator', np.ndarray]\n</code></pre> <p>Merge a PrecisionOperator instance with summary statistics DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>precision_op</code> <code>'PrecisionOperator'</code> <p>PrecisionOperator instance</p> required <code>sumstats</code> <code>DataFrame</code> <p>Summary statistics DataFrame</p> required <code>variant_id_col</code> <code>str</code> <p>Column name containing variant IDs</p> <code>'SNP'</code> <code>ref_allele_col</code> <code>str</code> <p>Column name containing reference allele</p> <code>'REF'</code> <code>alt_allele_col</code> <code>str</code> <p>Column name containing alternative allele</p> <code>'ALT'</code> <code>match_by_position</code> <code>bool</code> <p>Whether to match SNPs by position instead of ID</p> <code>False</code> <code>pos_col</code> <code>str</code> <p>Column name containing position</p> <code>'POS'</code> <code>table_format</code> <code>str</code> <p>Optional file format specification (e.g., 'vcf')</p> <code>''</code> <code>add_cols</code> <code>list[str]</code> <p>Optional list of column names from sumstats to append to variant_info</p> <code>None</code> <code>add_allelic_cols</code> <code>list[str]</code> <p>Optional list of column names from sumstats to append to variant_info, multiplied by the phase (-1 or 1) to align with ancestral/derived alleles. If no alleles are provided, these are added without sign-flipping.</p> <code>None</code> <code>modify_in_place</code> <code>bool</code> <p>Whether to modify the PrecisionOperator in place</p> <code>False</code> <p>Returns:</p> Type Description <code>'PrecisionOperator'</code> <p>Tuple containing:</p> <code>ndarray</code> <ul> <li>Modified PrecisionOperator with merged variant info and appended columns</li> </ul> <code>Tuple['PrecisionOperator', ndarray]</code> <ul> <li>Array of indices into sumstats DataFrame indicating which rows were successfully merged</li> </ul> Source code in <code>src/graphld/io.py</code> <pre><code>def merge_snplists(precision_op: \"PrecisionOperator\",\n                   sumstats: pl.DataFrame, *,\n                   variant_id_col: str = 'SNP',\n                   ref_allele_col: str = 'REF',\n                   alt_allele_col: str = 'ALT',\n                   match_by_position: bool = False,\n                   pos_col: str = 'POS',\n                   table_format: str = '',\n                   add_cols: list[str] = None,\n                   add_allelic_cols: list[str] = None,\n                   representatives_only: bool = False,\n                   modify_in_place: bool = False) -&gt; Tuple[\"PrecisionOperator\", np.ndarray]:\n    \"\"\"Merge a PrecisionOperator instance with summary statistics DataFrame.\n\n    Args:\n        precision_op: PrecisionOperator instance\n        sumstats: Summary statistics DataFrame\n        variant_id_col: Column name containing variant IDs\n        ref_allele_col: Column name containing reference allele\n        alt_allele_col: Column name containing alternative allele\n        match_by_position: Whether to match SNPs by position instead of ID\n        pos_col: Column name containing position\n        table_format: Optional file format specification (e.g., 'vcf')\n        add_cols: Optional list of column names from sumstats to append to variant_info\n        add_allelic_cols: Optional list of column names from sumstats to append to variant_info,\n            multiplied by the phase (-1 or 1) to align with ancestral/derived alleles.\n            If no alleles are provided, these are added without sign-flipping.\n        modify_in_place: Whether to modify the PrecisionOperator in place\n\n    Returns:\n        Tuple containing:\n        - Modified PrecisionOperator with merged variant info and appended columns\n        - Array of indices into sumstats DataFrame indicating which rows were successfully merged\n    \"\"\"\n    # Handle VCF format\n    if table_format.lower() == 'vcf':\n        match_by_position = True\n        pos_col = 'POS'\n        ref_allele_col = 'REF'\n        alt_allele_col = 'ALT'\n    elif table_format.lower() == 'ldsc':\n        match_by_position = False\n        ref_allele_col = 'A2'\n        alt_allele_col = 'A1'\n\n    # Find position column\n    pos_options = ['position', 'POS', 'BP']\n    if pos_col is not None:\n        pos_options.insert(0, pos_col)\n    pos_col = next((col for col in pos_options if col in sumstats.columns), None)\n    if pos_col is None:\n        raise ValueError(\n            f\"Could not find position column. Tried: {', '.join(pos_options)}\"\n        )\n\n    # Validate inputs\n    if match_by_position:\n        if pos_col not in sumstats.columns:\n            msg = (f\"Summary statistics must contain {pos_col} column \"\n                  f\"for position matching. Found columns: {', '.join(sumstats.columns)}\")\n            raise ValueError(msg)\n    else:\n        if variant_id_col not in sumstats.columns:\n            msg = (f\"Summary statistics must contain {variant_id_col} column. \"\n                  f\"Found columns: {', '.join(sumstats.columns)}\")\n            raise ValueError(msg)\n\n    # Match variants\n    match_by = ('position', pos_col) if match_by_position else ('site_ids', variant_id_col)\n    merged = precision_op.variant_info.join(\n        sumstats.with_row_index(name=\"row_nr\"),\n        left_on=[match_by[0]],\n        right_on=[match_by[1]],\n        suffix=\"_sumstats\",\n        how='inner'\n    )\n\n    # Check alleles if provided\n    phase = 1\n    if all(col in sumstats.columns for col in [ref_allele_col, alt_allele_col]):\n        phase = merge_alleles(\n            merged['anc_alleles'],\n            merged['deriv_alleles'],\n            merged[ref_allele_col],\n            merged[alt_allele_col]\n        ).alias('phase')\n        merged = merged.with_columns(phase)\n\n        # Update indices to only include variants with matching alleles\n        merged = merged.filter(pl.col('phase') != 0)\n        phase = merged['phase'].to_numpy()\n\n    add_cols = add_cols or []\n    add_allelic_cols = add_allelic_cols or []\n    new_cols = {}\n\n    # Check all columns exist\n    missing_cols = [col for col in add_cols + add_allelic_cols if col not in sumstats.columns]\n    if missing_cols:\n        msg = (f\"Requested columns not found in sumstats: {', '.join(missing_cols)}. \"\n              f\"Available columns: {', '.join(sumstats.columns)}\")\n        raise ValueError(msg)\n\n    # Add columns with appropriate transformations\n    for col in add_cols:\n        new_cols[col] = pl.col(col)\n\n    for col in add_allelic_cols:\n        new_cols[col] = pl.col(col) * phase\n\n    # Add all new columns at once if any\n    if new_cols:\n        merged = merged.with_columns(**new_cols)\n\n    # Sort by index and add is_representative column\n    merged = (\n        merged\n        .sort('index')\n        .with_columns(\n            pl.col('index').is_first_distinct().cast(pl.Int8).alias('is_representative')\n        )\n    )\n\n    # Create new PrecisionOperator with merged variant info\n    unique_indices = np.unique(merged['index'].to_numpy())\n    if modify_in_place:\n        precision_op.set_which_indices(unique_indices)\n    else:\n        precision_op = precision_op[unique_indices]\n\n    # Create mapping from old indices to new contiguous ones\n    index_map = {old_idx: new_idx for new_idx, old_idx in enumerate(unique_indices)}\n\n    # Update indices in merged data to be contiguous using efficient replace_strict\n    merged = merged.with_columns(\n        pl.col('index').replace_strict(index_map).alias('index')\n    )\n\n    if representatives_only:\n        merged = merged.filter(pl.col('is_representative') == 1)\n\n    precision_op.variant_info = merged\n    sumstat_indices = merged.select('row_nr').to_numpy().flatten().astype(int)\n\n    return precision_op, sumstat_indices\n</code></pre>"},{"location":"api/heritability/#graphld.heritability.partition_variants","title":"partition_variants","text":"<pre><code>partition_variants(ldgm_metadata: DataFrame, variant_data: DataFrame, *, chrom_col: Optional[str] = None, pos_col: Optional[str] = None) -&gt; List[pl.DataFrame]\n</code></pre> <p>Partition variant data according to LDGM blocks.</p> <p>Parameters:</p> Name Type Description Default <code>ldgm_metadata</code> <code>DataFrame</code> <p>DataFrame from read_ldgm_metadata containing block info</p> required <code>variant_data</code> <code>DataFrame</code> <p>DataFrame containing variant information</p> required <code>chrom_col</code> <code>Optional[str]</code> <p>Optional name of chromosome column. If None, tries common names</p> <code>None</code> <code>pos_col</code> <code>Optional[str]</code> <p>Optional name of position column. If None, tries common names</p> <code>None</code> <p>Returns:</p> Type Description <code>List[DataFrame]</code> <p>List of DataFrames, one per row in ldgm_metadata, containing variants</p> <code>List[DataFrame]</code> <p>that fall within each block's coordinates</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the file does not have the expected columns: variant_id, chromosome, position, ...</p> Source code in <code>src/graphld/io.py</code> <pre><code>def partition_variants(\n    ldgm_metadata: pl.DataFrame,\n    variant_data: pl.DataFrame,\n    *,\n    chrom_col: Optional[str] = None,\n    pos_col: Optional[str] = None\n) -&gt; List[pl.DataFrame]:\n    \"\"\"Partition variant data according to LDGM blocks.\n\n    Args:\n        ldgm_metadata: DataFrame from read_ldgm_metadata containing block info\n        variant_data: DataFrame containing variant information\n        chrom_col: Optional name of chromosome column. If None, tries common names\n        pos_col: Optional name of position column. If None, tries common names\n\n    Returns:\n        List of DataFrames, one per row in ldgm_metadata, containing variants\n        that fall within each block's coordinates\n\n    Raises:\n        ValueError: If the file does not have the expected columns: variant_id,\n            chromosome, position, ...\n    \"\"\"\n    # Find chromosome column\n    chrom_options = ['chrom', 'chromosome', 'CHR']\n    if chrom_col is not None:\n        chrom_options.insert(0, chrom_col)\n    chrom_col = next((col for col in chrom_options if col in variant_data.columns), None)\n    if chrom_col is None:\n        raise ValueError(\n            f\"Could not find chromosome column. Tried: {', '.join(chrom_options)}\"\n        )\n\n    # Find position column\n    pos_options = ['position', 'POS', 'BP']\n    if pos_col is not None:\n        pos_options.insert(0, pos_col)\n    pos_col = next((col for col in pos_options if col in variant_data.columns), None)\n    if pos_col is None:\n        raise ValueError(\n            f\"Could not find position column. Tried: {', '.join(pos_options)}\"\n        )\n\n    # Convert chromosome column to integer if needed\n    if variant_data[chrom_col].dtype != pl.Int64:\n        variant_data = variant_data.with_columns(\n            pl.col(chrom_col).cast(pl.Int64).alias(chrom_col)\n        )\n\n    # First sort variants by chromosome and position\n    sorted_variants = variant_data.sort([chrom_col, pos_col])\n\n    # Group blocks by chromosome\n    chrom_blocks = {}\n    for block in ldgm_metadata.iter_rows(named=True):\n        chrom = block['chrom']\n        if chrom not in chrom_blocks:\n            chrom_blocks[chrom] = []\n        chrom_blocks[chrom].append(block)\n\n    # Process each chromosome's blocks at once\n    partitioned = []\n    for chrom, blocks in chrom_blocks.items():\n        # Get all variants for this chromosome\n        chrom_variants = sorted_variants.filter(pl.col(chrom_col) == chrom)\n        if len(chrom_variants) == 0:\n            partitioned.extend([pl.DataFrame()] * len(blocks))\n            continue\n\n        # Get positions array for binary search\n        positions = chrom_variants.get_column(pos_col).to_numpy()\n\n        # Process each block\n        for block in blocks:\n            # Binary search for block boundaries\n            start_idx = np.searchsorted(positions, block['chromStart'])\n            end_idx = np.searchsorted(positions, block['chromEnd'])\n\n            # Extract variants for this block\n            block_variants = chrom_variants.slice(start_idx, end_idx - start_idx)\n            partitioned.append(block_variants)\n\n\n    return partitioned\n</code></pre>"},{"location":"api/heritability/#graphld.heritability.create_ldgm_metadata","title":"create_ldgm_metadata","text":"<pre><code>create_ldgm_metadata(directory: Union[str, Path], output_file: Optional[str] = None) -&gt; pl.DataFrame\n</code></pre> <p>Create metadata file for LDGM files in a directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Union[str, Path]</code> <p>Directory containing .snplist and .edgelist files</p> required <code>output_file</code> <code>Optional[str]</code> <p>Optional path to write CSV file. If None, only returns DataFrame</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Polars DataFrame containing metadata for each LDGM file</p> Source code in <code>src/graphld/io.py</code> <pre><code>def create_ldgm_metadata(\n    directory: Union[str, Path], output_file: Optional[str] = None\n) -&gt; pl.DataFrame:\n    \"\"\"Create metadata file for LDGM files in a directory.\n\n    Args:\n        directory: Directory containing .snplist and .edgelist files\n        output_file: Optional path to write CSV file. If None, only returns DataFrame\n\n    Returns:\n        Polars DataFrame containing metadata for each LDGM file\n    \"\"\"\n    directory = Path(directory)\n    if not directory.exists():\n        raise FileNotFoundError(f\"Directory not found: {directory}\")\n\n    # Find all edgelist files\n    edgelist_files = list(directory.glob(\"*.edgelist\"))\n    if not edgelist_files:\n        raise FileNotFoundError(f\"No .edgelist files found in {directory}\")\n\n    # Process each file\n    data = []\n    for edgefile in edgelist_files:\n        # Parse filename to get info\n        name = edgefile.name\n        parts = name.split('_')  # e.g. 1kg_chr1_2888443_4320284.EUR.edgelist\n        if len(parts) &lt; 4:\n            print(f\"Skipping {name}: unexpected filename format\")\n            continue\n\n        # Get chromosome and positions\n        try:\n            chrom = int(parts[1].replace('chr', ''))\n            chromStart = int(parts[2])\n            chromEnd = int(parts[3].split('.')[0])  # Remove population/extension\n        except (ValueError, IndexError):\n            print(f\"Skipping {name}: could not parse chromosome/position\")\n            continue\n\n        # Get population\n        try:\n            population = name.split('.')[-2]  # Second to last part\n        except IndexError:\n            print(f\"Skipping {name}: could not parse population\")\n            continue\n\n        # Find corresponding snplist file\n        base_name = name.split('.')[0]  # Remove population and extension\n        snplist_files = list(directory.glob(f\"{base_name}.snplist\"))\n        if not snplist_files:\n            print(f\"Skipping {name}: no matching .snplist file\")\n            continue\n        snplist_name = snplist_files[0].name\n\n        # Count variants in snplist\n        try:\n            snplist_df = pl.read_csv(snplist_files[0])\n            num_variants = len(snplist_df)\n        except Exception as e:\n            print(f\"Skipping {name}: error reading snplist: {e}\")\n            continue\n\n        # Count entries in edgelist\n        try:\n            edgelist_df = pl.read_csv(edgefile, has_header=False,\n                                    new_columns=['i', 'j', 'value'])\n\n            # Count unique diagonal indices\n            diag_mask = edgelist_df['i'] == edgelist_df['j']\n            num_indices = len(edgelist_df.filter(diag_mask)['i'].unique())\n\n            # Total number of entries\n            num_entries = len(edgelist_df)\n\n        except Exception as e:\n            print(f\"Skipping {name}: error reading edgelist: {e}\")\n            continue\n\n        # Add row to metadata\n        data.append({\n            'chrom': chrom,\n            'chromStart': chromStart,\n            'chromEnd': chromEnd,\n            'name': name,\n            'snplistName': snplist_name,\n            'population': population,\n            'numVariants': num_variants,\n            'numIndices': num_indices,\n            'numEntries': num_entries,\n            'info': ''\n        })\n\n    # Create DataFrame\n    if not data:\n        raise ValueError(\"No valid LDGM files found\")\n\n    df = pl.DataFrame(data)\n\n    # Sort by chromosome and start position\n    df = df.sort(['chrom', 'chromStart'])\n\n    # Write to file if requested\n    if output_file:\n        df.write_csv(output_file)\n\n    return df\n</code></pre>"},{"location":"api/heritability/#graphld.heritability.read_ldgm_metadata","title":"read_ldgm_metadata","text":"<pre><code>read_ldgm_metadata(filepath: Union[str, Path], *, populations: Optional[Union[str, List[str]]] = None, chromosomes: Optional[Union[int, List[int]]] = None, max_blocks: Optional[int] = None) -&gt; pl.DataFrame\n</code></pre> <p>Read LDGM metadata from CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>Union[str, Path]</code> <p>Path to metadata CSV file</p> required <code>populations</code> <code>Optional[Union[str, List[str]]]</code> <p>Optional population(s) to filter by</p> <code>None</code> <code>chromosomes</code> <code>Optional[Union[int, List[int]]]</code> <p>Optional chromosome(s) to filter by</p> <code>None</code> <code>max_blocks</code> <code>Optional[int]</code> <p>Optional maximum number of blocks to return</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Polars DataFrame containing LDGM metadata, filtered by population and chromosome</p> <code>DataFrame</code> <p>if specified, and limited to max_blocks if specified</p> Source code in <code>src/graphld/io.py</code> <pre><code>def read_ldgm_metadata(\n    filepath: Union[str, Path],\n    *,\n    populations: Optional[Union[str, List[str]]] = None,\n    chromosomes: Optional[Union[int, List[int]]] = None,\n    max_blocks: Optional[int] = None\n) -&gt; pl.DataFrame:\n    \"\"\"Read LDGM metadata from CSV file.\n\n    Args:\n        filepath: Path to metadata CSV file\n        populations: Optional population(s) to filter by\n        chromosomes: Optional chromosome(s) to filter by\n        max_blocks: Optional maximum number of blocks to return\n\n    Returns:\n        Polars DataFrame containing LDGM metadata, filtered by population and chromosome\n        if specified, and limited to max_blocks if specified\n    \"\"\"\n    try:\n        df = pl.read_csv(filepath)\n        required_cols = [\n            'chrom', 'chromStart', 'chromEnd', 'name', 'snplistName',\n            'population', 'numVariants', 'numIndices', 'numEntries', 'info'\n        ]\n        missing = [col for col in required_cols if col not in df.columns]\n        if missing:\n            raise ValueError(f\"Missing required columns: {', '.join(missing)}\")\n\n        # Filter by population if specified\n        if populations is not None:\n            if isinstance(populations, str):\n                populations = [populations]\n            df = df.filter(pl.col('population').is_in(populations))\n            if len(df) == 0:\n                raise ValueError(f\"No blocks found for populations: {populations}\")\n\n        # Filter by chromosome if specified\n        if chromosomes is not None:\n            if isinstance(chromosomes, int):\n                chromosomes = [chromosomes]\n            df = df.filter(pl.col('chrom').is_in(chromosomes))\n            if len(df) == 0:\n                raise ValueError(f\"No blocks found for chromosomes: {chromosomes}\")\n\n        # Sort by chromosome and position\n        df = df.sort(['chrom', 'chromStart'])\n\n        # Limit number of blocks if specified\n        if max_blocks is not None:\n            df = df.head(max_blocks)\n\n        return df\n\n    except Exception as e:\n        raise ValueError(f\"Error reading metadata file: {e}\") from e\n</code></pre>"},{"location":"api/heritability/#graphld.heritability.load_annotations","title":"load_annotations","text":"<pre><code>load_annotations(annot_path: str, chromosome: Optional[int] = None, infer_schema_length: int = 100000, add_alleles: bool = False, add_positions: bool = True, positions_file: str = POSITIONS_FILE, file_pattern: str = 'baselineLD.{chrom}.annot', exclude_bed: bool = False) -&gt; pl.DataFrame\n</code></pre> <p>Load annotation data for specified chromosome(s) and merge with LDGMs data.</p> <p>Parameters:</p> Name Type Description Default <code>annot_path</code> <code>str</code> <p>Path to directory containing annotation files</p> required <code>chromosome</code> <code>Optional[int]</code> <p>Specific chromosome number, or None for all chromosomes</p> <code>None</code> <code>infer_schema_length</code> <code>int</code> <p>Number of rows to infer schema from. Runs faster if this is smaller</p> <code>100000</code> <code>file_pattern</code> <code>str</code> <p>Filename pattern to match, with {chrom} as a placeholder for chromosome number</p> <code>'baselineLD.{chrom}.annot'</code> <code>exclude_bed</code> <code>bool</code> <p>If True, skip loading .bed files from the annotations directory</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing annotations</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no matching annotation files are found</p> Source code in <code>src/graphld/io.py</code> <pre><code>def load_annotations(annot_path: str,\n                    chromosome: Optional[int] = None,\n                    infer_schema_length: int = 100_000,\n                    add_alleles: bool = False,\n                    add_positions: bool = True,\n                    positions_file: str = POSITIONS_FILE,\n                    file_pattern: str = \"baselineLD.{chrom}.annot\",\n                    exclude_bed: bool = False\n                    ) -&gt; pl.DataFrame:\n    \"\"\"Load annotation data for specified chromosome(s) and merge with LDGMs data.\n\n    Args:\n        annot_path: Path to directory containing annotation files\n        chromosome: Specific chromosome number, or None for all chromosomes\n        infer_schema_length: Number of rows to infer schema from. Runs faster if this is smaller\n        but will throw an error if too small because floating-point columns will be\n        cast as integers.\n        file_pattern: Filename pattern to match, with {chrom} as a placeholder for chromosome number\n        exclude_bed: If True, skip loading .bed files from the annotations directory\n\n    Returns:\n        DataFrame containing annotations\n\n    Raises:\n        ValueError: If no matching annotation files are found\n    \"\"\"\n    import glob\n    import os\n\n    # Determine which chromosomes to process\n    if chromosome is not None:\n        chromosomes = [chromosome]\n    else:\n        chromosomes = range(1, 23)  # Assuming chromosomes 1-22\n\n    # Find matching files\n    annotations = []\n    for chromosome in chromosomes:\n        file_pattern = f\"*.{chromosome}.annot\"\n        matching_files = Path(annot_path).glob(file_pattern)\n\n        # Read all matching files for this chromosome\n        dfs = []\n        seen_cols = set()\n        for file_path in matching_files:\n            df = pl.scan_csv(file_path, separator='\\t', infer_schema_length=infer_schema_length)\n            # Drop columns that were already seen in previous files to avoid duplicates\n            schema_names = df.collect_schema().names()\n            cols_to_keep = [col for col in schema_names if col not in seen_cols]\n            if cols_to_keep:\n                df = df.select(cols_to_keep)\n                seen_cols.update(cols_to_keep)\n                dfs.append(df)\n\n        # Horizontally concatenate all dataframes for this chromosome\n        if dfs:\n            combined_df = pl.concat(dfs, how=\"horizontal\")\n            annotations.append(combined_df)\n\n    # Check if any files were found\n    if not annotations:\n        raise ValueError(\n            f\"No annotation files found in {annot_path} matching pattern {file_pattern}\"\n        )\n\n    # Concatenate all chromosome dataframes and handle different schemas\n    annotations = pl.concat(annotations, how=\"diagonal_relaxed\").collect()\n\n    # Convert binary columns to boolean to save memory\n    numeric_types = (pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.Float32, pl.Float64)\n    binary_cols = []\n    for col in annotations.columns:\n        # Skip non-numeric columns\n        if not isinstance(annotations[col].dtype, numeric_types):\n            continue\n        # Check if column only contains 0, 1 and null values\n        unique_vals = set(annotations[col].unique().drop_nulls())\n        if unique_vals == {0, 1}:\n            binary_cols.append(col)\n\n    # Convert binary columns to boolean\n    if binary_cols:\n        bool_exprs = [pl.col(col).cast(pl.Boolean) for col in binary_cols]\n        annotations = annotations.with_columns(bool_exprs)\n\n    if add_positions or add_alleles:\n        snplist_data = pl.read_csv(\n            positions_file,\n            separator=',',\n            columns=['chrom', 'site_ids', 'position', 'anc_alleles', 'deriv_alleles']\n        )\n\n        snplist_data = snplist_data.rename({\n                'chrom': 'CHR',\n                'site_ids': 'SNP',\n                'position': 'POS',\n                'anc_alleles': 'A2',\n                'deriv_alleles': 'A1'\n            })\n\n        with_columns = ['SNP']\n        if add_positions:\n            with_columns += ['CHR', 'POS']\n        if add_alleles:\n            with_columns += ['A2', 'A1']\n\n        snplist_data = snplist_data.select(with_columns)\n\n        # Existing coordinates might be in wrong genome build\n        annotations = annotations.drop(['CHR', 'BP'])\n\n        # Merge with positions\n        annotations = annotations.join(\n            snplist_data,\n            on='SNP',\n            how='inner'\n        )\n\n    if not add_positions:\n        annotations = annotations.rename({'BP': 'POS'})\n\n    bed_files = glob.glob(os.path.join(annot_path, \"*.bed\"))\n    if exclude_bed or not bed_files:\n        return annotations\n\n    # Process BED files if they exist\n\n    # Create a dictionary to store new annotation columns\n    bed_annotations = {}\n    for bed_file in bed_files:\n        # Get the name for this annotation from the filename\n        bed_name = os.path.splitext(os.path.basename(bed_file))[0]\n\n        # Read the BED file\n        bed_df = read_bed(bed_file)\n\n        # Convert chromosome names to match our format (e.g., \"chr1\" -&gt; 1)\n        bed_df = bed_df.with_columns([\n            pl.col(\"chrom\").str.replace(\"chr\", \"\").cast(pl.Int64).alias(\"chrom\")\n        ])\n\n        new_annot = np.zeros(len(annotations), dtype=bool)\n\n        # Group BED regions by chromosome\n        unique_chromosomes = annotations.get_column(\"CHR\").unique()\n        for chrom in unique_chromosomes:\n            chrom_indices = (annotations[\"CHR\"] == chrom).to_numpy()\n            if not chrom_indices.any():\n                continue\n            bed_regions = bed_df.filter(bed_df[\"chrom\"] == chrom).select(\"chromStart\", \"chromEnd\").to_numpy()\n            positions = annotations.filter(annotations[\"CHR\"] == chrom)[\"POS\"].to_numpy()\n            new_annot[chrom_indices] = _get_range_mask(\n                values=positions,\n                start=bed_regions[:, 0],\n                end=bed_regions[:, 1]\n                )\n\n        bed_annotations[bed_name] = new_annot\n\n    # Add all BED annotations to the main DataFrame\n    annotations = annotations.with_columns(**bed_annotations)\n\n    return annotations\n</code></pre>"},{"location":"api/heritability/#graphld.heritability.read_bed","title":"read_bed","text":"<pre><code>read_bed(bed_file: str, min_fields: int = 3, max_fields: int = 12, zero_based: bool = True) -&gt; pl.DataFrame\n</code></pre> <p>Read a UCSC BED format file.</p> <p>The BED format has 3 required fields and 9 optional fields: Required:     1. chrom - Chromosome name     2. chromStart - Start position (0-based)     3. chromEnd - End position (not included in feature) Optional:     4. name - Name of BED line     5. score - Score from 0-1000     6. strand - Strand: \"+\" or \"-\" or \".\"     7. thickStart - Starting position at which feature is drawn thickly     8. thickEnd - Ending position at which feature is drawn thickly     9. itemRgb - RGB value (e.g., \"255,0,0\")     10. blockCount - Number of blocks (e.g., exons)     11. blockSizes - Comma-separated list of block sizes     12. blockStarts - Comma-separated list of block starts relative to chromStart</p> <p>Parameters:</p> Name Type Description Default <code>bed_file</code> <code>str</code> <p>Path to BED format file</p> required <code>min_fields</code> <code>int</code> <p>Minimum number of fields required (default: 3)</p> <code>3</code> <code>max_fields</code> <code>int</code> <p>Maximum number of fields to read (default: 12)</p> <code>12</code> <code>zero_based</code> <code>bool</code> <p>If True (default), keeps positions 0-based. If False, adds 1 to start positions.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Polars DataFrame containing the BED data with appropriate column names and types.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If min_fields &lt; 3 or max_fields &gt; 12 or if file has inconsistent number of fields</p> Source code in <code>src/graphld/io.py</code> <pre><code>def read_bed(bed_file: str,\n             min_fields: int = 3,\n             max_fields: int = 12,\n             zero_based: bool = True) -&gt; pl.DataFrame:\n    \"\"\"Read a UCSC BED format file.\n\n    The BED format has 3 required fields and 9 optional fields:\n    Required:\n        1. chrom - Chromosome name\n        2. chromStart - Start position (0-based)\n        3. chromEnd - End position (not included in feature)\n    Optional:\n        4. name - Name of BED line\n        5. score - Score from 0-1000\n        6. strand - Strand: \"+\" or \"-\" or \".\"\n        7. thickStart - Starting position at which feature is drawn thickly\n        8. thickEnd - Ending position at which feature is drawn thickly\n        9. itemRgb - RGB value (e.g., \"255,0,0\")\n        10. blockCount - Number of blocks (e.g., exons)\n        11. blockSizes - Comma-separated list of block sizes\n        12. blockStarts - Comma-separated list of block starts relative to chromStart\n\n    Args:\n        bed_file: Path to BED format file\n        min_fields: Minimum number of fields required (default: 3)\n        max_fields: Maximum number of fields to read (default: 12)\n        zero_based: If True (default), keeps positions 0-based. If False, adds 1 to start positions.\n\n    Returns:\n        Polars DataFrame containing the BED data with appropriate column names and types.\n\n    Raises:\n        ValueError: If min_fields &lt; 3 or max_fields &gt; 12 or if file has\n            inconsistent number of fields\n    \"\"\"\n    if min_fields &lt; 3:\n        raise ValueError(\"BED format requires at least 3 fields\")\n    if max_fields &gt; 12:\n        raise ValueError(\"BED format has at most 12 fields\")\n    if min_fields &gt; max_fields:\n        raise ValueError(\"min_fields cannot be greater than max_fields\")\n\n    # Define all possible BED columns with their types\n    bed_columns = [\n        ('chrom', pl.Utf8),\n        ('chromStart', pl.Int64),\n        ('chromEnd', pl.Int64),\n        ('name', pl.Utf8),\n        ('score', pl.Int64),\n        ('strand', pl.Utf8),\n        ('thickStart', pl.Int64),\n        ('thickEnd', pl.Int64),\n        ('itemRgb', pl.Utf8),\n        ('blockCount', pl.Int64),\n        ('blockSizes', pl.Utf8),\n        ('blockStarts', pl.Utf8)\n    ]\n\n    # Read and parse the file\n    data = []\n    with open(bed_file) as f:\n        for line in f:\n            line = line.strip()\n            if not line or line.startswith(('browser', 'track', '#')):\n                continue\n            # Split on tabs or spaces and filter out empty strings\n            fields = [f for f in line.split() if f]\n            if not (min_fields &lt;= len(fields) &lt;= max_fields):\n                raise ValueError(\n                    f\"BED line has {len(fields)} fields, expected between \"\n                    f\"{min_fields} and {max_fields}: {line}\"\n                )\n            # Pad with None if we have fewer than max_fields\n            fields.extend([None] * (max_fields - len(fields)))\n            data.append(fields[:max_fields])\n\n    # Create schema for required fields\n    schema = {name: dtype for name, dtype in bed_columns[:max_fields]}\n\n    # Create DataFrame\n    df = pl.from_records(\n        data,\n        schema=schema,\n        orient=\"row\"\n    )\n\n    # Convert 0-based to 1-based coordinates if requested\n    if not zero_based:\n        df = df.with_columns([\n            pl.col('chromStart') + 1\n        ])\n        # Also convert thick positions if present\n        if 'thickStart' in df.columns:\n            df = df.with_columns([\n                pl.col('thickStart') + 1\n            ])\n\n    return df\n</code></pre>"},{"location":"api/heritability/#graphld.heritability.read_concat_snplists","title":"read_concat_snplists","text":"<pre><code>read_concat_snplists(ldgm_metadata: DataFrame, parent_dir: Path) -&gt; pl.LazyFrame\n</code></pre> <p>Read and concatenate snplists from LDGM metadata.</p> <p>Parameters:</p> Name Type Description Default <code>ldgm_metadata</code> <code>DataFrame</code> <p>DataFrame from read_ldgm_metadata containing block info</p> required <p>Returns:</p> Type Description <code>LazyFrame</code> <p>LazyFrame containing variant information concatenated across blocks.</p> Source code in <code>src/graphld/io.py</code> <pre><code>def read_concat_snplists(ldgm_metadata: pl.DataFrame, parent_dir: Path) -&gt; pl.LazyFrame:\n    \"\"\"Read and concatenate snplists from LDGM metadata.\n\n    Args:\n        ldgm_metadata: DataFrame from read_ldgm_metadata containing block info\n\n    Returns:\n        LazyFrame containing variant information concatenated across blocks.\n    \"\"\"\n    ldgms = [\n        load_ldgm(parent_dir / Path(row[\"name\"])) for row in ldgm_metadata.iter_rows(named=True)\n    ]\n    lazy_frames = [\n        ldgm.variant_info.lazy().with_columns(pl.lit(i).alias('block'))\n        for i, ldgm in enumerate(ldgms)\n    ]\n    return pl.concat(lazy_frames)\n</code></pre>"},{"location":"api/heritability/#graphld.heritability.gaussian_likelihood","title":"gaussian_likelihood","text":"<pre><code>gaussian_likelihood(pz: ndarray, M: PrecisionOperator) -&gt; float\n</code></pre> <p>Compute log-likelihood of GWAS summary statistics under a Gaussian model.</p> The model is <p>beta ~ MVN(0, D) z|beta ~ MVN(sqrt(n)Rbeta, R) where R is the LD matrix, n the sample size pz = inv(R) * z / sqrt(n) M = cov(pz) = D + inv(R)/n</p> <p>Parameters:</p> Name Type Description Default <code>pz</code> <code>ndarray</code> <p>Array of precision-premultiplied GWAS effect size estimates</p> required <code>M</code> <code>PrecisionOperator</code> <p>PrecisionOperator. This should be the covariance of pz.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Log-likelihood value</p> Source code in <code>src/graphld/likelihood.py</code> <pre><code>def gaussian_likelihood(\n    pz: np.ndarray,\n    M: PrecisionOperator,\n) -&gt; float:\n    \"\"\"Compute log-likelihood of GWAS summary statistics under a Gaussian model.\n\n    The model is:\n        beta ~ MVN(0, D)\n        z|beta ~ MVN(sqrt(n)*R*beta, R) where R is the LD matrix, n the sample size\n        pz = inv(R) * z / sqrt(n)\n        M = cov(pz) = D + inv(R)/n\n\n    Args:\n        pz: Array of precision-premultiplied GWAS effect size estimates\n        M: PrecisionOperator. This should be the covariance of pz.\n\n    Returns:\n        Log-likelihood value\n\n    \"\"\"\n    # Following scipy's convention:\n    # log_pdf = -0.5 * (n * log(2\u03c0) + log|\u03a3| + x^T \u03a3^{-1} x)\n    #        = -0.5 * (n * log(2\u03c0) - log|P| + x^T P x)\n    n = len(pz)\n    logdet = M.logdet()\n\n    # Compute quadratic form\n    b = M.solve(pz)\n    quad = np.sum(pz * b)\n\n    # Compute log likelihood\n    ll = -0.5 * (n * np.log(2 * np.pi) + logdet + quad)\n\n    return ll\n</code></pre>"},{"location":"api/heritability/#graphld.heritability.gaussian_likelihood_gradient","title":"gaussian_likelihood_gradient","text":"<pre><code>gaussian_likelihood_gradient(pz: ndarray, M: PrecisionOperator, del_M_del_a: Optional[ndarray] = None, n_samples: int = 10, seed: Optional[int] = None, trace_estimator: Optional[str] = 'xdiag') -&gt; np.ndarray\n</code></pre> <p>Computes the score under a Gaussian model.</p> <pre><code>The model is:\nbeta ~ MVN(0, D)\nz|beta ~ MVN(sqrt(n)*R*beta, R) where R is the LD matrix, n the sample size\npz = inv(R) * z / sqrt(n)\nM = cov(pz) = D + inv(R)/n\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pz</code> <code>ndarray</code> <p>Array of precision-premultiplied GWAS effect size estimates</p> required <code>M</code> <code>PrecisionOperator</code> <p>PrecisionOperator. This should be the covariance of pz.</p> required <code>del_M_del_a</code> <code>Optional[ndarray]</code> <p>Matrix of derivatives of M's diagonal elements wrt parameters a</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>Number of probe vectors for Hutchinson's method or xdiag</p> <code>10</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for generating probe vectors</p> <code>None</code> <code>trace_estimator</code> <code>Optional[str]</code> <p>Method for computing the trace estimator. Options: \"exact\", \"hutchinson\", \"xdiag\"</p> <code>'xdiag'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of diagonal elements of the gradient wrt M's diagonal elements,</p> <code>ndarray</code> <p>or with respect to parameters a if del_M_del_a is provided</p> Source code in <code>src/graphld/likelihood.py</code> <pre><code>def gaussian_likelihood_gradient(\n    pz: np.ndarray,\n    M: PrecisionOperator,\n    del_M_del_a: Optional[np.ndarray] = None,\n    n_samples: int = 10,\n    seed: Optional[int] = None,\n    trace_estimator: Optional[str] = \"xdiag\",\n) -&gt; np.ndarray:\n    \"\"\"Computes the score under a Gaussian model.\n\n        The model is:\n        beta ~ MVN(0, D)\n        z|beta ~ MVN(sqrt(n)*R*beta, R) where R is the LD matrix, n the sample size\n        pz = inv(R) * z / sqrt(n)\n        M = cov(pz) = D + inv(R)/n\n\n    Args:\n        pz: Array of precision-premultiplied GWAS effect size estimates\n        M: PrecisionOperator. This should be the covariance of pz.\n        del_M_del_a: Matrix of derivatives of M's diagonal elements wrt parameters a\n        n_samples: Number of probe vectors for Hutchinson's method or xdiag\n        seed: Random seed for generating probe vectors\n        trace_estimator: Method for computing the trace estimator.\n            Options: \"exact\", \"hutchinson\", \"xdiag\"\n\n    Returns:\n        Array of diagonal elements of the gradient wrt M's diagonal elements,\n        or with respect to parameters a if del_M_del_a is provided\n    \"\"\"\n    # Compute b = M^(-1) * pz\n    b = M.solve(pz)\n\n    # Compute diagonal elements of M^(-1)\n    minv_diag = M.inverse_diagonal(method=trace_estimator,\n                                    n_samples=n_samples,\n                                    seed=seed)\n\n    # Compute gradient diagonal elements\n    node_grad = -0.5 * (minv_diag.flatten() - b.flatten()**2)\n\n    return node_grad if del_M_del_a is None else node_grad @ del_M_del_a\n</code></pre>"},{"location":"api/heritability/#graphld.heritability.gaussian_likelihood_hessian","title":"gaussian_likelihood_hessian","text":"<pre><code>gaussian_likelihood_hessian(pz: ndarray, M: PrecisionOperator, del_M_del_a: Optional[ndarray] = None, diagonal_method: Optional[str] = None, n_samples: int = 100, seed: Optional[int] = None) -&gt; np.ndarray\n</code></pre> <p>Computes the average information matrix of the Gaussian log-likelihood.</p> The model is <p>beta ~ MVN(0, D) z|beta ~ MVN(sqrt(n)Rbeta, R) where R is the LD matrix, n the sample size pz = inv(R) * z / sqrt(n) M = cov(pz) = D + inv(R)/n</p> <p>Parameters:</p> Name Type Description Default <code>pz</code> <code>ndarray</code> <p>Array of precision-premultiplied GWAS effect size estimates</p> required <code>M</code> <code>PrecisionOperator</code> <p>PrecisionOperator. This should be the covariance of pz.</p> required <code>del_M_del_a</code> <code>Optional[ndarray]</code> <p>Matrix of derivatives of M's diagonal elements wrt parameters a. If None, only the diagonal elements are computed.</p> <code>None</code> <code>diagonal_method</code> <code>Optional[str]</code> <p>Method for computing the diagonal of the Hessian. Options: \"exact\", \"hutchinson\", \"xdiag\", None (default)</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>Number of probe vectors for Hutchinson's method or xdiag</p> <code>100</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for generating probe vectors</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Matrix of second derivatives wrt parameters a, or array of diagonal elements</p> <code>ndarray</code> <p>if del_M_del_a is None</p> Source code in <code>src/graphld/likelihood.py</code> <pre><code>def gaussian_likelihood_hessian(\n    pz: np.ndarray,\n    M: PrecisionOperator,\n    del_M_del_a: Optional[np.ndarray] = None,\n    diagonal_method: Optional[str]=None,\n    n_samples: int=100,\n    seed: Optional[int] = None,\n) -&gt; np.ndarray:\n    \"\"\"Computes the average information matrix of the Gaussian log-likelihood.\n\n    The model is:\n        beta ~ MVN(0, D)\n        z|beta ~ MVN(sqrt(n)*R*beta, R) where R is the LD matrix, n the sample size\n        pz = inv(R) * z / sqrt(n)\n        M = cov(pz) = D + inv(R)/n\n\n    Args:\n        pz: Array of precision-premultiplied GWAS effect size estimates\n        M: PrecisionOperator. This should be the covariance of pz.\n        del_M_del_a: Matrix of derivatives of M's diagonal elements wrt parameters a.\n            If None, only the diagonal elements are computed.\n        diagonal_method: Method for computing the diagonal of the Hessian.\n            Options: \"exact\", \"hutchinson\", \"xdiag\", None (default)\n        n_samples: Number of probe vectors for Hutchinson's method or xdiag\n        seed: Random seed for generating probe vectors\n\n    Returns:\n        Matrix of second derivatives wrt parameters a, or array of diagonal elements\n        if del_M_del_a is None\n    \"\"\"\n\n    # Compute b = M^(-1) * pz\n    b = M.solve(pz)\n    if b.ndim == 1:\n        b = b.reshape(-1, 1)\n\n    # If del_M_del_a is None, compute only the diagonal of the Hessian\n    if del_M_del_a is None:\n        minv_diag = M.inverse_diagonal(method=diagonal_method, n_samples=n_samples, seed=seed)\n        hess_diag = -0.5 * minv_diag.flatten() * b.flatten()**2\n        return hess_diag\n\n    # Compute b_scaled = b .* del_sigma_del_a\n    b_scaled = b * del_M_del_a\n\n    # Compute M^(-1) * b_scaled\n    minv_b_scaled = M.solve(b_scaled)\n\n    # Compute Hessian: -1/2 * b_scaled^T * M^(-1) * b_scaled\n    hess = -0.5 * (b_scaled.T @ minv_b_scaled)\n\n    return hess\n</code></pre>"},{"location":"api/heritability/#graphld.heritability.softmax_robust","title":"softmax_robust","text":"<pre><code>softmax_robust(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Numerically stable softmax implementation.</p> Source code in <code>src/graphld/heritability.py</code> <pre><code>def softmax_robust(x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Numerically stable softmax implementation.\"\"\"\n    y = x + np.log1p(np.exp(-x))\n    mask = x &lt; 0\n    y[mask] = np.log1p(np.exp(x[mask]))\n    return y\n</code></pre>"},{"location":"api/heritability/#graphld.heritability.run_graphREML","title":"run_graphREML","text":"<pre><code>run_graphREML(model_options: ModelOptions, method_options: MethodOptions, summary_stats: DataFrame, annotation_data: DataFrame, ldgm_metadata_path: str, populations: Union[str, List[str]] = None, chromosomes: Optional[Union[int, List[int]]] = None)\n</code></pre> <p>Wrapper function for GraphREML.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>Model instance containing parameters and settings</p> required <code>summary_stats</code> <code>DataFrame</code> <p>DataFrame containing GWAS summary statistics</p> required <code>annotation_data</code> <code>DataFrame</code> <p>DataFrame containing variant annotations</p> required <code>ldgm_metadata_path</code> <code>str</code> <p>Path to LDGM metadata file</p> required <code>populations</code> <code>Union[str, List[str]]</code> <p>Optional list of populations to include</p> <code>None</code> <code>chromosomes</code> <code>Optional[Union[int, List[int]]]</code> <p>Optional list of chromosomes to include</p> <code>None</code> <code>num_processes</code> <p>Optional number of processes for parallel computation</p> required <code>max_iterations</code> <p>Maximum number of iterations</p> required <code>convergence_tol</code> <p>Convergence tolerance</p> required <code>match_by_position</code> <p>If True, match variants by position instead of variant ID</p> required <code>run_in_serial</code> <p>If True, run in serial instead of parallel</p> required <code>num_iterations</code> <p>Number of repetitions</p> required <p>Returns:</p> Type Description <p>Dictionary containing:</p> <ul> <li>estimated parameters</li> </ul> <ul> <li>heritability estimates</li> </ul> <ul> <li>standard errors</li> </ul> <ul> <li>convergence diagnostics</li> </ul> Source code in <code>src/graphld/heritability.py</code> <pre><code>def run_graphREML(\n    model_options: ModelOptions,\n    method_options: MethodOptions,\n    summary_stats: pl.DataFrame,\n    annotation_data: pl.DataFrame,\n    ldgm_metadata_path: str,\n    populations: Union[str, List[str]] = None,\n    chromosomes: Optional[Union[int, List[int]]] = None,\n):\n    \"\"\"Wrapper function for GraphREML.\n\n    Args:\n        model: Model instance containing parameters and settings\n        summary_stats: DataFrame containing GWAS summary statistics\n        annotation_data: DataFrame containing variant annotations\n        ldgm_metadata_path: Path to LDGM metadata file\n        populations: Optional list of populations to include\n        chromosomes: Optional list of chromosomes to include\n        num_processes: Optional number of processes for parallel computation\n        max_iterations: Maximum number of iterations\n        convergence_tol: Convergence tolerance\n        match_by_position: If True, match variants by position instead of variant ID\n        run_in_serial: If True, run in serial instead of parallel\n        num_iterations: Number of repetitions\n\n    Returns:\n        Dictionary containing:\n        - estimated parameters\n        - heritability estimates\n        - standard errors\n        - convergence diagnostics\n    \"\"\"\n    if populations is None:\n        raise ValueError(\"Populations must be provided\")\n\n    if model_options.binary_annotations_only:\n        annotation_data, model_options.annotation_columns = _filter_binary_annotations(\n            annotation_data, model_options.annotation_columns, method_options.verbose\n        )\n\n    # Merge summary stats with annotations\n    merge_how = \"right\" if method_options.use_surrogate_markers else \"inner\"\n    if not method_options.match_by_position:\n        join_cols = [\"SNP\"]\n        merged_data = summary_stats.join(annotation_data, on=join_cols, how=merge_how)\n\n        # Handle column renaming - ensure we have consistent CHR and POS columns\n        if \"CHR_right\" in merged_data.columns and \"POS_right\" in merged_data.columns:\n            # Drop existing CHR and POS columns if they exist to avoid duplicates\n            merged_data = (\n                merged_data.drop(\"CHR\")\n                .drop(\"POS\")\n                .rename({\"CHR_right\": \"CHR\", \"POS_right\": \"POS\"})\n            )\n\n    else:\n        join_cols = [\"CHR\", \"POS\"]\n        merged_data = summary_stats.join(annotation_data, on=join_cols, how=merge_how)\n\n    # Deduplicate chr/pos pairs with multiple entries in the summary statistics\n    merged_data = merged_data.unique(subset=join_cols, keep=\"first\")\n\n    if merged_data.is_empty():\n        raise ValueError(\n            \"No overlapping variants found between summary statistics and annotations.\"\n        )\n\n    # Print shape of each DataFrame\n    if method_options.verbose:\n        print(f\"Summary stats shape: {summary_stats.shape}\")\n        print(f\"Annotation data shape: {annotation_data.shape}\")\n        print(f\"Merged data shape: {merged_data.shape}\")\n        mean_chisq = (merged_data[\"Z\"] ** 2).mean()\n        print(f\"Mean chisq: {mean_chisq}\")\n        max_chisq = (merged_data[\"Z\"] ** 2).max()\n        print(f\"Max chisq: {max_chisq}\")\n\n    # If sample size not provided, try to get it from sumstats\n    if model_options.sample_size is None:\n        if \"N\" in merged_data.columns:\n            mean_n = merged_data[\"N\"].mean()\n            if mean_n is not None:\n                model_options.sample_size = float(mean_n)\n                if method_options.verbose:\n                    print(\n                        f\"Using sample size N={model_options.sample_size} from sumstats\"\n                    )\n            else:\n                model_options.sample_size = 1\n\n    run_fn = GraphREML.run_serial if method_options.run_serial else GraphREML.run\n    return run_fn(\n        ldgm_metadata_path,\n        populations=populations,\n        chromosomes=chromosomes,\n        sumstats=merged_data,\n        num_processes=method_options.num_processes,\n        worker_params=(model_options, method_options),\n        num_params=len(model_options.annotation_columns),\n        model=model_options,\n        method=method_options,\n        num_iterations=method_options.num_iterations,\n        verbose=method_options.verbose,\n        convergence_tol=method_options.convergence_tol,\n        sample_size=model_options.sample_size,\n    )\n</code></pre>"},{"location":"api/io/","title":"graphld.io","text":"<p>Core I/O functions for loading LDGMs and working with variant data.</p>"},{"location":"api/io/#graphld.io","title":"io","text":"<p>Input/output operations for LDGM files.</p>"},{"location":"api/io/#graphld.io.load_ldgm","title":"load_ldgm","text":"<pre><code>load_ldgm(filepath: str, snplist_path: Optional[str] = None, population: Optional[str] = 'EUR', snps_only: bool = False) -&gt; Union['PrecisionOperator', List['PrecisionOperator']]\n</code></pre> <p>Load an LDGM from a single LD block's edgelist and snplist files.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the .edgelist file or directory containing it</p> required <code>snplist_path</code> <code>Optional[str]</code> <p>Optional path to .snplist file or directory. If None, uses filepath</p> <code>None</code> <code>population</code> <code>Optional[str]</code> <p>Optional population name to filter files and set allele frequency column. Defaults to \"EUR\"</p> <code>'EUR'</code> <code>snps_only</code> <code>bool</code> <p>Import snplist data for SNPs only (smaller memory usage)</p> <code>False</code> <p>Returns:</p> Type Description <code>Union['PrecisionOperator', List['PrecisionOperator']]</code> <p>If filepath is a directory: List of PrecisionOperator instances, one for each edgelist file</p> <code>Union['PrecisionOperator', List['PrecisionOperator']]</code> <p>If filepath is a file: Single PrecisionOperator instance with loaded precision matrix and variant info</p> Source code in <code>src/graphld/io.py</code> <pre><code>def load_ldgm(filepath: str, snplist_path: Optional[str] = None, population: Optional[str] = \"EUR\",\n              snps_only: bool = False) -&gt; Union[\"PrecisionOperator\", List[\"PrecisionOperator\"]]:\n    \"\"\"\n    Load an LDGM from a single LD block's edgelist and snplist files.\n\n    Args:\n        filepath: Path to the .edgelist file or directory containing it\n        snplist_path: Optional path to .snplist file or directory. If None, uses filepath\n        population: Optional population name to filter files and set allele frequency\n            column. Defaults to \"EUR\"\n        snps_only: Import snplist data for SNPs only (smaller memory usage)\n\n    Returns:\n        If filepath is a directory:\n            List of PrecisionOperator instances, one for each edgelist file\n        If filepath is a file:\n            Single PrecisionOperator instance with loaded precision matrix and variant info\n    \"\"\"\n    from scipy.sparse import csc_matrix\n\n    from .precision import PrecisionOperator\n\n    # Handle directory vs file input\n    filepath = Path(filepath)\n    if filepath.is_dir():\n        pattern = \"*.edgelist\"\n        if population:\n            pattern = f\"*{population}*.edgelist\"\n        edgelist_files = list(filepath.glob(pattern))\n        if not edgelist_files:\n            raise FileNotFoundError(f\"No edgelist files found in {filepath}\")\n\n        # Load each file and return a list of PrecisionOperators\n        operators = []\n        for edgelist_file in edgelist_files:\n            operator = load_ldgm(edgelist_file, snplist_path, population, snps_only)\n            operators.append(operator)\n        return operators\n\n    # Use provided snplist path or find corresponding snplist file\n    if snplist_path is None:\n        snplist_path = filepath.parent\n        pattern = filepath.stem.split('.')[0]  # Remove all extensions\n        if pattern.endswith(f\".{population}\"):\n            pattern = pattern[:-len(f\".{population}\")]\n        snplist_files = list(Path(snplist_path).glob(f\"{pattern}*.snplist\"))\n        if not snplist_files:\n            raise FileNotFoundError(f\"No matching snplist file found for {filepath}\")\n        snplist_file = snplist_files[0]\n    else:\n        snplist_file = Path(snplist_path)\n        if not snplist_file.exists():\n            raise FileNotFoundError(f\"Snplist file not found: {snplist_file}\")\n\n    # Load edgelist data\n    edgelist = pl.read_csv(filepath, separator=',', has_header=False,\n                          new_columns=['i', 'j', 'value'])\n\n    # Create sparse matrix\n    matrix = csc_matrix(\n        (edgelist['value'].to_numpy(),\n         (edgelist['i'].to_numpy(), edgelist['j'].to_numpy()))\n    )\n\n    # Make matrix symmetric\n    matrix_t = matrix.T\n    diag_vals = matrix.diagonal().copy()\n    matrix = matrix + matrix_t\n    matrix.setdiag(diag_vals, k=0)\n\n    # Verify diagonal values\n    assert np.allclose(matrix.diagonal(), diag_vals), \"Diagonal values not set correctly\"\n\n    # Create mask for rows/cols with nonzeros on diagonal\n    diag = matrix.diagonal()\n    nonzero_where = np.where(diag != 0)[0]\n    n_nonzero = len(nonzero_where)\n\n    # Load variant info\n    variant_info = pl.read_csv(snplist_file, separator=',')\n    num_rows = variant_info['index'].max() + 1\n\n    # Create mapping from old indices to new indices\n    rows = np.full(num_rows, -1)\n    rows[nonzero_where] = np.arange(n_nonzero)\n\n    # If population is specified and exists as a column, rename it to 'af'\n    if population and population in variant_info.columns:\n        variant_info = variant_info.rename({population: 'af'})\n    elif 'af' not in variant_info.columns:\n        available_cols = \", \".join(variant_info.columns)\n        raise ValueError(\n            f\"Neither 'af' column nor '{population}' column found in snplist. \"\n            f\"Available columns: {available_cols}\"\n        )\n\n    # Store original indices and update with new mapping\n    variant_info = variant_info.with_columns([\n        pl.col('index').alias('original_index'),\n        pl.col('index').map_elements(lambda x: rows[x], return_dtype=pl.Int64).alias('index')\n    ])\n\n    # Filter out variants with no corresponding matrix row\n    variant_info = variant_info.filter(pl.col('index') &gt;= 0)\n\n    # Subset matrix to rows/cols with nonzero diagonal\n    matrix = matrix[nonzero_where][:, nonzero_where]\n\n    return PrecisionOperator(matrix, variant_info)\n</code></pre>"},{"location":"api/io/#graphld.io.merge_alleles","title":"merge_alleles","text":"<pre><code>merge_alleles(anc_alleles: Series, deriv_alleles: Series, ref_alleles: Series, alt_alleles: Series) -&gt; pl.Series\n</code></pre> <p>Compare alleles between two sources and return phase information.</p> <p>Parameters:</p> Name Type Description Default <code>anc_alleles</code> <code>Series</code> <p>Ancestral alleles from PrecisionOperator</p> required <code>deriv_alleles</code> <code>Series</code> <p>Derived alleles from PrecisionOperator</p> required <code>ref_alleles</code> <code>Series</code> <p>Reference alleles from summary statistics</p> required <code>alt_alleles</code> <code>Series</code> <p>Alternative alleles from summary statistics</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Series of integers indicating phase: 1: Alleles match exactly</p> <code>Series</code> <p>-1: Alleles match but are swapped 0: Alleles do not match</p> Source code in <code>src/graphld/io.py</code> <pre><code>def merge_alleles(anc_alleles: pl.Series, deriv_alleles: pl.Series,\n                  ref_alleles: pl.Series, alt_alleles: pl.Series) -&gt; pl.Series:\n    \"\"\"Compare alleles between two sources and return phase information.\n\n    Args:\n        anc_alleles: Ancestral alleles from PrecisionOperator\n        deriv_alleles: Derived alleles from PrecisionOperator\n        ref_alleles: Reference alleles from summary statistics\n        alt_alleles: Alternative alleles from summary statistics\n\n    Returns:\n        Series of integers indicating phase:\n         1: Alleles match exactly\n        -1: Alleles match but are swapped\n         0: Alleles do not match\n    \"\"\"\n    # Convert to numpy arrays for faster comparison\n    anc = anc_alleles.to_numpy()\n    der = deriv_alleles.to_numpy()\n    ref = ref_alleles.to_numpy()\n    alt = alt_alleles.to_numpy()\n\n    # Make case-insensitive\n    anc = np.char.lower(anc.astype(str))\n    der = np.char.lower(der.astype(str))\n    ref = np.char.lower(ref.astype(str))\n    alt = np.char.lower(alt.astype(str))\n\n    # Check matches\n    exact_match = (anc == ref) &amp; (der == alt)\n    flipped_match = (anc == alt) &amp; (der == ref)\n\n    # Null alleles are given NaN phase so that corresponding Z scores will be NaN\n    null_match = (ref_alleles.is_null().to_numpy() &amp; alt_alleles.is_null().to_numpy())\n\n    # Convert to phase\n    phase = np.zeros(len(anc), dtype=np.float32)\n    phase[exact_match] = 1\n    phase[flipped_match] = -1\n    phase[null_match] = np.nan\n\n    return pl.Series(phase)\n</code></pre>"},{"location":"api/io/#graphld.io.merge_snplists","title":"merge_snplists","text":"<pre><code>merge_snplists(precision_op: 'PrecisionOperator', sumstats: DataFrame, *, variant_id_col: str = 'SNP', ref_allele_col: str = 'REF', alt_allele_col: str = 'ALT', match_by_position: bool = False, pos_col: str = 'POS', table_format: str = '', add_cols: list[str] = None, add_allelic_cols: list[str] = None, representatives_only: bool = False, modify_in_place: bool = False) -&gt; Tuple['PrecisionOperator', np.ndarray]\n</code></pre> <p>Merge a PrecisionOperator instance with summary statistics DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>precision_op</code> <code>'PrecisionOperator'</code> <p>PrecisionOperator instance</p> required <code>sumstats</code> <code>DataFrame</code> <p>Summary statistics DataFrame</p> required <code>variant_id_col</code> <code>str</code> <p>Column name containing variant IDs</p> <code>'SNP'</code> <code>ref_allele_col</code> <code>str</code> <p>Column name containing reference allele</p> <code>'REF'</code> <code>alt_allele_col</code> <code>str</code> <p>Column name containing alternative allele</p> <code>'ALT'</code> <code>match_by_position</code> <code>bool</code> <p>Whether to match SNPs by position instead of ID</p> <code>False</code> <code>pos_col</code> <code>str</code> <p>Column name containing position</p> <code>'POS'</code> <code>table_format</code> <code>str</code> <p>Optional file format specification (e.g., 'vcf')</p> <code>''</code> <code>add_cols</code> <code>list[str]</code> <p>Optional list of column names from sumstats to append to variant_info</p> <code>None</code> <code>add_allelic_cols</code> <code>list[str]</code> <p>Optional list of column names from sumstats to append to variant_info, multiplied by the phase (-1 or 1) to align with ancestral/derived alleles. If no alleles are provided, these are added without sign-flipping.</p> <code>None</code> <code>modify_in_place</code> <code>bool</code> <p>Whether to modify the PrecisionOperator in place</p> <code>False</code> <p>Returns:</p> Type Description <code>'PrecisionOperator'</code> <p>Tuple containing:</p> <code>ndarray</code> <ul> <li>Modified PrecisionOperator with merged variant info and appended columns</li> </ul> <code>Tuple['PrecisionOperator', ndarray]</code> <ul> <li>Array of indices into sumstats DataFrame indicating which rows were successfully merged</li> </ul> Source code in <code>src/graphld/io.py</code> <pre><code>def merge_snplists(precision_op: \"PrecisionOperator\",\n                   sumstats: pl.DataFrame, *,\n                   variant_id_col: str = 'SNP',\n                   ref_allele_col: str = 'REF',\n                   alt_allele_col: str = 'ALT',\n                   match_by_position: bool = False,\n                   pos_col: str = 'POS',\n                   table_format: str = '',\n                   add_cols: list[str] = None,\n                   add_allelic_cols: list[str] = None,\n                   representatives_only: bool = False,\n                   modify_in_place: bool = False) -&gt; Tuple[\"PrecisionOperator\", np.ndarray]:\n    \"\"\"Merge a PrecisionOperator instance with summary statistics DataFrame.\n\n    Args:\n        precision_op: PrecisionOperator instance\n        sumstats: Summary statistics DataFrame\n        variant_id_col: Column name containing variant IDs\n        ref_allele_col: Column name containing reference allele\n        alt_allele_col: Column name containing alternative allele\n        match_by_position: Whether to match SNPs by position instead of ID\n        pos_col: Column name containing position\n        table_format: Optional file format specification (e.g., 'vcf')\n        add_cols: Optional list of column names from sumstats to append to variant_info\n        add_allelic_cols: Optional list of column names from sumstats to append to variant_info,\n            multiplied by the phase (-1 or 1) to align with ancestral/derived alleles.\n            If no alleles are provided, these are added without sign-flipping.\n        modify_in_place: Whether to modify the PrecisionOperator in place\n\n    Returns:\n        Tuple containing:\n        - Modified PrecisionOperator with merged variant info and appended columns\n        - Array of indices into sumstats DataFrame indicating which rows were successfully merged\n    \"\"\"\n    # Handle VCF format\n    if table_format.lower() == 'vcf':\n        match_by_position = True\n        pos_col = 'POS'\n        ref_allele_col = 'REF'\n        alt_allele_col = 'ALT'\n    elif table_format.lower() == 'ldsc':\n        match_by_position = False\n        ref_allele_col = 'A2'\n        alt_allele_col = 'A1'\n\n    # Find position column\n    pos_options = ['position', 'POS', 'BP']\n    if pos_col is not None:\n        pos_options.insert(0, pos_col)\n    pos_col = next((col for col in pos_options if col in sumstats.columns), None)\n    if pos_col is None:\n        raise ValueError(\n            f\"Could not find position column. Tried: {', '.join(pos_options)}\"\n        )\n\n    # Validate inputs\n    if match_by_position:\n        if pos_col not in sumstats.columns:\n            msg = (f\"Summary statistics must contain {pos_col} column \"\n                  f\"for position matching. Found columns: {', '.join(sumstats.columns)}\")\n            raise ValueError(msg)\n    else:\n        if variant_id_col not in sumstats.columns:\n            msg = (f\"Summary statistics must contain {variant_id_col} column. \"\n                  f\"Found columns: {', '.join(sumstats.columns)}\")\n            raise ValueError(msg)\n\n    # Match variants\n    match_by = ('position', pos_col) if match_by_position else ('site_ids', variant_id_col)\n    merged = precision_op.variant_info.join(\n        sumstats.with_row_index(name=\"row_nr\"),\n        left_on=[match_by[0]],\n        right_on=[match_by[1]],\n        suffix=\"_sumstats\",\n        how='inner'\n    )\n\n    # Check alleles if provided\n    phase = 1\n    if all(col in sumstats.columns for col in [ref_allele_col, alt_allele_col]):\n        phase = merge_alleles(\n            merged['anc_alleles'],\n            merged['deriv_alleles'],\n            merged[ref_allele_col],\n            merged[alt_allele_col]\n        ).alias('phase')\n        merged = merged.with_columns(phase)\n\n        # Update indices to only include variants with matching alleles\n        merged = merged.filter(pl.col('phase') != 0)\n        phase = merged['phase'].to_numpy()\n\n    add_cols = add_cols or []\n    add_allelic_cols = add_allelic_cols or []\n    new_cols = {}\n\n    # Check all columns exist\n    missing_cols = [col for col in add_cols + add_allelic_cols if col not in sumstats.columns]\n    if missing_cols:\n        msg = (f\"Requested columns not found in sumstats: {', '.join(missing_cols)}. \"\n              f\"Available columns: {', '.join(sumstats.columns)}\")\n        raise ValueError(msg)\n\n    # Add columns with appropriate transformations\n    for col in add_cols:\n        new_cols[col] = pl.col(col)\n\n    for col in add_allelic_cols:\n        new_cols[col] = pl.col(col) * phase\n\n    # Add all new columns at once if any\n    if new_cols:\n        merged = merged.with_columns(**new_cols)\n\n    # Sort by index and add is_representative column\n    merged = (\n        merged\n        .sort('index')\n        .with_columns(\n            pl.col('index').is_first_distinct().cast(pl.Int8).alias('is_representative')\n        )\n    )\n\n    # Create new PrecisionOperator with merged variant info\n    unique_indices = np.unique(merged['index'].to_numpy())\n    if modify_in_place:\n        precision_op.set_which_indices(unique_indices)\n    else:\n        precision_op = precision_op[unique_indices]\n\n    # Create mapping from old indices to new contiguous ones\n    index_map = {old_idx: new_idx for new_idx, old_idx in enumerate(unique_indices)}\n\n    # Update indices in merged data to be contiguous using efficient replace_strict\n    merged = merged.with_columns(\n        pl.col('index').replace_strict(index_map).alias('index')\n    )\n\n    if representatives_only:\n        merged = merged.filter(pl.col('is_representative') == 1)\n\n    precision_op.variant_info = merged\n    sumstat_indices = merged.select('row_nr').to_numpy().flatten().astype(int)\n\n    return precision_op, sumstat_indices\n</code></pre>"},{"location":"api/io/#graphld.io.partition_variants","title":"partition_variants","text":"<pre><code>partition_variants(ldgm_metadata: DataFrame, variant_data: DataFrame, *, chrom_col: Optional[str] = None, pos_col: Optional[str] = None) -&gt; List[pl.DataFrame]\n</code></pre> <p>Partition variant data according to LDGM blocks.</p> <p>Parameters:</p> Name Type Description Default <code>ldgm_metadata</code> <code>DataFrame</code> <p>DataFrame from read_ldgm_metadata containing block info</p> required <code>variant_data</code> <code>DataFrame</code> <p>DataFrame containing variant information</p> required <code>chrom_col</code> <code>Optional[str]</code> <p>Optional name of chromosome column. If None, tries common names</p> <code>None</code> <code>pos_col</code> <code>Optional[str]</code> <p>Optional name of position column. If None, tries common names</p> <code>None</code> <p>Returns:</p> Type Description <code>List[DataFrame]</code> <p>List of DataFrames, one per row in ldgm_metadata, containing variants</p> <code>List[DataFrame]</code> <p>that fall within each block's coordinates</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the file does not have the expected columns: variant_id, chromosome, position, ...</p> Source code in <code>src/graphld/io.py</code> <pre><code>def partition_variants(\n    ldgm_metadata: pl.DataFrame,\n    variant_data: pl.DataFrame,\n    *,\n    chrom_col: Optional[str] = None,\n    pos_col: Optional[str] = None\n) -&gt; List[pl.DataFrame]:\n    \"\"\"Partition variant data according to LDGM blocks.\n\n    Args:\n        ldgm_metadata: DataFrame from read_ldgm_metadata containing block info\n        variant_data: DataFrame containing variant information\n        chrom_col: Optional name of chromosome column. If None, tries common names\n        pos_col: Optional name of position column. If None, tries common names\n\n    Returns:\n        List of DataFrames, one per row in ldgm_metadata, containing variants\n        that fall within each block's coordinates\n\n    Raises:\n        ValueError: If the file does not have the expected columns: variant_id,\n            chromosome, position, ...\n    \"\"\"\n    # Find chromosome column\n    chrom_options = ['chrom', 'chromosome', 'CHR']\n    if chrom_col is not None:\n        chrom_options.insert(0, chrom_col)\n    chrom_col = next((col for col in chrom_options if col in variant_data.columns), None)\n    if chrom_col is None:\n        raise ValueError(\n            f\"Could not find chromosome column. Tried: {', '.join(chrom_options)}\"\n        )\n\n    # Find position column\n    pos_options = ['position', 'POS', 'BP']\n    if pos_col is not None:\n        pos_options.insert(0, pos_col)\n    pos_col = next((col for col in pos_options if col in variant_data.columns), None)\n    if pos_col is None:\n        raise ValueError(\n            f\"Could not find position column. Tried: {', '.join(pos_options)}\"\n        )\n\n    # Convert chromosome column to integer if needed\n    if variant_data[chrom_col].dtype != pl.Int64:\n        variant_data = variant_data.with_columns(\n            pl.col(chrom_col).cast(pl.Int64).alias(chrom_col)\n        )\n\n    # First sort variants by chromosome and position\n    sorted_variants = variant_data.sort([chrom_col, pos_col])\n\n    # Group blocks by chromosome\n    chrom_blocks = {}\n    for block in ldgm_metadata.iter_rows(named=True):\n        chrom = block['chrom']\n        if chrom not in chrom_blocks:\n            chrom_blocks[chrom] = []\n        chrom_blocks[chrom].append(block)\n\n    # Process each chromosome's blocks at once\n    partitioned = []\n    for chrom, blocks in chrom_blocks.items():\n        # Get all variants for this chromosome\n        chrom_variants = sorted_variants.filter(pl.col(chrom_col) == chrom)\n        if len(chrom_variants) == 0:\n            partitioned.extend([pl.DataFrame()] * len(blocks))\n            continue\n\n        # Get positions array for binary search\n        positions = chrom_variants.get_column(pos_col).to_numpy()\n\n        # Process each block\n        for block in blocks:\n            # Binary search for block boundaries\n            start_idx = np.searchsorted(positions, block['chromStart'])\n            end_idx = np.searchsorted(positions, block['chromEnd'])\n\n            # Extract variants for this block\n            block_variants = chrom_variants.slice(start_idx, end_idx - start_idx)\n            partitioned.append(block_variants)\n\n\n    return partitioned\n</code></pre>"},{"location":"api/io/#graphld.io.create_ldgm_metadata","title":"create_ldgm_metadata","text":"<pre><code>create_ldgm_metadata(directory: Union[str, Path], output_file: Optional[str] = None) -&gt; pl.DataFrame\n</code></pre> <p>Create metadata file for LDGM files in a directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Union[str, Path]</code> <p>Directory containing .snplist and .edgelist files</p> required <code>output_file</code> <code>Optional[str]</code> <p>Optional path to write CSV file. If None, only returns DataFrame</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Polars DataFrame containing metadata for each LDGM file</p> Source code in <code>src/graphld/io.py</code> <pre><code>def create_ldgm_metadata(\n    directory: Union[str, Path], output_file: Optional[str] = None\n) -&gt; pl.DataFrame:\n    \"\"\"Create metadata file for LDGM files in a directory.\n\n    Args:\n        directory: Directory containing .snplist and .edgelist files\n        output_file: Optional path to write CSV file. If None, only returns DataFrame\n\n    Returns:\n        Polars DataFrame containing metadata for each LDGM file\n    \"\"\"\n    directory = Path(directory)\n    if not directory.exists():\n        raise FileNotFoundError(f\"Directory not found: {directory}\")\n\n    # Find all edgelist files\n    edgelist_files = list(directory.glob(\"*.edgelist\"))\n    if not edgelist_files:\n        raise FileNotFoundError(f\"No .edgelist files found in {directory}\")\n\n    # Process each file\n    data = []\n    for edgefile in edgelist_files:\n        # Parse filename to get info\n        name = edgefile.name\n        parts = name.split('_')  # e.g. 1kg_chr1_2888443_4320284.EUR.edgelist\n        if len(parts) &lt; 4:\n            print(f\"Skipping {name}: unexpected filename format\")\n            continue\n\n        # Get chromosome and positions\n        try:\n            chrom = int(parts[1].replace('chr', ''))\n            chromStart = int(parts[2])\n            chromEnd = int(parts[3].split('.')[0])  # Remove population/extension\n        except (ValueError, IndexError):\n            print(f\"Skipping {name}: could not parse chromosome/position\")\n            continue\n\n        # Get population\n        try:\n            population = name.split('.')[-2]  # Second to last part\n        except IndexError:\n            print(f\"Skipping {name}: could not parse population\")\n            continue\n\n        # Find corresponding snplist file\n        base_name = name.split('.')[0]  # Remove population and extension\n        snplist_files = list(directory.glob(f\"{base_name}.snplist\"))\n        if not snplist_files:\n            print(f\"Skipping {name}: no matching .snplist file\")\n            continue\n        snplist_name = snplist_files[0].name\n\n        # Count variants in snplist\n        try:\n            snplist_df = pl.read_csv(snplist_files[0])\n            num_variants = len(snplist_df)\n        except Exception as e:\n            print(f\"Skipping {name}: error reading snplist: {e}\")\n            continue\n\n        # Count entries in edgelist\n        try:\n            edgelist_df = pl.read_csv(edgefile, has_header=False,\n                                    new_columns=['i', 'j', 'value'])\n\n            # Count unique diagonal indices\n            diag_mask = edgelist_df['i'] == edgelist_df['j']\n            num_indices = len(edgelist_df.filter(diag_mask)['i'].unique())\n\n            # Total number of entries\n            num_entries = len(edgelist_df)\n\n        except Exception as e:\n            print(f\"Skipping {name}: error reading edgelist: {e}\")\n            continue\n\n        # Add row to metadata\n        data.append({\n            'chrom': chrom,\n            'chromStart': chromStart,\n            'chromEnd': chromEnd,\n            'name': name,\n            'snplistName': snplist_name,\n            'population': population,\n            'numVariants': num_variants,\n            'numIndices': num_indices,\n            'numEntries': num_entries,\n            'info': ''\n        })\n\n    # Create DataFrame\n    if not data:\n        raise ValueError(\"No valid LDGM files found\")\n\n    df = pl.DataFrame(data)\n\n    # Sort by chromosome and start position\n    df = df.sort(['chrom', 'chromStart'])\n\n    # Write to file if requested\n    if output_file:\n        df.write_csv(output_file)\n\n    return df\n</code></pre>"},{"location":"api/io/#graphld.io.read_ldgm_metadata","title":"read_ldgm_metadata","text":"<pre><code>read_ldgm_metadata(filepath: Union[str, Path], *, populations: Optional[Union[str, List[str]]] = None, chromosomes: Optional[Union[int, List[int]]] = None, max_blocks: Optional[int] = None) -&gt; pl.DataFrame\n</code></pre> <p>Read LDGM metadata from CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>Union[str, Path]</code> <p>Path to metadata CSV file</p> required <code>populations</code> <code>Optional[Union[str, List[str]]]</code> <p>Optional population(s) to filter by</p> <code>None</code> <code>chromosomes</code> <code>Optional[Union[int, List[int]]]</code> <p>Optional chromosome(s) to filter by</p> <code>None</code> <code>max_blocks</code> <code>Optional[int]</code> <p>Optional maximum number of blocks to return</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Polars DataFrame containing LDGM metadata, filtered by population and chromosome</p> <code>DataFrame</code> <p>if specified, and limited to max_blocks if specified</p> Source code in <code>src/graphld/io.py</code> <pre><code>def read_ldgm_metadata(\n    filepath: Union[str, Path],\n    *,\n    populations: Optional[Union[str, List[str]]] = None,\n    chromosomes: Optional[Union[int, List[int]]] = None,\n    max_blocks: Optional[int] = None\n) -&gt; pl.DataFrame:\n    \"\"\"Read LDGM metadata from CSV file.\n\n    Args:\n        filepath: Path to metadata CSV file\n        populations: Optional population(s) to filter by\n        chromosomes: Optional chromosome(s) to filter by\n        max_blocks: Optional maximum number of blocks to return\n\n    Returns:\n        Polars DataFrame containing LDGM metadata, filtered by population and chromosome\n        if specified, and limited to max_blocks if specified\n    \"\"\"\n    try:\n        df = pl.read_csv(filepath)\n        required_cols = [\n            'chrom', 'chromStart', 'chromEnd', 'name', 'snplistName',\n            'population', 'numVariants', 'numIndices', 'numEntries', 'info'\n        ]\n        missing = [col for col in required_cols if col not in df.columns]\n        if missing:\n            raise ValueError(f\"Missing required columns: {', '.join(missing)}\")\n\n        # Filter by population if specified\n        if populations is not None:\n            if isinstance(populations, str):\n                populations = [populations]\n            df = df.filter(pl.col('population').is_in(populations))\n            if len(df) == 0:\n                raise ValueError(f\"No blocks found for populations: {populations}\")\n\n        # Filter by chromosome if specified\n        if chromosomes is not None:\n            if isinstance(chromosomes, int):\n                chromosomes = [chromosomes]\n            df = df.filter(pl.col('chrom').is_in(chromosomes))\n            if len(df) == 0:\n                raise ValueError(f\"No blocks found for chromosomes: {chromosomes}\")\n\n        # Sort by chromosome and position\n        df = df.sort(['chrom', 'chromStart'])\n\n        # Limit number of blocks if specified\n        if max_blocks is not None:\n            df = df.head(max_blocks)\n\n        return df\n\n    except Exception as e:\n        raise ValueError(f\"Error reading metadata file: {e}\") from e\n</code></pre>"},{"location":"api/io/#graphld.io.load_annotations","title":"load_annotations","text":"<pre><code>load_annotations(annot_path: str, chromosome: Optional[int] = None, infer_schema_length: int = 100000, add_alleles: bool = False, add_positions: bool = True, positions_file: str = POSITIONS_FILE, file_pattern: str = 'baselineLD.{chrom}.annot', exclude_bed: bool = False) -&gt; pl.DataFrame\n</code></pre> <p>Load annotation data for specified chromosome(s) and merge with LDGMs data.</p> <p>Parameters:</p> Name Type Description Default <code>annot_path</code> <code>str</code> <p>Path to directory containing annotation files</p> required <code>chromosome</code> <code>Optional[int]</code> <p>Specific chromosome number, or None for all chromosomes</p> <code>None</code> <code>infer_schema_length</code> <code>int</code> <p>Number of rows to infer schema from. Runs faster if this is smaller</p> <code>100000</code> <code>file_pattern</code> <code>str</code> <p>Filename pattern to match, with {chrom} as a placeholder for chromosome number</p> <code>'baselineLD.{chrom}.annot'</code> <code>exclude_bed</code> <code>bool</code> <p>If True, skip loading .bed files from the annotations directory</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing annotations</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no matching annotation files are found</p> Source code in <code>src/graphld/io.py</code> <pre><code>def load_annotations(annot_path: str,\n                    chromosome: Optional[int] = None,\n                    infer_schema_length: int = 100_000,\n                    add_alleles: bool = False,\n                    add_positions: bool = True,\n                    positions_file: str = POSITIONS_FILE,\n                    file_pattern: str = \"baselineLD.{chrom}.annot\",\n                    exclude_bed: bool = False\n                    ) -&gt; pl.DataFrame:\n    \"\"\"Load annotation data for specified chromosome(s) and merge with LDGMs data.\n\n    Args:\n        annot_path: Path to directory containing annotation files\n        chromosome: Specific chromosome number, or None for all chromosomes\n        infer_schema_length: Number of rows to infer schema from. Runs faster if this is smaller\n        but will throw an error if too small because floating-point columns will be\n        cast as integers.\n        file_pattern: Filename pattern to match, with {chrom} as a placeholder for chromosome number\n        exclude_bed: If True, skip loading .bed files from the annotations directory\n\n    Returns:\n        DataFrame containing annotations\n\n    Raises:\n        ValueError: If no matching annotation files are found\n    \"\"\"\n    import glob\n    import os\n\n    # Determine which chromosomes to process\n    if chromosome is not None:\n        chromosomes = [chromosome]\n    else:\n        chromosomes = range(1, 23)  # Assuming chromosomes 1-22\n\n    # Find matching files\n    annotations = []\n    for chromosome in chromosomes:\n        file_pattern = f\"*.{chromosome}.annot\"\n        matching_files = Path(annot_path).glob(file_pattern)\n\n        # Read all matching files for this chromosome\n        dfs = []\n        seen_cols = set()\n        for file_path in matching_files:\n            df = pl.scan_csv(file_path, separator='\\t', infer_schema_length=infer_schema_length)\n            # Drop columns that were already seen in previous files to avoid duplicates\n            schema_names = df.collect_schema().names()\n            cols_to_keep = [col for col in schema_names if col not in seen_cols]\n            if cols_to_keep:\n                df = df.select(cols_to_keep)\n                seen_cols.update(cols_to_keep)\n                dfs.append(df)\n\n        # Horizontally concatenate all dataframes for this chromosome\n        if dfs:\n            combined_df = pl.concat(dfs, how=\"horizontal\")\n            annotations.append(combined_df)\n\n    # Check if any files were found\n    if not annotations:\n        raise ValueError(\n            f\"No annotation files found in {annot_path} matching pattern {file_pattern}\"\n        )\n\n    # Concatenate all chromosome dataframes and handle different schemas\n    annotations = pl.concat(annotations, how=\"diagonal_relaxed\").collect()\n\n    # Convert binary columns to boolean to save memory\n    numeric_types = (pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.Float32, pl.Float64)\n    binary_cols = []\n    for col in annotations.columns:\n        # Skip non-numeric columns\n        if not isinstance(annotations[col].dtype, numeric_types):\n            continue\n        # Check if column only contains 0, 1 and null values\n        unique_vals = set(annotations[col].unique().drop_nulls())\n        if unique_vals == {0, 1}:\n            binary_cols.append(col)\n\n    # Convert binary columns to boolean\n    if binary_cols:\n        bool_exprs = [pl.col(col).cast(pl.Boolean) for col in binary_cols]\n        annotations = annotations.with_columns(bool_exprs)\n\n    if add_positions or add_alleles:\n        snplist_data = pl.read_csv(\n            positions_file,\n            separator=',',\n            columns=['chrom', 'site_ids', 'position', 'anc_alleles', 'deriv_alleles']\n        )\n\n        snplist_data = snplist_data.rename({\n                'chrom': 'CHR',\n                'site_ids': 'SNP',\n                'position': 'POS',\n                'anc_alleles': 'A2',\n                'deriv_alleles': 'A1'\n            })\n\n        with_columns = ['SNP']\n        if add_positions:\n            with_columns += ['CHR', 'POS']\n        if add_alleles:\n            with_columns += ['A2', 'A1']\n\n        snplist_data = snplist_data.select(with_columns)\n\n        # Existing coordinates might be in wrong genome build\n        annotations = annotations.drop(['CHR', 'BP'])\n\n        # Merge with positions\n        annotations = annotations.join(\n            snplist_data,\n            on='SNP',\n            how='inner'\n        )\n\n    if not add_positions:\n        annotations = annotations.rename({'BP': 'POS'})\n\n    bed_files = glob.glob(os.path.join(annot_path, \"*.bed\"))\n    if exclude_bed or not bed_files:\n        return annotations\n\n    # Process BED files if they exist\n\n    # Create a dictionary to store new annotation columns\n    bed_annotations = {}\n    for bed_file in bed_files:\n        # Get the name for this annotation from the filename\n        bed_name = os.path.splitext(os.path.basename(bed_file))[0]\n\n        # Read the BED file\n        bed_df = read_bed(bed_file)\n\n        # Convert chromosome names to match our format (e.g., \"chr1\" -&gt; 1)\n        bed_df = bed_df.with_columns([\n            pl.col(\"chrom\").str.replace(\"chr\", \"\").cast(pl.Int64).alias(\"chrom\")\n        ])\n\n        new_annot = np.zeros(len(annotations), dtype=bool)\n\n        # Group BED regions by chromosome\n        unique_chromosomes = annotations.get_column(\"CHR\").unique()\n        for chrom in unique_chromosomes:\n            chrom_indices = (annotations[\"CHR\"] == chrom).to_numpy()\n            if not chrom_indices.any():\n                continue\n            bed_regions = bed_df.filter(bed_df[\"chrom\"] == chrom).select(\"chromStart\", \"chromEnd\").to_numpy()\n            positions = annotations.filter(annotations[\"CHR\"] == chrom)[\"POS\"].to_numpy()\n            new_annot[chrom_indices] = _get_range_mask(\n                values=positions,\n                start=bed_regions[:, 0],\n                end=bed_regions[:, 1]\n                )\n\n        bed_annotations[bed_name] = new_annot\n\n    # Add all BED annotations to the main DataFrame\n    annotations = annotations.with_columns(**bed_annotations)\n\n    return annotations\n</code></pre>"},{"location":"api/io/#graphld.io.read_bed","title":"read_bed","text":"<pre><code>read_bed(bed_file: str, min_fields: int = 3, max_fields: int = 12, zero_based: bool = True) -&gt; pl.DataFrame\n</code></pre> <p>Read a UCSC BED format file.</p> <p>The BED format has 3 required fields and 9 optional fields: Required:     1. chrom - Chromosome name     2. chromStart - Start position (0-based)     3. chromEnd - End position (not included in feature) Optional:     4. name - Name of BED line     5. score - Score from 0-1000     6. strand - Strand: \"+\" or \"-\" or \".\"     7. thickStart - Starting position at which feature is drawn thickly     8. thickEnd - Ending position at which feature is drawn thickly     9. itemRgb - RGB value (e.g., \"255,0,0\")     10. blockCount - Number of blocks (e.g., exons)     11. blockSizes - Comma-separated list of block sizes     12. blockStarts - Comma-separated list of block starts relative to chromStart</p> <p>Parameters:</p> Name Type Description Default <code>bed_file</code> <code>str</code> <p>Path to BED format file</p> required <code>min_fields</code> <code>int</code> <p>Minimum number of fields required (default: 3)</p> <code>3</code> <code>max_fields</code> <code>int</code> <p>Maximum number of fields to read (default: 12)</p> <code>12</code> <code>zero_based</code> <code>bool</code> <p>If True (default), keeps positions 0-based. If False, adds 1 to start positions.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Polars DataFrame containing the BED data with appropriate column names and types.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If min_fields &lt; 3 or max_fields &gt; 12 or if file has inconsistent number of fields</p> Source code in <code>src/graphld/io.py</code> <pre><code>def read_bed(bed_file: str,\n             min_fields: int = 3,\n             max_fields: int = 12,\n             zero_based: bool = True) -&gt; pl.DataFrame:\n    \"\"\"Read a UCSC BED format file.\n\n    The BED format has 3 required fields and 9 optional fields:\n    Required:\n        1. chrom - Chromosome name\n        2. chromStart - Start position (0-based)\n        3. chromEnd - End position (not included in feature)\n    Optional:\n        4. name - Name of BED line\n        5. score - Score from 0-1000\n        6. strand - Strand: \"+\" or \"-\" or \".\"\n        7. thickStart - Starting position at which feature is drawn thickly\n        8. thickEnd - Ending position at which feature is drawn thickly\n        9. itemRgb - RGB value (e.g., \"255,0,0\")\n        10. blockCount - Number of blocks (e.g., exons)\n        11. blockSizes - Comma-separated list of block sizes\n        12. blockStarts - Comma-separated list of block starts relative to chromStart\n\n    Args:\n        bed_file: Path to BED format file\n        min_fields: Minimum number of fields required (default: 3)\n        max_fields: Maximum number of fields to read (default: 12)\n        zero_based: If True (default), keeps positions 0-based. If False, adds 1 to start positions.\n\n    Returns:\n        Polars DataFrame containing the BED data with appropriate column names and types.\n\n    Raises:\n        ValueError: If min_fields &lt; 3 or max_fields &gt; 12 or if file has\n            inconsistent number of fields\n    \"\"\"\n    if min_fields &lt; 3:\n        raise ValueError(\"BED format requires at least 3 fields\")\n    if max_fields &gt; 12:\n        raise ValueError(\"BED format has at most 12 fields\")\n    if min_fields &gt; max_fields:\n        raise ValueError(\"min_fields cannot be greater than max_fields\")\n\n    # Define all possible BED columns with their types\n    bed_columns = [\n        ('chrom', pl.Utf8),\n        ('chromStart', pl.Int64),\n        ('chromEnd', pl.Int64),\n        ('name', pl.Utf8),\n        ('score', pl.Int64),\n        ('strand', pl.Utf8),\n        ('thickStart', pl.Int64),\n        ('thickEnd', pl.Int64),\n        ('itemRgb', pl.Utf8),\n        ('blockCount', pl.Int64),\n        ('blockSizes', pl.Utf8),\n        ('blockStarts', pl.Utf8)\n    ]\n\n    # Read and parse the file\n    data = []\n    with open(bed_file) as f:\n        for line in f:\n            line = line.strip()\n            if not line or line.startswith(('browser', 'track', '#')):\n                continue\n            # Split on tabs or spaces and filter out empty strings\n            fields = [f for f in line.split() if f]\n            if not (min_fields &lt;= len(fields) &lt;= max_fields):\n                raise ValueError(\n                    f\"BED line has {len(fields)} fields, expected between \"\n                    f\"{min_fields} and {max_fields}: {line}\"\n                )\n            # Pad with None if we have fewer than max_fields\n            fields.extend([None] * (max_fields - len(fields)))\n            data.append(fields[:max_fields])\n\n    # Create schema for required fields\n    schema = {name: dtype for name, dtype in bed_columns[:max_fields]}\n\n    # Create DataFrame\n    df = pl.from_records(\n        data,\n        schema=schema,\n        orient=\"row\"\n    )\n\n    # Convert 0-based to 1-based coordinates if requested\n    if not zero_based:\n        df = df.with_columns([\n            pl.col('chromStart') + 1\n        ])\n        # Also convert thick positions if present\n        if 'thickStart' in df.columns:\n            df = df.with_columns([\n                pl.col('thickStart') + 1\n            ])\n\n    return df\n</code></pre>"},{"location":"api/io/#graphld.io.read_concat_snplists","title":"read_concat_snplists","text":"<pre><code>read_concat_snplists(ldgm_metadata: DataFrame, parent_dir: Path) -&gt; pl.LazyFrame\n</code></pre> <p>Read and concatenate snplists from LDGM metadata.</p> <p>Parameters:</p> Name Type Description Default <code>ldgm_metadata</code> <code>DataFrame</code> <p>DataFrame from read_ldgm_metadata containing block info</p> required <p>Returns:</p> Type Description <code>LazyFrame</code> <p>LazyFrame containing variant information concatenated across blocks.</p> Source code in <code>src/graphld/io.py</code> <pre><code>def read_concat_snplists(ldgm_metadata: pl.DataFrame, parent_dir: Path) -&gt; pl.LazyFrame:\n    \"\"\"Read and concatenate snplists from LDGM metadata.\n\n    Args:\n        ldgm_metadata: DataFrame from read_ldgm_metadata containing block info\n\n    Returns:\n        LazyFrame containing variant information concatenated across blocks.\n    \"\"\"\n    ldgms = [\n        load_ldgm(parent_dir / Path(row[\"name\"])) for row in ldgm_metadata.iter_rows(named=True)\n    ]\n    lazy_frames = [\n        ldgm.variant_info.lazy().with_columns(pl.lit(i).alias('block'))\n        for i, ldgm in enumerate(ldgms)\n    ]\n    return pl.concat(lazy_frames)\n</code></pre>"},{"location":"api/ldsc_io/","title":"graphld.ldsc_io","text":"<p>I/O functions for LDSC format files (summary statistics and annotations).</p>"},{"location":"api/ldsc_io/#graphld.ldsc_io","title":"ldsc_io","text":"<p>Functions for reading LDSC sumstats files.</p>"},{"location":"api/ldsc_io/#graphld.ldsc_io.read_ldsc_snplist","title":"read_ldsc_snplist","text":"<pre><code>read_ldsc_snplist(file: Union[str, Path], add_positions: bool = True, positions_file: str = POSITIONS_FILE) -&gt; pl.DataFrame\n</code></pre> <p>Read LDSC snplist file format.</p> Source code in <code>src/graphld/ldsc_io.py</code> <pre><code>def read_ldsc_snplist(\n    file: Union[str, Path],\n    add_positions: bool = True,\n    positions_file: str = POSITIONS_FILE,\n) -&gt; pl.DataFrame:\n    \"\"\"Read LDSC snplist file format.\"\"\"\n    print(f\"Reading LDSC snplist file: {file}\")\n    df = pl.read_csv(\n        file,\n        separator='\\t',\n        columns=['SNP', 'A1', 'A2'],\n        has_header=True,\n    )\n\n    if add_positions:\n        df = _add_positions_to_df(df, positions_file)\n\n    return df\n</code></pre>"},{"location":"api/ldsc_io/#graphld.ldsc_io.read_ldsc_sumstats","title":"read_ldsc_sumstats","text":"<pre><code>read_ldsc_sumstats(file: Union[str, Path], add_positions: bool = True, positions_file: str = POSITIONS_FILE, maximum_missingness: float = 1.0) -&gt; pl.DataFrame\n</code></pre> <p>Read LDSC sumstats file format.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Union[str, Path]</code> <p>Path to LDSC sumstats file</p> required <code>add_positions</code> <code>bool</code> <p>If True, merge with external file to add positions</p> <code>True</code> <code>positions_file</code> <code>str</code> <p>File containing RSIDs and positions, defaults to data/rsid_position.csv</p> <code>POSITIONS_FILE</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with columns: SNP, N, Z, A1, A2</p> <code>DataFrame</code> <p>If add_positions=True, also includes: CHR, POS</p> Source code in <code>src/graphld/ldsc_io.py</code> <pre><code>def read_ldsc_sumstats(\n    file: Union[str, Path],\n    add_positions: bool = True,\n    positions_file: str = POSITIONS_FILE,\n    maximum_missingness: float = 1.0,\n) -&gt; pl.DataFrame:\n    \"\"\"Read LDSC sumstats file format.\n\n    Args:\n        file: Path to LDSC sumstats file\n        add_positions: If True, merge with external file to add positions\n        positions_file: File containing RSIDs and positions, defaults to data/rsid_position.csv\n\n    Returns:\n        DataFrame with columns: SNP, N, Z, A1, A2\n        If add_positions=True, also includes: CHR, POS\n    \"\"\"\n    print(f\"Reading LDSC sumstats file: {file} with max missingness: {maximum_missingness}\")\n    # Dynamically detect columns\n    with open(file, 'r') as f:\n        header = f.readline().strip().split('\\t')\n\n    # Determine schema overrides based on available columns\n    if 'Z' in header:\n        schema_overrides = {'SNP': pl.Utf8, 'N': pl.Float64, 'Z': pl.Float64, 'A1': pl.Utf8, 'A2': pl.Utf8}\n    elif 'Beta' in header and 'se' in header:\n        schema_overrides = {'SNP': pl.Utf8, 'N': pl.Float64, 'Beta': pl.Float64, 'se': pl.Float64, 'A1': pl.Utf8, 'A2': pl.Utf8}\n    else:\n        raise ValueError(\"Unsupported LDSC sumstats format\")\n\n    # Read the file using polars\n    df = pl.read_csv(\n        file,\n        separator='\\t',\n        schema_overrides=schema_overrides,\n    )\n\n    # Compute Z score if needed\n    if 'Beta' in df.columns and 'Z' not in df.columns:\n        df = df.with_columns(\n            (pl.col('Beta') / pl.col('se')).alias('Z')\n        ).drop(['Beta', 'se'])\n\n    # Validate required columns\n    required_cols = {'SNP', 'N', 'Z', 'A1', 'A2'}\n    missing_cols = required_cols - set(df.columns)\n    if missing_cols:\n        raise ValueError(f\"Missing required columns: {missing_cols}\")\n\n    # Rename A1, A2 to ALT, REF\n    df = df.with_columns(\n        pl.col('A1').alias('ALT'),\n        pl.col('A2').alias('REF')\n    ).drop(['A1', 'A2'])\n\n    if add_positions:\n        df = _add_positions_to_df(df, positions_file)\n\n    # Drop variants with N &lt; (1 - maximum_missingness) * max(N)\n    print(f\"Num variants before filtering by missingness: {len(df)}\")\n    df = df.filter(pl.col('N') &gt;= (1 - maximum_missingness) * pl.col('N').max())\n    print(f\"Num variants after filtering by missingness: {len(df)}\")\n\n    return df\n</code></pre>"},{"location":"api/likelihood/","title":"graphld.likelihood","text":"<p>Gaussian likelihood functions for GWAS summary statistics under an infinitesimal model.</p>"},{"location":"api/likelihood/#model","title":"Model","text":"<p>The likelihood of GWAS summary statistics under an infinitesimal model is:</p> <p>$$\\beta \\sim N(0, D)$$</p> <p>$$z|\\beta \\sim N(n^{1/2}R\\beta, R)$$</p> <p>where:</p> <ul> <li>$\\beta$ is the effect-size vector in s.d-per-s.d. units</li> <li>$D$ is a diagonal matrix of per-variant heritabilities</li> <li>$z$ is the GWAS summary statistic vector</li> <li>$R$ is the LD correlation matrix</li> <li>$n$ is the sample size</li> </ul>"},{"location":"api/likelihood/#precision-premultiplied-statistics","title":"Precision-Premultiplied Statistics","text":"<p>The likelihood functions operate on precision-premultiplied GWAS summary statistics:</p> <p>$$pz = n^{-1/2} R^{-1}z \\sim N(0, M), \\quad M = D + n^{-1}R^{-1}$$</p>"},{"location":"api/likelihood/#graphld.likelihood","title":"likelihood","text":"<p>Functions for computing likelihoods in the LDGM model.</p>"},{"location":"api/likelihood/#graphld.likelihood.gaussian_likelihood","title":"gaussian_likelihood","text":"<pre><code>gaussian_likelihood(pz: ndarray, M: PrecisionOperator) -&gt; float\n</code></pre> <p>Compute log-likelihood of GWAS summary statistics under a Gaussian model.</p> The model is <p>beta ~ MVN(0, D) z|beta ~ MVN(sqrt(n)Rbeta, R) where R is the LD matrix, n the sample size pz = inv(R) * z / sqrt(n) M = cov(pz) = D + inv(R)/n</p> <p>Parameters:</p> Name Type Description Default <code>pz</code> <code>ndarray</code> <p>Array of precision-premultiplied GWAS effect size estimates</p> required <code>M</code> <code>PrecisionOperator</code> <p>PrecisionOperator. This should be the covariance of pz.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Log-likelihood value</p> Source code in <code>src/graphld/likelihood.py</code> <pre><code>def gaussian_likelihood(\n    pz: np.ndarray,\n    M: PrecisionOperator,\n) -&gt; float:\n    \"\"\"Compute log-likelihood of GWAS summary statistics under a Gaussian model.\n\n    The model is:\n        beta ~ MVN(0, D)\n        z|beta ~ MVN(sqrt(n)*R*beta, R) where R is the LD matrix, n the sample size\n        pz = inv(R) * z / sqrt(n)\n        M = cov(pz) = D + inv(R)/n\n\n    Args:\n        pz: Array of precision-premultiplied GWAS effect size estimates\n        M: PrecisionOperator. This should be the covariance of pz.\n\n    Returns:\n        Log-likelihood value\n\n    \"\"\"\n    # Following scipy's convention:\n    # log_pdf = -0.5 * (n * log(2\u03c0) + log|\u03a3| + x^T \u03a3^{-1} x)\n    #        = -0.5 * (n * log(2\u03c0) - log|P| + x^T P x)\n    n = len(pz)\n    logdet = M.logdet()\n\n    # Compute quadratic form\n    b = M.solve(pz)\n    quad = np.sum(pz * b)\n\n    # Compute log likelihood\n    ll = -0.5 * (n * np.log(2 * np.pi) + logdet + quad)\n\n    return ll\n</code></pre>"},{"location":"api/likelihood/#graphld.likelihood.gaussian_likelihood_gradient","title":"gaussian_likelihood_gradient","text":"<pre><code>gaussian_likelihood_gradient(pz: ndarray, M: PrecisionOperator, del_M_del_a: Optional[ndarray] = None, n_samples: int = 10, seed: Optional[int] = None, trace_estimator: Optional[str] = 'xdiag') -&gt; np.ndarray\n</code></pre> <p>Computes the score under a Gaussian model.</p> <pre><code>The model is:\nbeta ~ MVN(0, D)\nz|beta ~ MVN(sqrt(n)*R*beta, R) where R is the LD matrix, n the sample size\npz = inv(R) * z / sqrt(n)\nM = cov(pz) = D + inv(R)/n\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pz</code> <code>ndarray</code> <p>Array of precision-premultiplied GWAS effect size estimates</p> required <code>M</code> <code>PrecisionOperator</code> <p>PrecisionOperator. This should be the covariance of pz.</p> required <code>del_M_del_a</code> <code>Optional[ndarray]</code> <p>Matrix of derivatives of M's diagonal elements wrt parameters a</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>Number of probe vectors for Hutchinson's method or xdiag</p> <code>10</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for generating probe vectors</p> <code>None</code> <code>trace_estimator</code> <code>Optional[str]</code> <p>Method for computing the trace estimator. Options: \"exact\", \"hutchinson\", \"xdiag\"</p> <code>'xdiag'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of diagonal elements of the gradient wrt M's diagonal elements,</p> <code>ndarray</code> <p>or with respect to parameters a if del_M_del_a is provided</p> Source code in <code>src/graphld/likelihood.py</code> <pre><code>def gaussian_likelihood_gradient(\n    pz: np.ndarray,\n    M: PrecisionOperator,\n    del_M_del_a: Optional[np.ndarray] = None,\n    n_samples: int = 10,\n    seed: Optional[int] = None,\n    trace_estimator: Optional[str] = \"xdiag\",\n) -&gt; np.ndarray:\n    \"\"\"Computes the score under a Gaussian model.\n\n        The model is:\n        beta ~ MVN(0, D)\n        z|beta ~ MVN(sqrt(n)*R*beta, R) where R is the LD matrix, n the sample size\n        pz = inv(R) * z / sqrt(n)\n        M = cov(pz) = D + inv(R)/n\n\n    Args:\n        pz: Array of precision-premultiplied GWAS effect size estimates\n        M: PrecisionOperator. This should be the covariance of pz.\n        del_M_del_a: Matrix of derivatives of M's diagonal elements wrt parameters a\n        n_samples: Number of probe vectors for Hutchinson's method or xdiag\n        seed: Random seed for generating probe vectors\n        trace_estimator: Method for computing the trace estimator.\n            Options: \"exact\", \"hutchinson\", \"xdiag\"\n\n    Returns:\n        Array of diagonal elements of the gradient wrt M's diagonal elements,\n        or with respect to parameters a if del_M_del_a is provided\n    \"\"\"\n    # Compute b = M^(-1) * pz\n    b = M.solve(pz)\n\n    # Compute diagonal elements of M^(-1)\n    minv_diag = M.inverse_diagonal(method=trace_estimator,\n                                    n_samples=n_samples,\n                                    seed=seed)\n\n    # Compute gradient diagonal elements\n    node_grad = -0.5 * (minv_diag.flatten() - b.flatten()**2)\n\n    return node_grad if del_M_del_a is None else node_grad @ del_M_del_a\n</code></pre>"},{"location":"api/likelihood/#graphld.likelihood.gaussian_likelihood_hessian","title":"gaussian_likelihood_hessian","text":"<pre><code>gaussian_likelihood_hessian(pz: ndarray, M: PrecisionOperator, del_M_del_a: Optional[ndarray] = None, diagonal_method: Optional[str] = None, n_samples: int = 100, seed: Optional[int] = None) -&gt; np.ndarray\n</code></pre> <p>Computes the average information matrix of the Gaussian log-likelihood.</p> The model is <p>beta ~ MVN(0, D) z|beta ~ MVN(sqrt(n)Rbeta, R) where R is the LD matrix, n the sample size pz = inv(R) * z / sqrt(n) M = cov(pz) = D + inv(R)/n</p> <p>Parameters:</p> Name Type Description Default <code>pz</code> <code>ndarray</code> <p>Array of precision-premultiplied GWAS effect size estimates</p> required <code>M</code> <code>PrecisionOperator</code> <p>PrecisionOperator. This should be the covariance of pz.</p> required <code>del_M_del_a</code> <code>Optional[ndarray]</code> <p>Matrix of derivatives of M's diagonal elements wrt parameters a. If None, only the diagonal elements are computed.</p> <code>None</code> <code>diagonal_method</code> <code>Optional[str]</code> <p>Method for computing the diagonal of the Hessian. Options: \"exact\", \"hutchinson\", \"xdiag\", None (default)</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>Number of probe vectors for Hutchinson's method or xdiag</p> <code>100</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for generating probe vectors</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Matrix of second derivatives wrt parameters a, or array of diagonal elements</p> <code>ndarray</code> <p>if del_M_del_a is None</p> Source code in <code>src/graphld/likelihood.py</code> <pre><code>def gaussian_likelihood_hessian(\n    pz: np.ndarray,\n    M: PrecisionOperator,\n    del_M_del_a: Optional[np.ndarray] = None,\n    diagonal_method: Optional[str]=None,\n    n_samples: int=100,\n    seed: Optional[int] = None,\n) -&gt; np.ndarray:\n    \"\"\"Computes the average information matrix of the Gaussian log-likelihood.\n\n    The model is:\n        beta ~ MVN(0, D)\n        z|beta ~ MVN(sqrt(n)*R*beta, R) where R is the LD matrix, n the sample size\n        pz = inv(R) * z / sqrt(n)\n        M = cov(pz) = D + inv(R)/n\n\n    Args:\n        pz: Array of precision-premultiplied GWAS effect size estimates\n        M: PrecisionOperator. This should be the covariance of pz.\n        del_M_del_a: Matrix of derivatives of M's diagonal elements wrt parameters a.\n            If None, only the diagonal elements are computed.\n        diagonal_method: Method for computing the diagonal of the Hessian.\n            Options: \"exact\", \"hutchinson\", \"xdiag\", None (default)\n        n_samples: Number of probe vectors for Hutchinson's method or xdiag\n        seed: Random seed for generating probe vectors\n\n    Returns:\n        Matrix of second derivatives wrt parameters a, or array of diagonal elements\n        if del_M_del_a is None\n    \"\"\"\n\n    # Compute b = M^(-1) * pz\n    b = M.solve(pz)\n    if b.ndim == 1:\n        b = b.reshape(-1, 1)\n\n    # If del_M_del_a is None, compute only the diagonal of the Hessian\n    if del_M_del_a is None:\n        minv_diag = M.inverse_diagonal(method=diagonal_method, n_samples=n_samples, seed=seed)\n        hess_diag = -0.5 * minv_diag.flatten() * b.flatten()**2\n        return hess_diag\n\n    # Compute b_scaled = b .* del_sigma_del_a\n    b_scaled = b * del_M_del_a\n\n    # Compute M^(-1) * b_scaled\n    minv_b_scaled = M.solve(b_scaled)\n\n    # Compute Hessian: -1/2 * b_scaled^T * M^(-1) * b_scaled\n    hess = -0.5 * (b_scaled.T @ minv_b_scaled)\n\n    return hess\n</code></pre>"},{"location":"api/multiprocessing/","title":"graphld.multiprocessing_template","text":"<p>Parallel processing utilities for LDGM-based algorithms.</p> <p><code>ParallelProcessor</code> is a base class for implementing parallel algorithms with LDGMs, wrapping Python's <code>multiprocessing</code> module. It splits work among processes, each of which loads a subset of LD blocks.</p>"},{"location":"api/multiprocessing/#graphld.multiprocessing_template","title":"multiprocessing_template","text":"<p>Base class for parallel processing applications.</p>"},{"location":"api/multiprocessing/#graphld.multiprocessing_template.SharedData","title":"SharedData","text":"<pre><code>SharedData(sizes: Dict[str, Union[int, None]])\n</code></pre> <p>Wrapper for shared memory data structures.</p> <p>Attributes:</p> Name Type Description <code>_data_dict</code> <p>Dictionary mapping keys to shared memory objects</p> <p>Initialize shared memory objects.</p> <p>Parameters:</p> Name Type Description Default <code>sizes</code> <code>Dict[str, Union[int, None]]</code> <p>Dictionary mapping keys to array sizes.   If size is None, creates a shared float value.</p> required Source code in <code>src/graphld/multiprocessing_template.py</code> <pre><code>def __init__(self, sizes: Dict[str, Union[int, None]]):\n    \"\"\"Initialize shared memory objects.\n\n    Args:\n        sizes: Dictionary mapping keys to array sizes.\n              If size is None, creates a shared float value.\n    \"\"\"\n    self._data_dict = {}\n    for key, size in sizes.items():\n        if size is None:\n            self._data_dict[key] = Value('d', 0.0)\n        else:\n            self._data_dict[key] = Array('d', size)\n</code></pre>"},{"location":"api/multiprocessing/#graphld.multiprocessing_template.SharedData.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key: Union[str, Tuple[str, slice]]) -&gt; Union[np.ndarray, float]\n</code></pre> <p>Get numpy array view or float value.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Union[str, Tuple[str, slice]]</code> <p>Key in data dictionary or tuple of (key, slice)</p> required <p>Returns:</p> Type Description <code>Union[ndarray, float]</code> <p>numpy array view for Array, float for Value</p> Source code in <code>src/graphld/multiprocessing_template.py</code> <pre><code>def __getitem__(\n    self, key: Union[str, Tuple[str, slice]]\n) -&gt; Union[np.ndarray, float]:\n    \"\"\"Get numpy array view or float value.\n\n    Args:\n        key: Key in data dictionary or tuple of (key, slice)\n\n    Returns:\n        numpy array view for Array, float for Value\n    \"\"\"\n    if isinstance(key, tuple):\n        array_key, slice_obj = key\n        return self[array_key][slice_obj]\n\n    data = self._data_dict[key]\n    if hasattr(data, 'get_obj'):  # Array has get_obj, Value doesn't\n        return np.frombuffer(data.get_obj(), dtype=np.float64)\n    return data.value\n</code></pre>"},{"location":"api/multiprocessing/#graphld.multiprocessing_template.SharedData.__setitem__","title":"__setitem__","text":"<pre><code>__setitem__(key: Union[str, Tuple[str, slice]], value: Union[ndarray, float])\n</code></pre> <p>Set array contents or float value.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Union[str, Tuple[str, slice]]</code> <p>Key in data dictionary or tuple of (key, slice)</p> required <code>value</code> <code>Union[ndarray, float]</code> <p>Array or float to set</p> required Source code in <code>src/graphld/multiprocessing_template.py</code> <pre><code>def __setitem__(\n    self,\n    key: Union[str, Tuple[str, slice]],\n    value: Union[np.ndarray, float]\n):\n    \"\"\"Set array contents or float value.\n\n    Args:\n        key: Key in data dictionary or tuple of (key, slice)\n        value: Array or float to set\n    \"\"\"\n    if isinstance(key, tuple):\n        array_key, slice_obj = key\n        data = self._data_dict[array_key]\n        if hasattr(data, 'get_obj'):\n            data[slice_obj] = value\n        else:\n            raise ValueError(\"Slice assignment only supported for Array types\")\n        return\n\n    data = self._data_dict[key]\n    if hasattr(data, 'get_obj'):  # Array has get_obj, Value doesn't\n        np.copyto(np.frombuffer(data.get_obj(), dtype=np.float64), value)\n    else:\n        data.value = float(value)\n</code></pre>"},{"location":"api/multiprocessing/#graphld.multiprocessing_template.SerialManager","title":"SerialManager","text":"<pre><code>SerialManager(num_processes: int)\n</code></pre> <p>Manager for debugging by running workers in serial.</p> <p>Initialize worker manager.</p> <p>Parameters:</p> Name Type Description Default <code>num_processes</code> <code>int</code> <p>Number of worker processes to manage</p> required Source code in <code>src/graphld/multiprocessing_template.py</code> <pre><code>def __init__(self, num_processes: int):\n    \"\"\"Initialize worker manager.\n\n    Args:\n        num_processes: Number of worker processes to manage\n    \"\"\"\n    self.flags = [Value('i', 0) for _ in range(num_processes)]\n    self.functions: List[Callable] = []\n    self.arguments: List[tuple] = []\n    self.states: List[Any] = []\n</code></pre>"},{"location":"api/multiprocessing/#graphld.multiprocessing_template.SerialManager.start_workers","title":"start_workers","text":"<pre><code>start_workers(flag: Optional[int] = None) -&gt; None\n</code></pre> <p>Signal workers to start processing.</p> <p>Parameters:</p> Name Type Description Default <code>flag</code> <code>Optional[int]</code> <p>Optional flag value to set (default: 1)</p> <code>None</code> Source code in <code>src/graphld/multiprocessing_template.py</code> <pre><code>def start_workers(self, flag: Optional[int] = None) -&gt; None:\n    \"\"\"Signal workers to start processing.\n\n    Args:\n        flag: Optional flag value to set (default: 1)\n    \"\"\"\n    for f in self.flags:\n        f.value = flag or 1\n\n    offset = 0\n    for i in range(len(self.flags)):\n        func, args, state = self.functions[i], self.arguments[i], self.states[i]\n        self.states[i] = func(state, offset, *args)\n        offset += self.states[i].shape[0]\n\n    assert all([f.value == 0 for f in self.flags])\n</code></pre>"},{"location":"api/multiprocessing/#graphld.multiprocessing_template.SerialManager.add_process","title":"add_process","text":"<pre><code>add_process(target: Callable, args: Tuple) -&gt; None\n</code></pre> <p>Add a worker process.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Callable</code> <p>Function to run in process</p> required <code>args</code> <code>Tuple</code> <p>Arguments to pass to function</p> required Source code in <code>src/graphld/multiprocessing_template.py</code> <pre><code>def add_process(self, target: Callable, args: Tuple) -&gt; None:\n    \"\"\"Add a worker process.\n\n    Args:\n        target: Function to run in process\n        args: Arguments to pass to function\n    \"\"\"\n    self.functions.append(target)\n    self.arguments.append(args)\n    self.states.append(None)\n</code></pre>"},{"location":"api/multiprocessing/#graphld.multiprocessing_template.SerialManager.shutdown","title":"shutdown","text":"<pre><code>shutdown() -&gt; None\n</code></pre> <p>Shutdown all worker processes.</p> Source code in <code>src/graphld/multiprocessing_template.py</code> <pre><code>def shutdown(self) -&gt; None:\n    \"\"\"Shutdown all worker processes.\"\"\"\n    pass\n</code></pre>"},{"location":"api/multiprocessing/#graphld.multiprocessing_template.WorkerManager","title":"WorkerManager","text":"<pre><code>WorkerManager(num_processes: int)\n</code></pre> <p>Manager for coordinating parallel worker processes.</p> <p>Attributes:</p> Name Type Description <code>flags</code> <p>List of shared flags for worker control</p> <code>processes</code> <code>List[Process]</code> <p>List of worker processes</p> <p>Initialize worker manager.</p> <p>Parameters:</p> Name Type Description Default <code>num_processes</code> <code>int</code> <p>Number of worker processes to manage</p> required Source code in <code>src/graphld/multiprocessing_template.py</code> <pre><code>def __init__(self, num_processes: int):\n    \"\"\"Initialize worker manager.\n\n    Args:\n        num_processes: Number of worker processes to manage\n    \"\"\"\n    self.flags = [Value('i', 0) for _ in range(num_processes)]\n    self.processes: List[Process] = []\n</code></pre>"},{"location":"api/multiprocessing/#graphld.multiprocessing_template.WorkerManager.start_workers","title":"start_workers","text":"<pre><code>start_workers(flag: Optional[int] = None) -&gt; None\n</code></pre> <p>Signal workers to start processing.</p> <p>Parameters:</p> Name Type Description Default <code>flag</code> <code>Optional[int]</code> <p>Optional flag value to set (default: 1)</p> <code>None</code> Source code in <code>src/graphld/multiprocessing_template.py</code> <pre><code>def start_workers(self, flag: Optional[int] = None) -&gt; None:\n    \"\"\"Signal workers to start processing.\n\n    Args:\n        flag: Optional flag value to set (default: 1)\n    \"\"\"\n    if flag is None:\n        flag = 1\n    for f in self.flags:\n        f.value = flag\n</code></pre>"},{"location":"api/multiprocessing/#graphld.multiprocessing_template.WorkerManager.await_workers","title":"await_workers","text":"<pre><code>await_workers() -&gt; None\n</code></pre> <p>Wait for all workers to finish current task; abort if a worker crashes.</p> Source code in <code>src/graphld/multiprocessing_template.py</code> <pre><code>def await_workers(self) -&gt; None:\n    \"\"\"Wait for all workers to finish current task; abort if a worker crashes.\"\"\"\n    while True:\n        # Kill all workers if any throws an error\n        if any(flag.value == -1 for flag in self.flags):\n            for f in self.flags:\n                f.value = -1\n            for p in self.processes:\n                p.join(timeout=0.5)\n            raise RuntimeError(\"Worker process crashed. Aborting.\")\n\n        # Normal completion\n        if all(flag.value &lt; 1 for flag in self.flags):\n            break\n\n        time.sleep(0.01)\n</code></pre>"},{"location":"api/multiprocessing/#graphld.multiprocessing_template.WorkerManager.add_process","title":"add_process","text":"<pre><code>add_process(target: Callable, args: Tuple) -&gt; None\n</code></pre> <p>Add a worker process.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Callable</code> <p>Function to run in process</p> required <code>args</code> <code>Tuple</code> <p>Arguments to pass to function</p> required Source code in <code>src/graphld/multiprocessing_template.py</code> <pre><code>def add_process(self, target: Callable, args: Tuple) -&gt; None:\n    \"\"\"Add a worker process.\n\n    Args:\n        target: Function to run in process\n        args: Arguments to pass to function\n    \"\"\"\n    process = Process(target=target, args=args)\n    process.start()\n    self.processes.append(process)\n</code></pre>"},{"location":"api/multiprocessing/#graphld.multiprocessing_template.WorkerManager.shutdown","title":"shutdown","text":"<pre><code>shutdown() -&gt; None\n</code></pre> <p>Shutdown all worker processes.</p> Source code in <code>src/graphld/multiprocessing_template.py</code> <pre><code>def shutdown(self) -&gt; None:\n    \"\"\"Shutdown all worker processes.\"\"\"\n    # Signal shutdown\n    for flag in self.flags:\n        flag.value = -1\n\n    # Wait for processes to finish\n    for process in self.processes:\n        process.join()\n</code></pre>"},{"location":"api/multiprocessing/#graphld.multiprocessing_template.ParallelProcessor","title":"ParallelProcessor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for parallel processing applications.</p> <p>This class provides a framework for parallel processing of LDGM files. Subclasses must implement the following methods:     - initialize: Set up shared memory arrays and data structures     - supervise: Monitor and control worker processes     - process_block: Process a single LDGM block</p>"},{"location":"api/multiprocessing/#graphld.multiprocessing_template.ParallelProcessor.create_shared_memory","title":"create_shared_memory  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>create_shared_memory(metadata: DataFrame, block_data: list, **kwargs) -&gt; SharedData\n</code></pre> <p>Initialize shared memory and data structures.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>DataFrame</code> <p>polars dataframe continaing LDGM metadata for each LD block</p> required <code>block_data</code> <code>list</code> <p>List of block-specific data</p> required <code>**kwargs</code> <p>Additional arguments passed from run()</p> <code>{}</code> <p>Returns:</p> Type Description <code>SharedData</code> <p>SharedData object containing shared memory arrays</p> Source code in <code>src/graphld/multiprocessing_template.py</code> <pre><code>@classmethod\n@abstractmethod\ndef create_shared_memory(\n    cls, metadata: pl.DataFrame, block_data: list, **kwargs\n) -&gt; 'SharedData':\n    \"\"\"Initialize shared memory and data structures.\n\n    Args:\n        metadata: polars dataframe continaing LDGM metadata for each LD block\n        block_data: List of block-specific data\n        **kwargs: Additional arguments passed from run()\n\n    Returns:\n        SharedData object containing shared memory arrays\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/multiprocessing/#graphld.multiprocessing_template.ParallelProcessor.supervise","title":"supervise  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>supervise(manager: Union[WorkerManager, SerialManager], shared_data: SharedData, block_data: list, **kwargs) -&gt; Any\n</code></pre> <p>Monitor workers and process results.</p> <p>Parameters:</p> Name Type Description Default <code>manager</code> <code>Union[WorkerManager, SerialManager]</code> <p>Worker manager for controlling processes</p> required <code>shared_data</code> <code>SharedData</code> <p>Shared memory data</p> required <code>**kwargs</code> <p>Additional arguments passed from run()</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Results of the parallel computation</p> Source code in <code>src/graphld/multiprocessing_template.py</code> <pre><code>@classmethod\n@abstractmethod\ndef supervise(cls, manager: Union[WorkerManager, SerialManager],\n            shared_data: SharedData,\n            block_data: list, **kwargs) -&gt; Any:\n    \"\"\"Monitor workers and process results.\n\n    Args:\n        manager: Worker manager for controlling processes\n        shared_data: Shared memory data\n        **kwargs: Additional arguments passed from run()\n\n    Returns:\n        Results of the parallel computation\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/multiprocessing/#graphld.multiprocessing_template.ParallelProcessor.process_block","title":"process_block  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>process_block(ldgm: PrecisionOperator, flag: Value, shared_data: SharedData, block_offset: int, block_data: Any = None, worker_params: Any = None) -&gt; None\n</code></pre> <p>Process single block.</p> <p>Parameters:</p> Name Type Description Default <code>ldgm</code> <code>PrecisionOperator</code> <p>LDGM object</p> required <code>flag</code> <code>Value</code> <p>Worker flag</p> required <code>shared_data</code> <code>SharedData</code> <p>Dictionary-like shared data object</p> required <code>block_offset</code> <code>int</code> <p>Offset for this block</p> required <code>block_data</code> <code>Any</code> <p>Optional block-specific data from prepare_block_data</p> <code>None</code> <code>worker_params</code> <code>Any</code> <p>Optional parameters passed to each worker process</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/graphld/multiprocessing_template.py</code> <pre><code>@classmethod\n@abstractmethod\ndef process_block(cls, ldgm: PrecisionOperator,\n                flag: Value,\n                shared_data: SharedData,\n                block_offset: int,\n                block_data: Any = None,\n                worker_params: Any = None) -&gt; None:\n    \"\"\"Process single block.\n\n    Args:\n        ldgm: LDGM object\n        flag: Worker flag\n        shared_data: Dictionary-like shared data object\n        block_offset: Offset for this block\n        block_data: Optional block-specific data from prepare_block_data\n        worker_params: Optional parameters passed to each worker process\n\n    Returns:\n        None\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/multiprocessing/#graphld.multiprocessing_template.ParallelProcessor.prepare_block_data","title":"prepare_block_data  <code>classmethod</code>","text":"<pre><code>prepare_block_data(metadata: DataFrame, **kwargs) -&gt; list\n</code></pre> <p>Prepare data specific to each block for processing.</p> <p>This method should return a list of length equal to the number of blocks, where each element contains any block-specific data needed by process_block. The base implementation returns None for each block.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>DataFrame</code> <p>Metadata DataFrame containing block information</p> required <code>**kwargs</code> <p>Additional arguments passed from run()</p> <code>{}</code> <p>Returns:</p> Type Description <code>list</code> <p>List of block-specific data, length equal to number of blocks</p> Source code in <code>src/graphld/multiprocessing_template.py</code> <pre><code>@classmethod\ndef prepare_block_data(cls, metadata: pl.DataFrame, **kwargs) -&gt; list:\n    \"\"\"Prepare data specific to each block for processing.\n\n    This method should return a list of length equal to the number of blocks,\n    where each element contains any block-specific data needed by process_block.\n    The base implementation returns None for each block.\n\n    Args:\n        metadata: Metadata DataFrame containing block information\n        **kwargs: Additional arguments passed from run()\n\n    Returns:\n        List of block-specific data, length equal to number of blocks\n    \"\"\"\n    return [None] * len(metadata)\n</code></pre>"},{"location":"api/multiprocessing/#graphld.multiprocessing_template.ParallelProcessor.worker","title":"worker  <code>classmethod</code>","text":"<pre><code>worker(files: list, block_data: list, flag: Value, shared_data: SharedData, offset: int, worker_params: Any = None) -&gt; None\n</code></pre> <p>Worker process that loads LDGMs and processes blocks.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>list</code> <p>List of LDGM files to process</p> required <code>block_data</code> <code>list</code> <p>List of block-specific data</p> required <code>flag</code> <code>Value</code> <p>Shared flag for worker control</p> required <code>shared_data</code> <code>SharedData</code> <p>Shared memory data</p> required <code>offset</code> <code>int</code> <p>In shared data, where to start processing</p> required <code>worker_params</code> <code>Any</code> <p>Optional parameters passed to process_block</p> <code>None</code> Source code in <code>src/graphld/multiprocessing_template.py</code> <pre><code>@classmethod\ndef worker(cls,\n           files: list,\n           block_data: list,\n           flag: Value,\n           shared_data: SharedData,\n           offset: int,\n           worker_params: Any = None\n           ) -&gt; None:\n    \"\"\"Worker process that loads LDGMs and processes blocks.\n\n    Args:\n        files: List of LDGM files to process\n        block_data: List of block-specific data\n        flag: Shared flag for worker control\n        shared_data: Shared memory data\n        offset: In shared data, where to start processing\n        worker_params: Optional parameters passed to process_block\n    \"\"\"\n    try:\n        # Load LDGMs once\n        ldgms = []\n        for file in files:\n            ldgm = load_ldgm(str(file))\n            ldgm.factor()\n            ldgms.append(ldgm)\n\n        while True:\n            # Wait for signal to start new iteration\n            while flag.value == 0:\n                time.sleep(0.01)\n\n            if flag.value == -1:  # shutdown signal\n                break\n\n            # Process all blocks and collect solutions\n            block_offset = offset\n            starting_flag = flag.value\n            for ldgm, data in zip(ldgms, block_data, strict=False):\n                try:\n                    cls.process_block(ldgm, flag, shared_data, block_offset, data, worker_params)\n                    # Check that process_block didn't modify the flag during normal execution\n                    assert flag.value == starting_flag, \"process_block should not change flag\"\n                except Exception:\n                    # Ensure the flag is flipped so the supervisor doesn't spin forever\n                    flag.value = -1\n                    raise\n                block_offset += ldgm.shape[0]\n            # Signal completion\n            flag.value = 0\n\n    except Exception as e:\n        print(f\"Error in worker: {e}\")\n        flag.value = -1\n</code></pre>"},{"location":"api/multiprocessing/#graphld.multiprocessing_template.ParallelProcessor.serial_worker","title":"serial_worker  <code>classmethod</code>","text":"<pre><code>serial_worker(ldgm: Optional[PrecisionOperator], offset: int, file: str, data: list, flag: Value, shared_data: SharedData, worker_params: Any) -&gt; PrecisionOperator\n</code></pre> <p>Worker process that loads LDGMs and processes blocks in serial.</p> <p>Parameters:</p> Name Type Description Default <code>ldgm</code> <code>Optional[PrecisionOperator]</code> <p>the LDGM that was previously loaded, or None</p> required <code>offset</code> <code>int</code> <p>In shared data, where to start processing</p> required <code>files</code> <p>List of LDGM files to process</p> required <code>data</code> <code>list</code> <p>List of block-specific data</p> required <code>flag</code> <code>Value</code> <p>Shared flag for worker control</p> required <code>shared_data</code> <code>SharedData</code> <p>Shared memory data</p> required <code>worker_params</code> <code>Any</code> <p>Optional parameters passed to process_block</p> required Source code in <code>src/graphld/multiprocessing_template.py</code> <pre><code>@classmethod\ndef serial_worker(cls,\n           ldgm: Optional[PrecisionOperator],\n           offset: int,\n           file: str,\n           data: list,\n           flag: Value,\n           shared_data: SharedData,\n           worker_params: Any\n           ) -&gt; PrecisionOperator:\n    \"\"\"Worker process that loads LDGMs and processes blocks in serial.\n\n    Args:\n        ldgm: the LDGM that was previously loaded, or None\n        offset: In shared data, where to start processing\n        files: List of LDGM files to process\n        data: List of block-specific data\n        flag: Shared flag for worker control\n        shared_data: Shared memory data\n        worker_params: Optional parameters passed to process_block\n    \"\"\"\n    if flag.value &lt;= 0:\n        raise ValueError(\"Serial worker should never be started with flag &lt;= 0\")\n\n    # Load LDGMs once\n    if ldgm is None:\n        ldgm = load_ldgm(str(file))\n        ldgm.factor()\n\n    # Process all blocks and collect solutions\n    cls.process_block(ldgm, flag, shared_data, offset, data, worker_params)\n\n    # Signal completion\n    flag.value = 0\n\n    return ldgm\n</code></pre>"},{"location":"api/multiprocessing/#graphld.multiprocessing_template.ParallelProcessor.run","title":"run  <code>classmethod</code>","text":"<pre><code>run(ldgm_metadata_path: str, populations: Optional[Union[str, List[str]]] = None, chromosomes: Optional[Union[int, List[int]]] = None, num_processes: Optional[int] = None, worker_params: Any = None, **kwargs) -&gt; Any\n</code></pre> <p>Run parallel computation.</p> <p>Parameters:</p> Name Type Description Default <code>ldgm_metadata_path</code> <code>str</code> <p>Path to metadata file</p> required <code>populations</code> <code>Optional[Union[str, List[str]]]</code> <p>Populations to process; None -&gt; all</p> <code>None</code> <code>chromosomes</code> <code>Optional[Union[int, List[int]]]</code> <p>Chromosomes to process; None -&gt; all</p> <code>None</code> <code>num_processes</code> <code>Optional[int]</code> <p>Number of processes to use</p> <code>None</code> <code>worker_params</code> <code>Any</code> <p>Optional parameters passed to each worker process</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Results of the parallel computation</p> Source code in <code>src/graphld/multiprocessing_template.py</code> <pre><code>@classmethod\ndef run(cls,\n        ldgm_metadata_path: str,\n        populations: Optional[Union[str, List[str]]] = None,\n        chromosomes: Optional[Union[int, List[int]]] = None,\n        num_processes: Optional[int] = None,\n        worker_params: Any = None,\n        **kwargs) -&gt; Any:\n    \"\"\"Run parallel computation.\n\n    Args:\n        ldgm_metadata_path: Path to metadata file\n        populations: Populations to process; None -&gt; all\n        chromosomes: Chromosomes to process; None -&gt; all\n        num_processes: Number of processes to use\n        worker_params: Optional parameters passed to each worker process\n        **kwargs: Additional arguments\n\n    Returns:\n        Results of the parallel computation\n    \"\"\"\n\n\n    # Read metadata first\n    metadata = read_ldgm_metadata(\n        ldgm_metadata_path,\n        populations=populations,\n        chromosomes=chromosomes\n    )\n\n    # Get list of files from metadata\n    ldgm_directory = Path(ldgm_metadata_path).parent\n    edgelist_files = [\n        ldgm_directory / block['name']\n        for block in metadata.iter_rows(named=True)\n    ]\n    if not edgelist_files:\n        raise FileNotFoundError(\"No edgelist files found in metadata\")\n\n    if num_processes is None:\n        num_processes = min(len(edgelist_files), cpu_count())\n\n    # Split files among processes\n    process_block_ranges, process_offsets = cls._split_blocks(metadata, num_processes)\n    process_files = [edgelist_files[start:end] for start, end in process_block_ranges]\n\n    # Data to be sent to each block individually\n    block_data = cls.prepare_block_data(metadata, **kwargs)\n    process_block_data = [block_data[start:end] for start, end in process_block_ranges]\n\n    # Data shared among all blocks\n    shared_data = cls.create_shared_memory(metadata, block_data, **kwargs)\n\n    # Create worker manager\n    manager = WorkerManager(num_processes)\n\n    # Start workers\n    for i in range(num_processes):\n        manager.add_process(\n            target=cls.worker,\n            args=(\n                process_files[i],\n                process_block_data[i],\n                manager.flags[i],\n                shared_data,\n                process_offsets[i],\n                worker_params,\n            )\n        )\n\n    # Run supervisor process\n    results = cls.supervise(manager, shared_data, block_data, **kwargs)\n\n    # Cleanup\n    manager.shutdown()\n\n    return results\n</code></pre>"},{"location":"api/multiprocessing/#graphld.multiprocessing_template.ParallelProcessor.run_serial","title":"run_serial  <code>classmethod</code>","text":"<pre><code>run_serial(ldgm_metadata_path: str, populations: Optional[Union[str, List[str]]] = None, chromosomes: Optional[Union[int, List[int]]] = None, num_processes: Optional[int] = None, worker_params: Any = None, **kwargs) -&gt; Any\n</code></pre> <p>Run parallel computation.</p> <p>Parameters:</p> Name Type Description Default <code>ldgm_metadata_path</code> <code>str</code> <p>Path to metadata file</p> required <code>populations</code> <code>Optional[Union[str, List[str]]]</code> <p>Populations to process; None -&gt; all</p> <code>None</code> <code>chromosomes</code> <code>Optional[Union[int, List[int]]]</code> <p>Chromosomes to process; None -&gt; all</p> <code>None</code> <code>num_processes</code> <code>Optional[int]</code> <p>Number of processes to use</p> <code>None</code> <code>worker_params</code> <code>Any</code> <p>Optional parameters passed to each worker process</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Results of the parallel computation</p> Source code in <code>src/graphld/multiprocessing_template.py</code> <pre><code>@classmethod\ndef run_serial(cls,\n        ldgm_metadata_path: str,\n        populations: Optional[Union[str, List[str]]] = None,\n        chromosomes: Optional[Union[int, List[int]]] = None,\n        num_processes: Optional[int] = None,\n        worker_params: Any = None,\n        **kwargs) -&gt; Any:\n    \"\"\"Run parallel computation.\n\n    Args:\n        ldgm_metadata_path: Path to metadata file\n        populations: Populations to process; None -&gt; all\n        chromosomes: Chromosomes to process; None -&gt; all\n        num_processes: Number of processes to use\n        worker_params: Optional parameters passed to each worker process\n        **kwargs: Additional arguments\n\n    Returns:\n        Results of the parallel computation\n    \"\"\"\n\n    # Read metadata first\n    metadata = read_ldgm_metadata(\n        ldgm_metadata_path,\n        populations=populations,\n        chromosomes=chromosomes\n    )\n\n    # Get list of files from metadata\n    ldgm_directory = Path(ldgm_metadata_path).parent\n    edgelist_files = [\n        ldgm_directory / block['name']\n        for block in metadata.iter_rows(named=True)\n    ]\n    if not edgelist_files:\n        raise FileNotFoundError(\"No edgelist files found in metadata\")\n\n    # Data to be sent to each block individually\n    block_data = cls.prepare_block_data(metadata, **kwargs)\n    num_blocks = len(block_data)\n\n    # Data shared among all blocks\n    shared_data = cls.create_shared_memory(metadata, block_data, **kwargs)\n\n    manager = SerialManager(num_blocks)\n\n    # Start workers\n    for i in range(num_blocks):\n        manager.add_process(\n            target=cls.serial_worker,\n            args=(\n                edgelist_files[i],\n                block_data[i],\n                manager.flags[i],\n                shared_data,\n                worker_params,\n            )\n        )\n\n    # Run supervisor process\n    results = cls.supervise(manager, shared_data, block_data, **kwargs)\n\n    # Cleanup\n    manager.shutdown()\n\n    return results\n</code></pre>"},{"location":"api/parquet_io/","title":"graphld.parquet_io","text":"<p>I/O functions for Parquet format summary statistics.</p> <p>Supports multi-trait parquet files with per-trait columns stored as <code>{trait}_BETA</code> and <code>{trait}_SE</code>.</p>"},{"location":"api/parquet_io/#graphld.parquet_io","title":"parquet_io","text":"<p>Functions for reading parquet summary statistics files.</p> <p>This module supports the parquet format produced by linear_dag's GWAS pipeline, which stores summary statistics with columns like {trait}_BETA and {trait}_SE. Multiple traits can be stored in a single file.</p>"},{"location":"api/parquet_io/#graphld.parquet_io.get_parquet_traits","title":"get_parquet_traits","text":"<pre><code>get_parquet_traits(file: Union[str, Path]) -&gt; list[str]\n</code></pre> <p>Get list of trait names from a parquet summary statistics file.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Union[str, Path]</code> <p>Path to parquet file</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of trait names found in the file</p> Source code in <code>src/graphld/parquet_io.py</code> <pre><code>def get_parquet_traits(file: Union[str, Path]) -&gt; list[str]:\n    \"\"\"Get list of trait names from a parquet summary statistics file.\n\n    Args:\n        file: Path to parquet file\n\n    Returns:\n        List of trait names found in the file\n    \"\"\"\n    schema = pl.read_parquet_schema(file)\n    traits = set()\n    for col in schema:\n        if col.endswith('_BETA'):\n            traits.add(col[:-5])  # Remove '_BETA' suffix\n        elif col.endswith('_SE'):\n            traits.add(col[:-3])  # Remove '_SE' suffix\n    return sorted(traits)\n</code></pre>"},{"location":"api/parquet_io/#graphld.parquet_io.read_parquet_sumstats","title":"read_parquet_sumstats","text":"<pre><code>read_parquet_sumstats(file: Union[str, Path], trait: Optional[str] = None, maximum_missingness: float = 1.0) -&gt; pl.DataFrame\n</code></pre> <p>Read parquet summary statistics file and return a single-trait DataFrame.</p> <p>The parquet format stores columns as {trait}_BETA and {trait}_SE for each trait. This function extracts a single trait and converts to a standard format with columns: SNP, CHR, POS, REF, ALT, N, Z.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Union[str, Path]</code> <p>Path to parquet file</p> required <code>trait</code> <code>Optional[str]</code> <p>Name of trait to extract. If None, uses the first trait found.</p> <code>None</code> <code>maximum_missingness</code> <code>float</code> <p>Maximum fraction of missing samples allowed (based on N column if present)</p> <code>1.0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with columns: SNP, CHR, POS, REF, ALT, N, Z</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified trait is not found in the file</p> Source code in <code>src/graphld/parquet_io.py</code> <pre><code>def read_parquet_sumstats(\n    file: Union[str, Path],\n    trait: Optional[str] = None,\n    maximum_missingness: float = 1.0,\n) -&gt; pl.DataFrame:\n    \"\"\"Read parquet summary statistics file and return a single-trait DataFrame.\n\n    The parquet format stores columns as {trait}_BETA and {trait}_SE for each trait.\n    This function extracts a single trait and converts to a standard format with\n    columns: SNP, CHR, POS, REF, ALT, N, Z.\n\n    Args:\n        file: Path to parquet file\n        trait: Name of trait to extract. If None, uses the first trait found.\n        maximum_missingness: Maximum fraction of missing samples allowed (based on N column if present)\n\n    Returns:\n        DataFrame with columns: SNP, CHR, POS, REF, ALT, N, Z\n\n    Raises:\n        ValueError: If the specified trait is not found in the file\n    \"\"\"\n    # Get available traits\n    available_traits = get_parquet_traits(file)\n    if not available_traits:\n        raise ValueError(f\"No traits found in parquet file {file}. \"\n                        \"Expected columns ending in _BETA and _SE.\")\n\n    # Determine which trait to use\n    if trait is None:\n        trait = available_traits[0]\n    elif trait not in available_traits:\n        raise ValueError(f\"Trait '{trait}' not found in parquet file. \"\n                        f\"Available traits: {available_traits}\")\n\n    # Read only necessary columns\n    beta_col = f\"{trait}_BETA\"\n    se_col = f\"{trait}_SE\"\n\n    # Read the schema to find variant info columns\n    schema = pl.read_parquet_schema(file)\n\n    # Standard variant info column mappings\n    variant_cols = []\n    col_renames = {}\n\n    # Check for various possible column names\n    if 'site_ids' in schema:\n        variant_cols.append('site_ids')\n        col_renames['site_ids'] = 'SNP'\n    elif 'SNP' in schema:\n        variant_cols.append('SNP')\n    elif 'rsid' in schema:\n        variant_cols.append('rsid')\n        col_renames['rsid'] = 'SNP'\n\n    if 'chrom' in schema:\n        variant_cols.append('chrom')\n        col_renames['chrom'] = 'CHR'\n    elif 'CHR' in schema:\n        variant_cols.append('CHR')\n\n    if 'position' in schema:\n        variant_cols.append('position')\n        col_renames['position'] = 'POS'\n    elif 'POS' in schema:\n        variant_cols.append('POS')\n    elif 'BP' in schema:\n        variant_cols.append('BP')\n        col_renames['BP'] = 'POS'\n\n    if 'ref' in schema:\n        variant_cols.append('ref')\n        col_renames['ref'] = 'REF'\n    elif 'REF' in schema:\n        variant_cols.append('REF')\n    elif 'A2' in schema:\n        variant_cols.append('A2')\n        col_renames['A2'] = 'REF'\n\n    if 'alt' in schema:\n        variant_cols.append('alt')\n        col_renames['alt'] = 'ALT'\n    elif 'ALT' in schema:\n        variant_cols.append('ALT')\n    elif 'A1' in schema:\n        variant_cols.append('A1')\n        col_renames['A1'] = 'ALT'\n\n    # Check for sample size column\n    has_n = False\n    if 'N' in schema:\n        variant_cols.append('N')\n        has_n = True\n    elif 'n' in schema:\n        variant_cols.append('n')\n        col_renames['n'] = 'N'\n        has_n = True\n\n    # Columns to read\n    columns_to_read = variant_cols + [beta_col, se_col]\n\n    # Read the parquet file\n    df = pl.read_parquet(file, columns=columns_to_read)\n\n    # Rename columns\n    if col_renames:\n        df = df.rename(col_renames)\n\n    # Compute Z score from BETA and SE\n    df = df.with_columns(\n        (pl.col(beta_col) / pl.col(se_col)).cast(pl.Float64).alias('Z')\n    )\n\n    # Drop the trait-specific columns, keeping just Z\n    df = df.drop([beta_col, se_col])\n\n    # Filter out rows with missing or invalid Z scores\n    df = df.filter(pl.col('Z').is_finite())\n\n    # Apply missingness filter if N column is present\n    if has_n and maximum_missingness &lt; 1.0:\n        max_n = df['N'].max()\n        min_n = (1 - maximum_missingness) * max_n\n        df = df.filter(pl.col('N') &gt;= min_n)\n\n    # If no N column, add a placeholder\n    if 'N' not in df.columns:\n        df = df.with_columns(pl.lit(None).cast(pl.Float64).alias('N'))\n\n    return df\n</code></pre>"},{"location":"api/parquet_io/#graphld.parquet_io.read_parquet_sumstats_multi","title":"read_parquet_sumstats_multi","text":"<pre><code>read_parquet_sumstats_multi(file: Union[str, Path], traits: Optional[list[str]] = None, maximum_missingness: float = 1.0) -&gt; dict[str, pl.DataFrame]\n</code></pre> <p>Read multiple traits from a parquet summary statistics file.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Union[str, Path]</code> <p>Path to parquet file</p> required <code>traits</code> <code>Optional[list[str]]</code> <p>List of trait names to extract. If None, extracts all traits.</p> <code>None</code> <code>maximum_missingness</code> <code>float</code> <p>Maximum fraction of missing samples allowed</p> <code>1.0</code> <p>Returns:</p> Type Description <code>dict[str, DataFrame]</code> <p>Dictionary mapping trait names to DataFrames</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any specified trait is not found in the file</p> Source code in <code>src/graphld/parquet_io.py</code> <pre><code>def read_parquet_sumstats_multi(\n    file: Union[str, Path],\n    traits: Optional[list[str]] = None,\n    maximum_missingness: float = 1.0,\n) -&gt; dict[str, pl.DataFrame]:\n    \"\"\"Read multiple traits from a parquet summary statistics file.\n\n    Args:\n        file: Path to parquet file\n        traits: List of trait names to extract. If None, extracts all traits.\n        maximum_missingness: Maximum fraction of missing samples allowed\n\n    Returns:\n        Dictionary mapping trait names to DataFrames\n\n    Raises:\n        ValueError: If any specified trait is not found in the file\n    \"\"\"\n    available_traits = get_parquet_traits(file)\n\n    if traits is None:\n        traits = available_traits\n    else:\n        # Validate all requested traits exist\n        missing = set(traits) - set(available_traits)\n        if missing:\n            raise ValueError(f\"Traits not found in parquet file: {missing}. \"\n                           f\"Available traits: {available_traits}\")\n\n    result = {}\n    for trait in traits:\n        result[trait] = read_parquet_sumstats(file, trait=trait,\n                                               maximum_missingness=maximum_missingness)\n\n    return result\n</code></pre>"},{"location":"api/precision/","title":"graphld.precision","text":"<p>Precision matrix operations using LDGM graphical models.</p> <p>The <code>PrecisionOperator</code> class is the core abstraction for working with LD matrices. It subclasses SciPy's <code>LinearOperator</code> and represents an LDGM precision matrix or its Schur complement.</p>"},{"location":"api/precision/#key-concepts","title":"Key Concepts","text":"<ul> <li>To compute <code>correlation_matrix[indices, indices] @ vector</code>, use <code>ldgm[indices].solve(vector)</code></li> <li>To compute <code>inv(correlation_matrix[indices, indices]) @ vector</code>, use <code>ldgm[indices] @ vector</code></li> </ul> <p>You cannot do this indexing manually - the submatrix of an inverse is not the inverse of a submatrix. See Section 5 of the supplementary material.</p>"},{"location":"api/precision/#graphld.precision","title":"precision","text":"<p>Precision matrix operations for LDGM.</p> <p>This module implements sparse precision matrix operations using scipy's LinearOperator interface for efficient matrix-vector operations.</p>"},{"location":"api/precision/#graphld.precision.PrecisionOperator","title":"PrecisionOperator  <code>dataclass</code>","text":"<pre><code>PrecisionOperator(_matrix: csc_matrix, variant_info: DataFrame, _which_indices: Optional[ndarray] = None, _solver: Optional[cholesky] = None, _cholesky_is_up_to_date: bool = False)\n</code></pre> <p>               Bases: <code>LinearOperator</code></p> <p>LDGM precision matrix class implementing the LinearOperator interface.</p> <p>This class provides an efficient implementation of precision matrix operations using the scipy.sparse.linalg.LinearOperator interface. It supports matrix-vector multiplication and other essential operations for working with LDGM precision matrices.</p> <p>Attributes:</p> Name Type Description <code>_matrix</code> <code>csc_matrix</code> <p>The precision matrix in sparse format</p> <code>variant_info</code> <code>DataFrame</code> <p>Polars DataFrame containing variant information</p> <code>_which_indices</code> <code>Optional[ndarray]</code> <p>Array of indices for current selection</p> <code>_solver</code> <code>Optional[cholesky]</code> <p>Previously computed Cholesky factorization</p> <code>_cholesky_is_up_to_date</code> <code>bool</code> <p>Flag indicating whether the Cholesky factorization is up to date</p>"},{"location":"api/precision/#graphld.precision.PrecisionOperator.shape","title":"shape  <code>property</code>","text":"<pre><code>shape\n</code></pre> <p>Get the shape of the matrix, accounting for partial indexing.</p>"},{"location":"api/precision/#graphld.precision.PrecisionOperator.dtype","title":"dtype  <code>property</code>","text":"<pre><code>dtype: dtype\n</code></pre> <p>Return the dtype of the precision matrix.</p>"},{"location":"api/precision/#graphld.precision.PrecisionOperator.nbytes","title":"nbytes  <code>property</code>","text":"<pre><code>nbytes: int\n</code></pre> <p>Return the total memory usage in bytes.</p>"},{"location":"api/precision/#graphld.precision.PrecisionOperator.matrix","title":"matrix  <code>property</code>","text":"<pre><code>matrix\n</code></pre> <p>Get the precision matrix.</p>"},{"location":"api/precision/#graphld.precision.PrecisionOperator.variant_indices","title":"variant_indices  <code>property</code>","text":"<pre><code>variant_indices\n</code></pre> <p>Get the indices of the variants in the precision matrix.</p>"},{"location":"api/precision/#graphld.precision.PrecisionOperator.diagonal_indices","title":"diagonal_indices  <code>cached</code> <code>property</code>","text":"<pre><code>diagonal_indices: ndarray\n</code></pre> <p>Get indices of diagonal elements corresponding to _which_indices in _matrix.data.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of indices into self._matrix.data where diagonal elements are stored</p>"},{"location":"api/precision/#graphld.precision.PrecisionOperator.inv","title":"inv  <code>property</code>","text":"<pre><code>inv: LinearOperator\n</code></pre> <p>A LinearOperator representing the LD correlation matrix.</p>"},{"location":"api/precision/#graphld.precision.PrecisionOperator.times_scalar","title":"times_scalar","text":"<pre><code>times_scalar(multiplier: float) -&gt; None\n</code></pre> <p>Multiply the precision matrix by a scalar in place.</p> Source code in <code>src/graphld/precision.py</code> <pre><code>def times_scalar(self, multiplier: float) -&gt; None:\n    \"\"\"Multiply the precision matrix by a scalar in place.\"\"\"\n    self._matrix *= multiplier\n    self.del_factor()\n</code></pre>"},{"location":"api/precision/#graphld.precision.PrecisionOperator.update_matrix","title":"update_matrix","text":"<pre><code>update_matrix(update: ndarray) -&gt; None\n</code></pre> <p>Update the precision matrix by adding values to its diagonal.</p> <p>If which_indices is set, only updates the corresponding diagonal elements.</p> <p>Parameters:</p> Name Type Description Default <code>update</code> <code>ndarray</code> <p>Vector of values to add to the diagonal elements</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If update shape doesn't match or would make diagonal non-positive</p> Source code in <code>src/graphld/precision.py</code> <pre><code>def update_matrix(self, update: np.ndarray) -&gt; None:\n    \"\"\"Update the precision matrix by adding values to its diagonal.\n\n    If which_indices is set, only updates the corresponding diagonal elements.\n\n    Args:\n        update: Vector of values to add to the diagonal elements\n\n    Raises:\n        ValueError: If update shape doesn't match or would make diagonal non-positive\n    \"\"\"\n    if len(update) != self.shape[0]:\n        msg = f\"Update vector length {len(update)} does not match matrix shape {self.shape}\"\n        raise ValueError(msg)\n\n    # if np.allclose(update, 0):\n    #     return\n\n    for idx, entry in zip(self.diagonal_indices, update, strict=False):\n        self._matrix.data[idx] += entry\n\n    if np.any(self._matrix.data[self.diagonal_indices] &lt;= 0):\n        raise ValueError(\"Update would make a diagonal element non-positive\")\n\n    self._cholesky_is_up_to_date = False\n</code></pre>"},{"location":"api/precision/#graphld.precision.PrecisionOperator.update_element","title":"update_element","text":"<pre><code>update_element(index: int, value: float) -&gt; None\n</code></pre> <p>Update a single diagonal element of the precision matrix.</p> <p>If which_indices is set, only updates the corresponding element.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The updated diagonal element is self._which_indices[index]</p> required <code>value</code> <code>float</code> <p>Value to add to the diagonal element</p> required Note <p>Updates the Cholesky factorization if it exists using a rank-1 update/downdate.</p> Source code in <code>src/graphld/precision.py</code> <pre><code>def update_element(self, index: int, value: float) -&gt; None:\n    \"\"\"Update a single diagonal element of the precision matrix.\n\n    If which_indices is set, only updates the corresponding element.\n\n    Args:\n        index: The updated diagonal element is self._which_indices[index]\n        value: Value to add to the diagonal element\n\n    Note:\n        Updates the Cholesky factorization if it exists using a rank-1 update/downdate.\n    \"\"\"\n    # Find diagonal element in sparse matrix\n    diag_pos = self.diagonal_indices[index]\n\n    # Convert to global index for Cholesky update\n    global_index = index if self._which_indices is None else self._which_indices[index]\n\n    # Update matrix value\n    new_val = self._matrix.data[diag_pos] + value\n    if new_val &lt;= 0:\n        msg = f\"Update would make diagonal element at index {global_index} non-positive\"\n        raise ValueError(msg)\n\n    self._matrix.data[diag_pos] = new_val\n\n    if not self._cholesky_is_up_to_date:\n        return\n\n    # Update Cholesky factorization\n    shape = (self._matrix.shape[0], 1)\n    e_sparse = csc_matrix(([np.sqrt(np.abs(value))], ([global_index], [0])), shape=shape)\n    self._solver.update_inplace(e_sparse, value &lt; 0)\n</code></pre>"},{"location":"api/precision/#graphld.precision.PrecisionOperator.factor","title":"factor","text":"<pre><code>factor() -&gt; None\n</code></pre> <p>Update the Cholesky factorization of the precision matrix.</p> Source code in <code>src/graphld/precision.py</code> <pre><code>def factor(self) -&gt; None:\n    \"\"\"Update the Cholesky factorization of the precision matrix.\"\"\"\n    if self._solver is None:\n        self._solver = cholesky(self._matrix)\n    else:\n        self._solver.cholesky_inplace(self._matrix)\n    self._cholesky_is_up_to_date = True\n</code></pre>"},{"location":"api/precision/#graphld.precision.PrecisionOperator.del_factor","title":"del_factor","text":"<pre><code>del_factor() -&gt; None\n</code></pre> <p>Free the memory used by the Cholesky factorization.</p> Source code in <code>src/graphld/precision.py</code> <pre><code>def del_factor(self) -&gt; None:\n    \"\"\"Free the memory used by the Cholesky factorization.\"\"\"\n    self._solver = None\n    self._cholesky_is_up_to_date = False\n</code></pre>"},{"location":"api/precision/#graphld.precision.PrecisionOperator.logdet","title":"logdet","text":"<pre><code>logdet() -&gt; float\n</code></pre> <p>Compute log determinant of the Schur complement.</p> <p>Returns:</p> Type Description <code>float</code> <p>Log determinant of the Schur complement</p> Source code in <code>src/graphld/precision.py</code> <pre><code>def logdet(self) -&gt; float:\n    \"\"\"Compute log determinant of the Schur complement.\n\n    Returns:\n        Log determinant of the Schur complement\n    \"\"\"\n    if not self._cholesky_is_up_to_date:\n        self.factor()\n\n    # Get boolean mask for current selection\n    mask = self._get_mask\n    if np.all(mask):\n        return self._solver.logdet()\n\n    # Get P11 submatrix for missing indices\n    P11 = self._matrix[~mask][:, ~mask]\n    logdet_P11 = cholesky(P11).logdet()\n\n    # Return logdet(P) - logdet(P11)\n    return self._solver.logdet() - logdet_P11\n</code></pre>"},{"location":"api/precision/#graphld.precision.PrecisionOperator.copy","title":"copy","text":"<pre><code>copy()\n</code></pre> <p>Copy the current LDGM instance.</p> Source code in <code>src/graphld/precision.py</code> <pre><code>def copy(self):\n    \"\"\"Copy the current LDGM instance.\"\"\"\n    which_indices = self._which_indices.copy() if self._which_indices is not None else None\n    solver = self._solver if self._solver is not None else None\n    return PrecisionOperator(self._matrix.copy(), self.variant_info.clone(),\n                            which_indices, solver, self._cholesky_is_up_to_date)\n</code></pre>"},{"location":"api/precision/#graphld.precision.PrecisionOperator.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key: Union[list, slice, ndarray]) -&gt; PrecisionOperator\n</code></pre> <p>Sets the _which_indices class attribute.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Union[list, slice, ndarray]</code> <p>Index, slice, or tuple of indices/slices to access. Can be: - List of indices - Array of indices - Boolean mask array - Slice object</p> required <p>Returns:</p> Type Description <code>PrecisionOperator</code> <p>New PrecisionOperator instance that shares the underlying matrix</p> Source code in <code>src/graphld/precision.py</code> <pre><code>def __getitem__(self, key: Union[list, slice, np.ndarray]) -&gt; 'PrecisionOperator':\n    \"\"\"Sets the _which_indices class attribute.\n\n    Args:\n        key: Index, slice, or tuple of indices/slices to access. Can be:\n            - List of indices\n            - Array of indices\n            - Boolean mask array\n            - Slice object\n\n    Returns:\n        New PrecisionOperator instance that shares the underlying matrix\n    \"\"\"\n    # Create new instance sharing the same matrix and variant_info\n    result = PrecisionOperator(\n        self._matrix,\n        self.variant_info,\n        self._which_indices,\n        None,\n        False\n    )\n    result.set_which_indices(key)\n    return result\n</code></pre>"},{"location":"api/precision/#graphld.precision.PrecisionOperator.set_which_indices","title":"set_which_indices","text":"<pre><code>set_which_indices(key: Union[list, slice, ndarray, int]) -&gt; None\n</code></pre> <p>Sets the _which_indices class attribute.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Union[list, slice, ndarray, int]</code> <p>Index, slice, or tuple of indices/slices to access. Can be: - List of indices - Array of indices - Boolean mask array - Slice object - Integer index</p> required Source code in <code>src/graphld/precision.py</code> <pre><code>def set_which_indices(self, key: Union[list, slice, np.ndarray, int]) -&gt; None:\n    \"\"\"Sets the _which_indices class attribute.\n\n    Args:\n        key: Index, slice, or tuple of indices/slices to access. Can be:\n            - List of indices\n            - Array of indices\n            - Boolean mask array\n            - Slice object\n            - Integer index\n    \"\"\"\n    # Convert key to indices array\n    if isinstance(key, slice):\n        # If we already have _which_indices, slice those instead of the full range\n        if self._which_indices is not None:\n            indices = self._which_indices[key]\n        else:\n            indices = np.arange(self._matrix.shape[0])[key]\n    elif isinstance(key, (list, np.ndarray)):\n        key = np.asarray(key)  # Convert list to array first\n        if key.dtype == bool:\n            # For boolean masks, convert to integer indices\n            if self._which_indices is not None:\n                # Apply mask to existing indices\n                indices = self._which_indices[key]\n            else:\n                indices = np.where(key)[0]\n        else:\n            # For integer indices, index into existing _which_indices if it exists\n            if self._which_indices is not None:\n                indices = self._which_indices[key]\n            else:\n                indices = key\n    elif isinstance(key, (int, np.integer)):\n        # For scalar indices, convert to array\n        if self._which_indices is not None:\n            indices = np.array([self._which_indices[key]])\n        else:\n            indices = np.array([key])\n    else:\n        raise TypeError(\"Invalid key type. Use integer, list, slice, or numpy array.\")\n\n    self._which_indices = indices\n</code></pre>"},{"location":"api/precision/#graphld.precision.PrecisionOperator.solve","title":"solve","text":"<pre><code>solve(b: ndarray, method: str = 'direct', tol: float = 1e-05, callback: Optional[Callable] = None, initialization: Optional[ndarray] = None) -&gt; np.ndarray\n</code></pre> <p>Solve the linear system Px = b.</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>ndarray</code> <p>Right-hand side vector or matrix</p> required <code>method</code> <code>str</code> <p>Solver method ('direct' or 'pcg')</p> <code>'direct'</code> <code>tol</code> <code>float</code> <p>Tolerance for PCG solver</p> <code>1e-05</code> <code>callback</code> <code>Optional[Callable]</code> <p>Optional callback for PCG solver</p> <code>None</code> <code>initialization</code> <code>Optional[ndarray]</code> <p>Optional initial guess for pcg</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Solution vector x</p> Source code in <code>src/graphld/precision.py</code> <pre><code>def solve(self,\n    b: np.ndarray,\n    method: str = \"direct\",\n    tol: float = 1e-5,\n    callback: Optional[Callable] = None,\n    initialization: Optional[np.ndarray] = None\n) -&gt; np.ndarray:\n    \"\"\"Solve the linear system Px = b.\n\n    Args:\n        b: Right-hand side vector or matrix\n        method: Solver method ('direct' or 'pcg')\n        tol: Tolerance for PCG solver\n        callback: Optional callback for PCG solver\n        initialization: Optional initial guess for pcg\n\n    Returns:\n        Solution vector x\n    \"\"\"\n    y = self._expand_vector(b)\n\n    if method == \"direct\":\n        if not self._cholesky_is_up_to_date:\n            self.factor()\n        solution = self._solver(y)\n    elif method == \"pcg\":\n        if self._solver is None:\n            self.factor()  # Some factorization is needed for use as a preconditioner\n        if self._cholesky_is_up_to_date:\n            solution = self._solver(y)\n        else:\n            solution = self._pcg(y, tol=tol, callback=callback, initialization=initialization)\n    else:\n        raise ValueError(\"Method must be either 'direct' or 'pcg'\")\n\n    if self._which_indices is not None:\n        solution = solution[self._which_indices, :]\n\n    return solution.reshape(b.shape)\n</code></pre>"},{"location":"api/precision/#graphld.precision.PrecisionOperator.solve_Lt","title":"solve_Lt","text":"<pre><code>solve_Lt(b: ndarray) -&gt; np.ndarray\n</code></pre> <p>Solve the triangular system L'x = b, where LL' = _matrix. If b ~ MVN(0, I), then x ~ MVN(0, R).</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>ndarray</code> <p>Right-hand side vector</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Solution vector x</p> Source code in <code>src/graphld/precision.py</code> <pre><code>def solve_Lt(self, b: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Solve the triangular system L'x = b, where LL' = _matrix.\n    If b ~ MVN(0, I), then x ~ MVN(0, R).\n\n    Args:\n        b: Right-hand side vector\n\n    Returns:\n        Solution vector x\n    \"\"\"\n    if not self._cholesky_is_up_to_date:\n        self.factor()\n\n    y = self._expand_vector(b)\n    solution = self._solver.solve_Lt(y, use_LDLt_decomposition=False)\n\n    if self._which_indices is not None:\n        solution = solution[self._which_indices, :]\n    return solution.reshape(b.shape)\n</code></pre>"},{"location":"api/precision/#graphld.precision.PrecisionOperator.variant_solve","title":"variant_solve","text":"<pre><code>variant_solve(b: ndarray) -&gt; np.ndarray\n</code></pre> <p>Computes correlation_matrix @ b where the dimension is number of variants, not number of indices. If two variants i,j have the same index (and are in perfect LD), b[i] and b[j] are summed, then solve() is called, then the solution vector is assigned the same values in entries i,j.</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>ndarray</code> <p>Right-hand side vector</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Solution vector z</p> Source code in <code>src/graphld/precision.py</code> <pre><code>def variant_solve(self, b: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Computes correlation_matrix @ b where the dimension is number of variants, not number of indices.\n    If two variants i,j have the same index (and are in perfect LD), b[i] and b[j] are summed, then\n    solve() is called, then the solution vector is assigned the same values in entries i,j.\n\n    Args:\n        b: Right-hand side vector\n\n    Returns:\n        Solution vector z\n    \"\"\"\n    if len(b) != len(self.variant_info):\n        raise ValueError(\"b must have the same length as the number of variants\")\n\n    y = np.zeros(self.shape[0])\n    np.add.at(y, self.variant_indices, b)\n\n    y = self.solve(y)\n\n    return y[self.variant_indices]\n</code></pre>"},{"location":"api/precision/#graphld.precision.PrecisionOperator.inverse_diagonal","title":"inverse_diagonal","text":"<pre><code>inverse_diagonal(method: str = 'xdiag', initialization: Optional[Tuple[ndarray, ndarray]] = None, n_samples: int = 100, seed: Optional[int] = None) -&gt; np.ndarray\n</code></pre> <p>Compute the diagonal elements of the inverse of the precision matrix.</p> <p>Parameters:</p> Name Type Description Default <code>initialization</code> <code>Optional[Tuple[ndarray, ndarray]]</code> <p>Optional tuple of matrices containing: - Initial guess for the diagonal elements - Initial guess for the off-diagonal elements</p> <code>None</code> <code>method</code> <code>str</code> <p>Method to use for computing diagonal elements ('exact', 'hutchinson', or 'xdiag')</p> <code>'xdiag'</code> <code>n_samples</code> <code>int</code> <p>Number of probe vectors for Hutchinson's method or xdiag</p> <code>100</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for generating probe vectors</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of diagonal elements of the inverse</p> Source code in <code>src/graphld/precision.py</code> <pre><code>def inverse_diagonal(\n    self,\n    method: str = \"xdiag\",\n    initialization: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n    n_samples: int = 100,\n    seed: Optional[int] = None,\n) -&gt; np.ndarray:\n    \"\"\"Compute the diagonal elements of the inverse of the precision matrix.\n\n    Args:\n        initialization: Optional tuple of matrices containing:\n            - Initial guess for the diagonal elements\n            - Initial guess for the off-diagonal elements\n        method: Method to use for computing diagonal elements\n            ('exact', 'hutchinson', or 'xdiag')\n        n_samples: Number of probe vectors for Hutchinson's method or xdiag\n        seed: Random seed for generating probe vectors\n\n    Returns:\n        Array of diagonal elements of the inverse\n    \"\"\"\n    if method not in [\"exact\", \"hutchinson\", \"xnys\", \"xdiag\"]:\n        raise ValueError(f\"Unknown method: {method}\")\n\n    # Slow, exact inversion\n    if method.lower() == \"exact\":\n        if initialization is not None:\n            raise ValueError(\"Initialization not supported for exact method\")\n\n        dense_matrix = self._matrix.toarray()\n        inv_matrix = np.linalg.inv(dense_matrix)\n        diag = np.diag(inv_matrix)\n\n        return diag if self._which_indices is None else diag[self._which_indices]\n\n    # Use a stochastic estimator\n    if initialization is not None:\n        v, pv = initialization\n    else:\n        rng = np.random.RandomState(seed)\n\n        # Rademacher random vectors\n        v = rng.choice([-1.0, 1.0], size=(self.shape[0], min(self.shape[0], n_samples)))\n        pv = v.copy()\n\n    if method.lower() == \"hutchinson\":\n        # Solve M * y = v for each probe vector\n        y = self.solve(v, method=\"pcg\", initialization=pv)\n\n        # Estimate diagonal elements as average element-wise product of v_i * y_i\n        diag_estimate = np.mean(v * y, axis=1)\n\n    elif method.lower() == \"xdiag\":\n        diag_estimate, y = self._xdiag_estimator(v, initialization=pv)\n\n    else:\n        raise NotImplementedError\n\n    return (diag_estimate, y) if initialization is not None else diag_estimate\n</code></pre>"},{"location":"api/score_test/","title":"score_test","text":"<p>Enrichment score test module for fast annotation testing.</p>"},{"location":"api/score_test/#submodules","title":"Submodules","text":"<ul> <li><code>score_test.score_test</code>: Core score test implementation</li> <li><code>score_test.score_test_io</code>: I/O functions for score test files</li> <li><code>score_test.meta_analysis</code>: Meta-analysis across traits</li> <li><code>score_test.convert_scores</code>: Convert between variant and gene-level scores</li> <li><code>score_test.genesets</code>: Gene set utilities</li> </ul>"},{"location":"api/score_test/#score_test","title":"score_test","text":"<p>GraphLD Score Test - Fast heritability enrichment testing for genomic annotations.</p> <p>This module provides a fast score test for testing genomic or gene annotations for heritability enrichment conditional upon a null model. The test produces Z scores where positive values indicate enrichment and negative values indicate depletion.</p> Main Functions <p>run_score_test: Run the enrichment score test on annotations load_variant_data: Load precomputed variant-level score statistics load_trait_data: Load trait-specific score statistics load_annotations: Load annotation data from LDSC format files load_gene_table: Load gene position table load_gene_sets_from_gmt: Load gene sets from GMT format files convert_gene_to_variant_annotations: Convert gene-level to variant-level annotations</p> <p>Example::</p> <pre><code>from score_test import run_score_test, load_variant_data, load_annotations\n\nvariant_data = load_variant_data(\"path/to/scores.h5\")\nannotations = load_annotations(\"path/to/annot_dir/\")\nresults = run_score_test(variant_data, annotations)\n</code></pre>"},{"location":"api/score_test/#score_test.run_score_test","title":"run_score_test","text":"<pre><code>run_score_test(trait_data: TraitData, annot: Annot) -&gt; Tuple[np.ndarray, np.ndarray]\n</code></pre> <p>Run approximate score test for hypothesis testing of new annotation or functional category.</p> <p>Unlike run_score_test, this function does not adjust for uncertainty in the fitted model parameters.</p> <p>Parameters:</p> Name Type Description Default <code>trait_data</code> <code>TraitData</code> <p>TraitData object containing variant data with gradients</p> required <code>annot</code> <code>Annot</code> <p>Annot object (VariantAnnot or GeneAnnot) containing annotations to test</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple of (point_estimates, jackknife_estimates)</p> Source code in <code>src/score_test/score_test.py</code> <pre><code>def run_score_test(trait_data: TraitData,\n    annot: Annot,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Run approximate score test for hypothesis testing of new annotation or functional category.\n\n    Unlike run_score_test, this function does not adjust for uncertainty in the fitted model parameters.\n\n    Args:\n        trait_data: TraitData object containing variant data with gradients\n        annot: Annot object (VariantAnnot or GeneAnnot) containing annotations to test\n\n    Returns:\n        Tuple of (point_estimates, jackknife_estimates)\n    \"\"\"\n    # Merge trait data with annotations\n    grad, _, _, test_annot, block_boundaries = annot.merge(trait_data)\n\n    noBlocks = len(block_boundaries) - 1\n\n    # Compute single-block derivatives\n    U_block = [] # Derivative of the log-likelihood for each test annotation\n    for i in range(noBlocks):\n        block = range(block_boundaries[i], block_boundaries[i+1])\n        Ui = grad[block].reshape(1,-1) @ test_annot[block, :]\n        U_block.append(Ui)\n    U_total = sum(U_block)\n\n    # Compute leave-one-out derivatives\n    U_jackknife = np.zeros((noBlocks, U_total.shape[1]))\n    for i in range(noBlocks):\n        block = range(block_boundaries[i], block_boundaries[i+1])\n\n        # deduct the score contributed from one LD block\n        U_jackknife[i, :] = U_total - U_block[i].ravel()\n\n    return U_total, U_jackknife\n</code></pre>"},{"location":"api/score_test/#score_test.load_trait_data","title":"load_trait_data","text":"<pre><code>load_trait_data(hdf5_path: str, trait_name: str, variant_table: DataFrame) -&gt; TraitData\n</code></pre> <p>Load trait data and combine with variant data into a TraitData object.</p> <p>Parameters:</p> Name Type Description Default <code>hdf5_path</code> <code>str</code> <p>Path to HDF5 file containing variant and trait data</p> required <code>trait_name</code> <code>str</code> <p>Name of the trait to load</p> required <code>variant_table</code> <code>DataFrame</code> <p>Pre-loaded variant table DataFrame</p> required <p>Returns:</p> Type Description <code>TraitData</code> <p>TraitData object containing variant dataframe with gradients/corrections and parameters</p> Source code in <code>src/score_test/score_test_io.py</code> <pre><code>def load_trait_data(\n    hdf5_path: str, trait_name: str, variant_table: pl.DataFrame\n) -&gt; \"TraitData\":\n    \"\"\"\n    Load trait data and combine with variant data into a TraitData object.\n\n    Args:\n        hdf5_path: Path to HDF5 file containing variant and trait data\n        trait_name: Name of the trait to load\n        variant_table: Pre-loaded variant table DataFrame\n\n    Returns:\n        TraitData object containing variant dataframe with gradients/corrections and parameters\n    \"\"\"\n    # Import at runtime to avoid circular import\n    try:\n        from .score_test import TraitData\n    except ImportError:\n        from score_test import TraitData\n\n    trait_hdf5 = load_trait_hdf5(hdf5_path, trait_name)\n\n    # Determine primary key column\n    possible_keys = [\"RSID\", \"gene_id\", \"gene_name\", \"CHR\", \"POS\"]\n    keys = [key for key in possible_keys if key in variant_table.columns]\n    if len(keys) == 0:\n        raise ValueError(\"variant_table must have one of: \" + \", \".join(possible_keys))\n\n    # Add trait-specific columns (gradient, hessian, etc.)\n    new_columns = []\n    for key_name, value in trait_hdf5.items():\n        if isinstance(value, np.ndarray):\n            decoded_value = _decode_bytes_array(value)\n            new_columns.append(pl.Series(name=key_name, values=decoded_value))\n\n    df = variant_table.with_columns(new_columns)\n\n    if \"parameters\" in trait_hdf5:\n        params = trait_hdf5[\"parameters\"][\"parameters\"]\n        jk_params = trait_hdf5[\"parameters\"][\"jackknife_parameters\"]\n    else:\n        params, jk_params = None, None\n\n    # Create TraitData - it will compute exclude_cols and annot_names via properties\n    trait_data = TraitData(\n        df=df,\n        params=params,\n        jk_params=jk_params,\n        keys=keys,\n    )\n\n    # Compute annotation names using the property\n    trait_data.annot_names = [\n        col for col in df.columns if col not in trait_data.exclude_cols\n    ]\n\n    return trait_data\n</code></pre>"},{"location":"api/score_test/#score_test.load_annotations","title":"load_annotations","text":"<pre><code>load_annotations(annot_path: str, chromosome: Optional[int] = None, infer_schema_length: int = 100000, add_positions: bool = True) -&gt; pl.DataFrame\n</code></pre> <p>Load annotation data for specified chromosome(s).</p> <p>Parameters:</p> Name Type Description Default <code>annot_path</code> <code>str</code> <p>Path to directory containing annotation files</p> required <code>chromosome</code> <code>Optional[int]</code> <p>Specific chromosome number, or None for all chromosomes</p> <code>None</code> <code>infer_schema_length</code> <code>int</code> <p>Number of rows to infer schema from</p> <code>100000</code> <code>add_positions</code> <code>bool</code> <p>If True, rename BP to POS</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing annotations</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no matching annotation files are found</p> Source code in <code>src/score_test/score_test_io.py</code> <pre><code>def load_annotations(\n    annot_path: str,\n    chromosome: Optional[int] = None,\n    infer_schema_length: int = 100_000,\n    add_positions: bool = True,\n) -&gt; pl.DataFrame:\n    \"\"\"Load annotation data for specified chromosome(s).\n\n    Args:\n        annot_path: Path to directory containing annotation files\n        chromosome: Specific chromosome number, or None for all chromosomes\n        infer_schema_length: Number of rows to infer schema from\n        add_positions: If True, rename BP to POS\n\n    Returns:\n        DataFrame containing annotations\n\n    Raises:\n        ValueError: If no matching annotation files are found\n    \"\"\"\n    # Determine which chromosomes to process\n    if chromosome is not None:\n        chromosomes = [chromosome]\n    else:\n        chromosomes = range(1, 23)  # Assuming chromosomes 1-22\n\n    # Find matching files\n    annotations = []\n    for chromosome in chromosomes:\n        file_pattern = f\"*.{chromosome}.annot\"\n        matching_files = Path(annot_path).glob(file_pattern)\n\n        # Read all matching files for this chromosome\n        dfs = []\n        cols_found = set()\n        for file_path in matching_files:\n            df = pl.scan_csv(\n                file_path, separator=\"\\t\", infer_schema_length=infer_schema_length\n            )\n            df = df.select([col for col in df.columns if col not in cols_found])\n            cols_found.update(df.columns)\n            dfs.append(df)\n\n        # Horizontally concatenate all dataframes for this chromosome\n        if dfs:\n            combined_df = pl.concat(dfs, how=\"horizontal\")\n            combined_df = combined_df.select(sorted(combined_df.columns))\n            annotations.append(combined_df)\n\n    # Check if any files were found\n    if not annotations:\n        raise ValueError(\n            f\"No annotation files found in {annot_path} matching pattern *.{{chrom}}.annot\"\n        )\n\n    # Concatenate all chromosome dataframes vertically\n    annotations = pl.concat(annotations, how=\"vertical\").collect()\n\n    # Convert binary columns to boolean to save memory\n    numeric_types = (pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.Float32, pl.Float64)\n    binary_cols = []\n    for col in annotations.columns:\n        # Skip non-numeric columns\n        if not isinstance(annotations[col].dtype, numeric_types):\n            continue\n        # Check if column only contains 0, 1 and null values\n        unique_vals = set(annotations[col].unique().drop_nulls())\n        if unique_vals == {0, 1}:\n            binary_cols.append(col)\n\n    # Convert binary columns to boolean\n    if binary_cols:\n        bool_exprs = [pl.col(col).cast(pl.Boolean) for col in binary_cols]\n        annotations = annotations.with_columns(bool_exprs)\n\n    if add_positions:\n        annotations = annotations.rename({\"BP\": \"POS\"})\n\n    return annotations\n</code></pre>"},{"location":"api/score_test/#score_test.load_gene_table","title":"load_gene_table","text":"<pre><code>load_gene_table(gene_table_path: str, chromosomes: list[int] | None = None) -&gt; pl.DataFrame\n</code></pre> <p>Load gene table and optionally filter to specific chromosomes.</p> <p>Parameters:</p> Name Type Description Default <code>gene_table_path</code> <code>str</code> <p>Path to gene table TSV</p> required <code>chromosomes</code> <code>list[int] | None</code> <p>Optional list of chromosome numbers to filter to</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Gene table DataFrame</p> Source code in <code>src/score_test/score_test_io.py</code> <pre><code>def load_gene_table(\n    gene_table_path: str, chromosomes: list[int] | None = None\n) -&gt; pl.DataFrame:\n    \"\"\"Load gene table and optionally filter to specific chromosomes.\n\n    Args:\n        gene_table_path: Path to gene table TSV\n        chromosomes: Optional list of chromosome numbers to filter to\n\n    Returns:\n        Gene table DataFrame\n    \"\"\"\n    schema = {\n        \"gene_id\": pl.Utf8,\n        \"gene_id_version\": pl.Utf8,\n        \"gene_name\": pl.Utf8,\n        \"start\": pl.Int64,\n        \"end\": pl.Int64,\n        \"CHR\": pl.Utf8,\n    }\n    gene_table = (\n        pl.scan_csv(gene_table_path, schema=schema, separator=\"\\t\", has_header=True)\n        .filter(pl.col(\"CHR\").is_in([str(i) for i in range(1, 23)]))\n        .filter(pl.col(\"gene_id\").is_not_null())\n        .with_columns(pl.col(\"gene_name\").fill_null(\"NA\"))\n        .with_columns(((pl.col(\"start\") + pl.col(\"end\")) / 2).alias(\"midpoint\"))\n        .with_columns(pl.col(\"midpoint\").alias(\"POS\"))\n        .sort(pl.col(\"CHR\").cast(pl.Int64), \"midpoint\")\n        .collect()\n    )\n\n    if chromosomes:\n        # Convert chromosomes to integers if they're strings\n        if isinstance(chromosomes[0], str):\n            chromosomes = [int(c) for c in chromosomes if c.isdigit()]\n        gene_table = gene_table.filter(pl.col(\"CHR\").cast(pl.Int64).is_in(chromosomes))\n\n    # Add POS column (using midpoint) for compatibility with position-based functions\n    gene_table = gene_table.with_columns(pl.col(\"midpoint\").cast(pl.Int64).alias(\"POS\"))\n\n    return gene_table\n</code></pre>"},{"location":"api/score_test/#score_test.load_gene_sets_from_gmt","title":"load_gene_sets_from_gmt","text":"<pre><code>load_gene_sets_from_gmt(gene_annot_dir: str) -&gt; dict[str, list[str]]\n</code></pre> <p>Load gene sets from GMT files in a directory.</p> <p>GMT format: set_namedescriptiongene1gene2... <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>Dictionary mapping set names to lists of genes</p> Source code in <code>src/score_test/score_test_io.py</code> <pre><code>def load_gene_sets_from_gmt(gene_annot_dir: str) -&gt; dict[str, list[str]]:\n    \"\"\"Load gene sets from GMT files in a directory.\n\n    GMT format: set_name&lt;tab&gt;description&lt;tab&gt;gene1&lt;tab&gt;gene2&lt;tab&gt;...\n\n    Returns:\n        Dictionary mapping set names to lists of genes\n    \"\"\"\n    import glob\n    from pathlib import Path\n\n    gmt_files = glob.glob(str(Path(gene_annot_dir) / \"*.gmt\"))\n    if not gmt_files:\n        raise FileNotFoundError(f\"No .gmt files found in {gene_annot_dir}\")\n\n    gene_sets = {}\n    for gmt_file in gmt_files:\n        with open(gmt_file, \"r\") as f:\n            for line in f:\n                parts = line.strip().split(\"\\t\")\n                if len(parts) &gt;= 3:\n                    set_name = parts[0]\n                    genes = parts[2:]  # Skip description\n                    gene_sets[set_name] = genes\n\n    return gene_sets\n</code></pre>"},{"location":"api/score_test/#score_test.convert_gene_to_variant_annotations","title":"convert_gene_to_variant_annotations","text":"<pre><code>convert_gene_to_variant_annotations(gene_annot, variant_table: DataFrame, gene_table: DataFrame, nearest_weights: ndarray)\n</code></pre> <p>Convert gene annotations to variant-level annotations.</p> <p>Can accept either: - GeneAnnot object with gene_sets and annot_names attributes - dict[str, list[str]] mapping gene set names to gene lists</p> <p>Parameters:</p> Name Type Description Default <code>gene_annot</code> <p>GeneAnnot object or dict mapping set names to lists of genes (symbols or IDs)</p> required <code>variant_table</code> <code>DataFrame</code> <p>Variant table DataFrame with CHR, POS, RSID columns</p> required <code>gene_table</code> <code>DataFrame</code> <p>Gene table DataFrame</p> required <code>nearest_weights</code> <code>ndarray</code> <p>Weights for k-nearest genes</p> required <p>Returns:</p> Type Description <p>VariantAnnot object with variant-level annotations (if gene_annot is GeneAnnot)</p> <p>or DataFrame with variant-level annotations in LDSC format (if gene_annot is dict)</p> Source code in <code>src/score_test/genesets.py</code> <pre><code>def convert_gene_to_variant_annotations(gene_annot,\n                                        variant_table: pl.DataFrame,\n                                        gene_table: pl.DataFrame,\n                                        nearest_weights: np.ndarray):\n    \"\"\"Convert gene annotations to variant-level annotations.\n\n    Can accept either:\n    - GeneAnnot object with gene_sets and annot_names attributes\n    - dict[str, list[str]] mapping gene set names to gene lists\n\n    Args:\n        gene_annot: GeneAnnot object or dict mapping set names to lists of genes (symbols or IDs)\n        variant_table: Variant table DataFrame with CHR, POS, RSID columns\n        gene_table: Gene table DataFrame\n        nearest_weights: Weights for k-nearest genes\n\n    Returns:\n        VariantAnnot object with variant-level annotations (if gene_annot is GeneAnnot)\n        or DataFrame with variant-level annotations in LDSC format (if gene_annot is dict)\n    \"\"\"\n    # Import at runtime to avoid circular import\n    try:\n        from .score_test import VariantAnnot\n    except ImportError:\n        from score_test import VariantAnnot\n\n    # Handle both GeneAnnot objects and plain dicts\n    if hasattr(gene_annot, 'gene_sets'):\n        gene_sets = gene_annot.gene_sets\n        annot_names = gene_annot.annot_names\n        return_variant_annot = True\n    else:\n        gene_sets = gene_annot\n        annot_names = None\n        return_variant_annot = False\n\n    # Determine if using gene IDs or symbols\n    first_set = next(iter(gene_sets.values()))\n    use_gene_id = _is_gene_id(first_set[0]) if first_set else False\n    gene_key = 'gene_id' if use_gene_id else 'gene_name'\n\n    # Get gene-variant matrix\n    gv_matrix = gene_variant_matrix(variant_table, gene_table, nearest_weights)\n\n    # Convert each gene set to variant-level annotation\n    variant_annots = {}\n    gene_identifiers = gene_table[gene_key].to_list()\n\n    for set_name, genes in gene_sets.items():\n        gene_set = set(genes)\n        gene_values = np.array([1.0 if gene in gene_set else 0.0\n                                for gene in gene_identifiers], dtype=np.float64)\n        variant_values = (gv_matrix @ gene_values.reshape(-1, 1)).ravel()\n        variant_annots[set_name] = variant_values\n\n    # Create output DataFrame in LDSC format\n    df_annot = pl.DataFrame({\n        'CHR': variant_table['CHR'],\n        'BP': variant_table['POS'],\n        'RSID': variant_table['RSID'],\n        'CM': pl.Series([0.0] * len(variant_table)),\n        **variant_annots\n    })\n\n    if return_variant_annot:\n        return VariantAnnot(df_annot, annot_names)\n    else:\n        return df_annot\n</code></pre>"},{"location":"api/score_test/#score_test.gene_variant_matrix","title":"gene_variant_matrix","text":"<pre><code>gene_variant_matrix(variant_table: DataFrame, gene_table: DataFrame, nearest_weights: ndarray) -&gt; csr_matrix\n</code></pre> <p>Compute gene-variant matrix from data.</p> <p>Parameters:</p> Name Type Description Default <code>variant_table</code> <code>DataFrame</code> <p>DataFrame with CHR, POS columns</p> required <code>gene_table</code> <code>DataFrame</code> <p>DataFrame with CHR, POS columns (POS is midpoint for genes)</p> required <code>nearest_weights</code> <code>ndarray</code> <p>Weights for k-nearest genes</p> required <p>Returns:</p> Type Description <code>csr_matrix</code> <p>Sparse matrix mapping genes to variants (ngenes x nvariants)</p> Source code in <code>src/score_test/genesets.py</code> <pre><code>def gene_variant_matrix(variant_table: pl.DataFrame, gene_table: pl.DataFrame,\n                        nearest_weights: np.ndarray) -&gt; csr_matrix:\n    \"\"\"Compute gene-variant matrix from data.\n\n    Args:\n        variant_table: DataFrame with CHR, POS columns\n        gene_table: DataFrame with CHR, POS columns (POS is midpoint for genes)\n        nearest_weights: Weights for k-nearest genes\n\n    Returns:\n        Sparse matrix mapping genes to variants (ngenes x nvariants)\n    \"\"\"\n    variant_positions = _compute_positions(variant_table)\n    gene_positions = _compute_positions(gene_table)\n    return _get_gene_variant_matrix(variant_positions, gene_positions, nearest_weights)\n</code></pre>"},{"location":"api/score_test/#score_test.score_test","title":"score_test","text":""},{"location":"api/score_test/#score_test.score_test.TraitData","title":"TraitData  <code>dataclass</code>","text":"<pre><code>TraitData(df: DataFrame, params: ndarray | None = None, jk_params: ndarray | None = None, annot_names: List[str] = None, keys: List[str] = None)\n</code></pre>"},{"location":"api/score_test/#score_test.score_test.TraitData.exclude_cols","title":"exclude_cols  <code>property</code>","text":"<pre><code>exclude_cols: set[str]\n</code></pre> <p>Columns to exclude when determining annotation names.</p>"},{"location":"api/score_test/#score_test.score_test.Annot","title":"Annot","text":"<pre><code>Annot(annot_names: List[str], other_key: str)\n</code></pre> <p>Base class for annotations that can be merged with TraitData.</p> <p>Parameters:</p> Name Type Description Default <code>annot_names</code> <code>List[str]</code> <p>List of annotation column names to test</p> required <code>other_key</code> <code>str</code> <p>Column name to use for merging (e.g., 'SNP', 'gene_id', 'gene_name')</p> required Source code in <code>src/score_test/score_test.py</code> <pre><code>def __init__(self, annot_names: List[str], other_key: str):\n    \"\"\"\n    Args:\n        annot_names: List of annotation column names to test\n        other_key: Column name to use for merging (e.g., 'SNP', 'gene_id', 'gene_name')\n    \"\"\"\n    self.annot_names = annot_names\n    self.other_key = other_key\n</code></pre>"},{"location":"api/score_test/#score_test.score_test.Annot.merge","title":"merge","text":"<pre><code>merge(trait_data: TraitData) -&gt; tuple[pl.DataFrame, np.ndarray, np.ndarray, np.ndarray]\n</code></pre> <p>Merge with TraitData and return extracted arrays.</p> <p>Returns:</p> Type Description <code>tuple[DataFrame, ndarray, ndarray, ndarray]</code> <p>Tuple of (merged_df, test_annot, model_annot, block_boundaries)</p> Source code in <code>src/score_test/score_test.py</code> <pre><code>def merge(self, trait_data: TraitData) -&gt; tuple[pl.DataFrame, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Merge with TraitData and return extracted arrays.\n\n    Returns:\n        Tuple of (merged_df, test_annot, model_annot, block_boundaries)\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement merge()\")\n</code></pre>"},{"location":"api/score_test/#score_test.score_test.VariantAnnot","title":"VariantAnnot","text":"<pre><code>VariantAnnot(df: DataFrame, annot_names: List[str])\n</code></pre> <p>               Bases: <code>Annot</code></p> <p>Variant-level annotations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with variant annotations (must have 'RSID' column)</p> required <code>annot_names</code> <code>List[str]</code> <p>List of annotation column names to test</p> required Source code in <code>src/score_test/score_test.py</code> <pre><code>def __init__(self, df: pl.DataFrame, annot_names: List[str]):\n    \"\"\"\n    Args:\n        df: DataFrame with variant annotations (must have 'RSID' column)\n        annot_names: List of annotation column names to test\n    \"\"\"\n    super().__init__(annot_names, other_key='RSID')\n    self.df = df\n</code></pre>"},{"location":"api/score_test/#score_test.score_test.VariantAnnot.perturb","title":"perturb","text":"<pre><code>perturb(fraction: float, seed: int | None = None)\n</code></pre> <p>Perturb binary annotations.</p> Source code in <code>src/score_test/score_test.py</code> <pre><code>def perturb(self, fraction: float, seed: int | None = None):\n    \"\"\"Perturb binary annotations.\"\"\"\n\n    def _perturb_binary_vector(vals: np.ndarray, fraction: float, rng: np.random.Generator) -&gt; np.ndarray:\n        p = np.mean(vals)\n        assert 0 &lt;= p &lt;= 1, f\"p must be between 0 and 1, got {p}\"\n        perturb_mask = (rng.random(len(vals)) &lt; fraction)\n        vals[perturb_mask] = rng.binomial(1, p, np.sum(perturb_mask)).astype(vals.dtype)\n        return vals\n\n    rng = np.random.default_rng(seed)\n    new_cols = []\n    kept_names = []\n    for col in self.annot_names:\n        vals = self.df[col].to_numpy().copy()\n\n        if np.any(np.isnan(vals)):\n            raise ValueError(f\"Annotation '{col}' contains NaNs\")\n\n        unique = np.unique(vals)\n        if np.all(np.isin(unique, [0, 1])):\n            vals = _perturb_binary_vector(vals, fraction, rng)\n            new_cols.append(pl.Series(col, vals))\n            kept_names.append(col)\n\n    if new_cols:\n        self.df = self.df.with_columns(new_cols)\n\n    self.annot_names = kept_names\n</code></pre>"},{"location":"api/score_test/#score_test.score_test.VariantAnnot.merge","title":"merge","text":"<pre><code>merge(trait_data: TraitData) -&gt; tuple[np.ndarray, np.ndarray | None, np.ndarray, np.ndarray]\n</code></pre> <p>Merge variant annotations with TraitData.</p> <p>Returns:</p> Name Type Description <code>ndarray</code> <p>Tuple of (grad, correction, test_annot, block_boundaries)</p> <code>Note</code> <code>ndarray | None</code> <p>correction is None if hessian is not available</p> Source code in <code>src/score_test/score_test.py</code> <pre><code>def merge(self, trait_data: TraitData) -&gt; tuple[np.ndarray, np.ndarray | None, np.ndarray, np.ndarray]:\n    \"\"\"Merge variant annotations with TraitData.\n\n    Returns:\n        Tuple of (grad, correction, test_annot, block_boundaries)\n        Note: correction is None if hessian is not available\n    \"\"\"\n    # Check if trait_data has the required key\n    if self.other_key not in trait_data.keys:\n        raise ValueError(f\"TraitData does not have required key '{self.other_key}'. Available keys: {trait_data.keys}\")\n\n    # Verify merge key exists in annotation DataFrame\n    if self.other_key not in self.df.columns:\n        raise ValueError(f\"Merge key '{self.other_key}' not found in annotation DataFrame. \"\n                       f\"Available columns: {self.df.columns}\")\n\n    df_merged = trait_data.df.join(\n        self.df,\n        left_on=self.other_key,\n        right_on=self.other_key,\n        how='inner',\n        maintain_order='left'\n    )\n    block_boundaries = get_block_boundaries(df_merged['jackknife_blocks'].to_numpy())\n\n    # Extract arrays from merged dataframe\n    grad = df_merged['gradient'].to_numpy().astype(np.float64)\n    hessian = df_merged['hessian'].to_numpy().astype(np.float64) if 'hessian' in df_merged.columns else None\n    model_annot = df_merged[trait_data.annot_names].to_numpy().astype(np.float64) if trait_data.annot_names else None\n    test_annot = df_merged[self.annot_names].to_numpy().astype(np.float64)\n\n    return grad, hessian, model_annot, test_annot, block_boundaries\n</code></pre>"},{"location":"api/score_test/#score_test.score_test.GeneAnnot","title":"GeneAnnot","text":"<pre><code>GeneAnnot(gene_sets: dict[str, list[str]])\n</code></pre> <p>               Bases: <code>Annot</code></p> <p>Gene-level annotations.</p> <p>Parameters:</p> Name Type Description Default <code>gene_sets</code> <code>dict[str, list[str]]</code> <p>Dictionary mapping set names to lists of genes</p> required Source code in <code>src/score_test/score_test.py</code> <pre><code>def __init__(self, gene_sets: dict[str, list[str]]):\n    \"\"\"\n    Args:\n        gene_sets: Dictionary mapping set names to lists of genes\n    \"\"\"\n    self.gene_sets = gene_sets\n\n    # Determine gene key from first gene in first set\n    # Import helper function\n    try:\n        from .genesets import _is_gene_id\n    except ImportError:\n        from genesets import _is_gene_id\n\n    first_gene = next(iter(next(iter(gene_sets.values()))))\n    self.other_key = 'gene_id' if _is_gene_id(first_gene) else 'gene_name'\n\n    super().__init__(list(gene_sets.keys()), self.other_key)\n</code></pre>"},{"location":"api/score_test/#score_test.score_test.GeneAnnot.merge","title":"merge","text":"<pre><code>merge(trait_data: TraitData) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]\n</code></pre> <p>Merge gene annotations with gene-level TraitData.</p> <p>Creates one-hot encodings for each gene set and returns unmodified grad/correction from the TraitData.</p> <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray, ndarray, ndarray]</code> <p>Tuple of (grad, correction, test_annot, block_boundaries)</p> Source code in <code>src/score_test/score_test.py</code> <pre><code>def merge(self, trait_data: TraitData) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Merge gene annotations with gene-level TraitData.\n\n    Creates one-hot encodings for each gene set and returns unmodified grad/correction\n    from the TraitData.\n\n    Returns:\n        Tuple of (grad, correction, test_annot, block_boundaries)\n    \"\"\"\n    # Check if trait_data has the required key\n    if self.other_key not in trait_data.keys:\n        raise ValueError(f\"TraitData does not have required key '{self.other_key}'. Available keys: {trait_data.keys}\")\n    # Use the appropriate key for merging (gene_id or gene_name)\n    # For gene-level data, we need to match using the same identifier type as the gene sets\n    merge_key = self.other_key\n\n    # Verify merge key exists in trait_data\n    if merge_key not in trait_data.df.columns:\n        raise ValueError(f\"Merge key '{merge_key}' not found in trait_data.df\")\n\n    # Get gene identifiers from trait_data\n    gene_ids = trait_data.df[merge_key].to_list()\n\n    # Create one-hot encoding for each gene set\n    # TODO vectorize\n    test_annot_dict = {}\n    for set_name, gene_list in self.gene_sets.items():\n        # Create binary indicator: 1 if gene is in set, 0 otherwise\n        gene_set = set(gene_list)\n        test_annot_dict[set_name] = [1.0 if gene in gene_set else 0.0 for gene in gene_ids]\n\n    # Create DataFrame with one-hot encodings\n    test_annot_df = pl.DataFrame(test_annot_dict)\n    test_annot = test_annot_df.to_numpy().astype(np.float64)\n\n    # Extract grad and correction (unmodified from TraitData)\n    grad = trait_data.df['gradient'].to_numpy().astype(np.float64)\n\n    # Get block boundaries\n    block_boundaries = get_block_boundaries(trait_data.df['jackknife_blocks'].to_numpy())\n\n    return grad, None, None, test_annot, block_boundaries\n</code></pre>"},{"location":"api/score_test/#score_test.score_test.GenomeAnnot","title":"GenomeAnnot","text":"<pre><code>GenomeAnnot()\n</code></pre> <p>               Bases: <code>Annot</code></p> <p>Genome region annotations (from BED files).</p> <p>TODO: Implement GenomeAnnot for BED file annotations.</p> Source code in <code>src/score_test/score_test.py</code> <pre><code>def __init__(self):\n    \"\"\"TODO: Implement GenomeAnnot for BED file annotations.\"\"\"\n    raise NotImplementedError(\"GenomeAnnot not yet implemented\")\n</code></pre>"},{"location":"api/score_test/#score_test.score_test.GenomeAnnot.merge","title":"merge","text":"<pre><code>merge(trait_data: TraitData) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]\n</code></pre> <p>Merge genome region annotations with TraitData.</p> <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray, ndarray, ndarray]</code> <p>Tuple of (grad, correction, test_annot, block_boundaries)</p> Source code in <code>src/score_test/score_test.py</code> <pre><code>def merge(self, trait_data: TraitData) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Merge genome region annotations with TraitData.\n\n    Returns:\n        Tuple of (grad, correction, test_annot, block_boundaries)\n    \"\"\"\n    # TODO: Implement BED file annotation logic\n    raise NotImplementedError(\"GenomeAnnot.merge() not yet implemented\")\n</code></pre>"},{"location":"api/score_test/#score_test.score_test.run_score_test","title":"run_score_test","text":"<pre><code>run_score_test(trait_data: TraitData, annot: Annot) -&gt; Tuple[np.ndarray, np.ndarray]\n</code></pre> <p>Run approximate score test for hypothesis testing of new annotation or functional category.</p> <p>Unlike run_score_test, this function does not adjust for uncertainty in the fitted model parameters.</p> <p>Parameters:</p> Name Type Description Default <code>trait_data</code> <code>TraitData</code> <p>TraitData object containing variant data with gradients</p> required <code>annot</code> <code>Annot</code> <p>Annot object (VariantAnnot or GeneAnnot) containing annotations to test</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple of (point_estimates, jackknife_estimates)</p> Source code in <code>src/score_test/score_test.py</code> <pre><code>def run_score_test(trait_data: TraitData,\n    annot: Annot,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Run approximate score test for hypothesis testing of new annotation or functional category.\n\n    Unlike run_score_test, this function does not adjust for uncertainty in the fitted model parameters.\n\n    Args:\n        trait_data: TraitData object containing variant data with gradients\n        annot: Annot object (VariantAnnot or GeneAnnot) containing annotations to test\n\n    Returns:\n        Tuple of (point_estimates, jackknife_estimates)\n    \"\"\"\n    # Merge trait data with annotations\n    grad, _, _, test_annot, block_boundaries = annot.merge(trait_data)\n\n    noBlocks = len(block_boundaries) - 1\n\n    # Compute single-block derivatives\n    U_block = [] # Derivative of the log-likelihood for each test annotation\n    for i in range(noBlocks):\n        block = range(block_boundaries[i], block_boundaries[i+1])\n        Ui = grad[block].reshape(1,-1) @ test_annot[block, :]\n        U_block.append(Ui)\n    U_total = sum(U_block)\n\n    # Compute leave-one-out derivatives\n    U_jackknife = np.zeros((noBlocks, U_total.shape[1]))\n    for i in range(noBlocks):\n        block = range(block_boundaries[i], block_boundaries[i+1])\n\n        # deduct the score contributed from one LD block\n        U_jackknife[i, :] = U_total - U_block[i].ravel()\n\n    return U_total, U_jackknife\n</code></pre>"},{"location":"api/score_test/#score_test.score_test.main","title":"main","text":"<pre><code>main(variant_stats_hdf5, output_fp, variant_annot_dir, gene_annot_dir, random_genes, random_variants, gene_table, nearest_weights, annotations, trait_name, verbose, seed, perturb_annot)\n</code></pre> <p>Run score test for annotation enrichment.</p> Source code in <code>src/score_test/score_test.py</code> <pre><code>@click.command(context_settings={'help_option_names': ['-h', '--help']})\n@click.argument('variant_stats_hdf5', type=click.Path(exists=True))\n@click.argument('output_fp', required=False, default=None)\n@click.option('-a', '--variant-annot-dir', 'variant_annot_dir', type=click.Path(exists=True),\n              help=\"Directory containing variant-level annotation files (.annot).\")\n@click.option('-g', '--gene-annot-dir', 'gene_annot_dir', type=click.Path(exists=True),\n              help=\"Directory containing gene-level annotations to convert to variant-level.\")\n@click.option('--random-genes', 'random_genes',\n              help=\"Comma-separated probabilities (0-1) for random gene-level annotations (e.g., '0.1,0.01').\")\n@click.option('--random-variants', 'random_variants',\n              help=\"Comma-separated probabilities (0-1) for random variant-level annotations (e.g., '0.1,0.01').\")\n@click.option('--gene-table', default='data/genes.tsv', type=click.Path(exists=True),\n              help=\"Path to gene table TSV file (required for gene-level options).\")\n@click.option('--nearest-weights', default='0.4,0.2,0.1,0.1,0.1,0.05,0.05',\n              help=\"Comma-separated weights for k-nearest genes (for gene-level options).\")\n@click.option('--annotations',\n              help=\"Optional comma-separated list of specific annotation names to test.\")\n@click.option('-n', '--name', 'trait_name',\n              help=\"Specific trait name to process from HDF5 file. If omitted, all traits are processed.\")\n@click.option('-v', '--verbose', is_flag=True,\n              help='Enable verbose output (log messages and results to console).')\n@click.option('--seed', type=int, default=None,\n              help='Seed for generating random annotations.')\n@click.option('--perturb-annot', type=float, default=0,\n              help='Fraction of variants to perturb for calibration testing.')\ndef main(variant_stats_hdf5, output_fp, variant_annot_dir, gene_annot_dir, random_genes,\n         random_variants, gene_table, nearest_weights, annotations, trait_name, verbose,\n         seed, perturb_annot):\n    \"\"\"Run score test for annotation enrichment.\"\"\"\n\n    _setup_logging(output_fp, verbose)\n    logging.info(f\"{time.strftime('%Y-%m-%d %H:%M:%S')}\")\n    logging.info(f\"Command: {' '.join(sys.argv)}\")\n    start_time = time.time()\n\n    # Set random seed if provided\n    if seed:\n        np.random.seed(seed)\n\n    # Parse annotation names if provided\n    annot_names_filter = [a.strip() for a in annotations.split(',')] if annotations else None\n\n    # Detect if this is gene-level or variant-level data\n    is_gene_level = is_gene_level_hdf5(variant_stats_hdf5)\n    data_type = \"gene\" if is_gene_level else \"variant\"\n\n    # Load row data (variants or genes)\n    data_table = load_row_data(variant_stats_hdf5)\n    logging.info(f\"Loaded {len(data_table)} {data_type}s from {variant_stats_hdf5}\")\n\n    # Load annotations based on source type\n    weights = np.array([float(w) for w in nearest_weights.split(',')], dtype=np.float64)\n\n    num_provided = 0\n    annot = None\n\n    if variant_annot_dir:\n        if is_gene_level:\n            raise click.UsageError(\"Cannot use --variant-annot-dir with gene-level HDF5 file\")\n        annot = load_variant_annotations(variant_annot_dir, annot_names_filter)\n        logging.info(f\"Loaded {len(annot.annot_names)} variant annotations from {variant_annot_dir}\")\n        num_provided += 1\n\n    if gene_annot_dir:\n        gene_annot: GeneAnnot = load_gene_annotations(\n            gene_annot_dir, data_table, gene_table, weights, annot_names_filter\n        )\n        if is_gene_level:\n            # For gene-level data, use gene annotations directly\n            annot = gene_annot\n            logging.info(f\"Loaded {len(annot.annot_names)} gene annotations from {gene_annot_dir}\")\n        else:\n            # For variant-level data, convert gene to variant annotations\n            chromosomes = data_table['CHR'].unique().sort().to_list()\n            gene_table_df = load_gene_table(gene_table, chromosomes)\n            annot = convert_gene_to_variant_annotations(gene_annot, data_table, gene_table_df, weights)\n            logging.info(f\"Loaded {len(annot.annot_names)} gene annotations from {gene_annot_dir}\")\n        num_provided += 1\n\n    if random_genes:\n        probs = _parse_probs(random_genes)\n        gene_annot: GeneAnnot = create_random_gene_annotations(\n            data_table, gene_table, probs\n        )\n        if is_gene_level:\n            # For gene-level data, use gene annotations directly\n            annot = gene_annot\n            logging.info(f\"Created {len(annot.annot_names)} random gene annotations\")\n        else:\n            # For variant-level data, convert gene to variant annotations\n            chromosomes = data_table['CHR'].unique().sort().to_list()\n            gene_table_df = load_gene_table(gene_table, chromosomes)\n            annot = convert_gene_to_variant_annotations(gene_annot, data_table, gene_table_df, weights)\n            logging.info(f\"Created {len(annot.annot_names)} random gene annotations\")\n        num_provided += 1\n\n    if random_variants:\n        if is_gene_level:\n            raise click.UsageError(\"Cannot use --random-variants with gene-level HDF5 file\")\n        probs = _parse_probs(random_variants)\n        annot = create_random_variant_annotations(data_table, probs)\n        logging.info(f\"Created {len(annot.annot_names)} random variant annotations\")\n        num_provided += 1\n\n    if num_provided != 1:\n        msg = \"Must specify exactly one of: --variant-annot-dir, \" + \\\n            \"--gene-annot-dir, --random-genes, --random-variants\"\n        raise click.UsageError(msg)\n\n    # Apply perturbation if requested\n    if perturb_annot &gt; 0:\n        if isinstance(annot, VariantAnnot):\n             logging.info(f\"Perturbing annotations with fraction {perturb_annot}\")\n             annot.perturb(perturb_annot, seed)\n        else:\n             logging.warning(\"Perturbation only supported for VariantAnnot\")\n\n    # Run the score test\n    results_dict = {'annotation' : annot.annot_names}\n    trait_names = get_trait_names(variant_stats_hdf5, trait_name)\n\n    # Store results for each trait for meta-analysis\n    trait_results = {}\n\n    for trait in trait_names:\n        trait_data = load_trait_data(variant_stats_hdf5, trait_name=trait, variant_table=data_table)\n\n        point_estimates, jackknife_estimates = run_score_test(\n                trait_data=trait_data,\n                annot=annot,\n            )\n\n        # Store for meta-analysis\n        trait_results[trait] = (point_estimates, jackknife_estimates)\n\n        # Compute Z-scores\n        std_dev = np.std(jackknife_estimates, axis=0)\n        z_col = f\"{trait}_Z\"\n        z_scores = point_estimates.ravel() / std_dev / np.sqrt(jackknife_estimates.shape[0] - 1)\n        results_dict[z_col] = z_scores\n\n    # Load trait groups and perform meta-analyses\n    trait_groups = get_trait_groups(variant_stats_hdf5)\n    for group_name, group_traits in trait_groups.items():\n        # Filter to traits that were actually processed\n        group_traits = [t for t in group_traits if t in trait_results]\n        meta = MetaAnalysis()\n        for trait in group_traits:\n            meta.update(*trait_results[trait])\n        z_col = f\"{group_name}_Z\"\n        results_dict[z_col] = meta.z_scores.ravel()\n        logging.info(f\"Computed meta-analysis for group '{group_name}' with {len(group_traits)} traits\")\n\n    results_df = pl.DataFrame(results_dict)\n\n    if verbose or not output_fp:\n        with pl.Config(tbl_rows=-1, tbl_cols=-1):\n            print(results_df)\n\n    if random_genes or random_variants or perturb_annot &gt; 0:\n        print(\"\\nRoot mean squared Z-scores:\")\n        for col in [c for c in results_df.columns if c.endswith('_Z')]:\n            print(f\"{col}: {np.sqrt(np.mean(results_df[col].to_numpy()**2)):.4f}\")\n\n    if output_fp:\n        results_df.write_csv(output_fp + \".txt\", separator='\\t')\n        logging.info(f'Results written to {output_fp}.txt')\n\n    logging.info(f'Total time: {time.time()-start_time:.2f}s')\n</code></pre>"},{"location":"api/score_test/#score_test.score_test_io","title":"score_test_io","text":"<p>I/O operations for score test.</p>"},{"location":"api/score_test/#score_test.score_test_io.is_gene_level_hdf5","title":"is_gene_level_hdf5","text":"<pre><code>is_gene_level_hdf5(hdf5_path: str) -&gt; bool\n</code></pre> <p>Check if HDF5 file contains gene-level data.</p> <p>Parameters:</p> Name Type Description Default <code>hdf5_path</code> <code>str</code> <p>Path to HDF5 file</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if file contains gene-level data, False for variant-level</p> Source code in <code>src/score_test/score_test_io.py</code> <pre><code>def is_gene_level_hdf5(hdf5_path: str) -&gt; bool:\n    \"\"\"Check if HDF5 file contains gene-level data.\n\n    Args:\n        hdf5_path: Path to HDF5 file\n\n    Returns:\n        True if file contains gene-level data, False for variant-level\n    \"\"\"\n    with h5py.File(hdf5_path, \"r\") as f:\n        # Check metadata attribute first\n        if \"data_type\" in f.attrs:\n            return f.attrs[\"data_type\"] == \"gene\"\n        # Fallback: check if row_data contains gene_id\n        if \"row_data\" not in f:\n            raise ValueError(\"HDF5 file must contain 'row_data' group\")\n        return \"gene_id\" in f[\"row_data\"]\n</code></pre>"},{"location":"api/score_test/#score_test.score_test_io.load_row_data","title":"load_row_data","text":"<pre><code>load_row_data(hdf5_path: str) -&gt; pl.DataFrame\n</code></pre> <p>Load row data table from HDF5 file format.</p> <p>Parameters:</p> Name Type Description Default <code>hdf5_path</code> <code>str</code> <p>Path to the HDF5 file</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Polars DataFrame containing row data (variants or genes)</p> Source code in <code>src/score_test/score_test_io.py</code> <pre><code>def load_row_data(hdf5_path: str) -&gt; pl.DataFrame:\n    \"\"\"\n    Load row data table from HDF5 file format.\n\n    Args:\n        hdf5_path: Path to the HDF5 file\n\n    Returns:\n        Polars DataFrame containing row data (variants or genes)\n    \"\"\"\n\n    if not os.path.exists(hdf5_path):\n        raise FileNotFoundError(f\"HDF5 file not found: {hdf5_path}\")\n\n    with h5py.File(hdf5_path, \"r\") as f:\n        if \"row_data\" not in f:\n            raise ValueError(\"HDF5 file must contain 'row_data' group\")\n\n        # Use helper to load all datasets\n        data = _load_hdf5_group(f[\"row_data\"])\n\n        # Remove attributes (prefixed with @)\n        data = {k: v for k, v in data.items() if not k.startswith(\"@\")}\n\n        # Process arrays: flatten 2D columns with shape (n, 1) and convert bytes to strings\n        for key, arr in data.items():\n            if hasattr(arr, \"shape\"):\n                # Flatten 2D arrays with single column\n                if len(arr.shape) == 2 and arr.shape[1] == 1:\n                    data[key] = arr.ravel()\n                    arr = data[key]  # update reference\n\n                # Convert bytes to strings\n                data[key] = _decode_bytes_array(arr)\n\n    return pl.DataFrame(data)\n</code></pre>"},{"location":"api/score_test/#score_test.score_test_io.get_trait_names","title":"get_trait_names","text":"<pre><code>get_trait_names(hdf5_path: str, trait_name: Optional[str] = None) -&gt; List[str]\n</code></pre> <p>Get list of trait names from HDF5 file.</p> <p>Parameters:</p> Name Type Description Default <code>hdf5_path</code> <code>str</code> <p>Path to the HDF5 file containing trait data</p> required <code>trait_name</code> <code>Optional[str]</code> <p>Optional specific trait name or meta-analysis name.        If provided, returns list with just this trait, or expands meta-analysis to its traits.        If None, returns all trait names in the file.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of trait names</p> Source code in <code>src/score_test/score_test_io.py</code> <pre><code>def get_trait_names(hdf5_path: str, trait_name: Optional[str] = None) -&gt; List[str]:\n    \"\"\"\n    Get list of trait names from HDF5 file.\n\n    Args:\n        hdf5_path: Path to the HDF5 file containing trait data\n        trait_name: Optional specific trait name or meta-analysis name.\n                   If provided, returns list with just this trait, or expands meta-analysis to its traits.\n                   If None, returns all trait names in the file.\n\n    Returns:\n        List of trait names\n    \"\"\"\n    with h5py.File(hdf5_path, \"r\") as f:\n        if trait_name:\n            # Check if it's a trait first\n            if trait_name in f[\"traits\"].keys():\n                return [trait_name]\n\n            # Check if it's a meta-analysis group\n            groups = get_trait_groups(hdf5_path)\n            if trait_name in groups:\n                return groups[trait_name]\n\n            # Not found as either trait or meta-analysis\n            raise ValueError(\n                f\"Trait or meta-analysis '{trait_name}' not found in HDF5 file\"\n            )\n        else:\n            return list(f[\"traits\"].keys())\n</code></pre>"},{"location":"api/score_test/#score_test.score_test_io.load_trait_hdf5","title":"load_trait_hdf5","text":"<pre><code>load_trait_hdf5(hdf5_path: str, trait_name: str) -&gt; dict\n</code></pre> <p>Load trait data from HDF5 file format.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with keys: gradient (required), and optionally parameters, jackknife_parameters, other datasets</p> Source code in <code>src/score_test/score_test_io.py</code> <pre><code>def load_trait_hdf5(hdf5_path: str, trait_name: str) -&gt; dict:\n    \"\"\"\n    Load trait data from HDF5 file format.\n\n    Returns:\n        Dictionary with keys: gradient (required), and optionally parameters, jackknife_parameters, other datasets\n    \"\"\"\n\n    with h5py.File(hdf5_path, \"r\") as f:\n        trait_group = f[f\"traits/{trait_name}\"]\n\n        # Use helper to load everything recursively\n        data = _load_hdf5_group(trait_group)\n\n    return data\n</code></pre>"},{"location":"api/score_test/#score_test.score_test_io.load_trait_data","title":"load_trait_data","text":"<pre><code>load_trait_data(hdf5_path: str, trait_name: str, variant_table: DataFrame) -&gt; TraitData\n</code></pre> <p>Load trait data and combine with variant data into a TraitData object.</p> <p>Parameters:</p> Name Type Description Default <code>hdf5_path</code> <code>str</code> <p>Path to HDF5 file containing variant and trait data</p> required <code>trait_name</code> <code>str</code> <p>Name of the trait to load</p> required <code>variant_table</code> <code>DataFrame</code> <p>Pre-loaded variant table DataFrame</p> required <p>Returns:</p> Type Description <code>TraitData</code> <p>TraitData object containing variant dataframe with gradients/corrections and parameters</p> Source code in <code>src/score_test/score_test_io.py</code> <pre><code>def load_trait_data(\n    hdf5_path: str, trait_name: str, variant_table: pl.DataFrame\n) -&gt; \"TraitData\":\n    \"\"\"\n    Load trait data and combine with variant data into a TraitData object.\n\n    Args:\n        hdf5_path: Path to HDF5 file containing variant and trait data\n        trait_name: Name of the trait to load\n        variant_table: Pre-loaded variant table DataFrame\n\n    Returns:\n        TraitData object containing variant dataframe with gradients/corrections and parameters\n    \"\"\"\n    # Import at runtime to avoid circular import\n    try:\n        from .score_test import TraitData\n    except ImportError:\n        from score_test import TraitData\n\n    trait_hdf5 = load_trait_hdf5(hdf5_path, trait_name)\n\n    # Determine primary key column\n    possible_keys = [\"RSID\", \"gene_id\", \"gene_name\", \"CHR\", \"POS\"]\n    keys = [key for key in possible_keys if key in variant_table.columns]\n    if len(keys) == 0:\n        raise ValueError(\"variant_table must have one of: \" + \", \".join(possible_keys))\n\n    # Add trait-specific columns (gradient, hessian, etc.)\n    new_columns = []\n    for key_name, value in trait_hdf5.items():\n        if isinstance(value, np.ndarray):\n            decoded_value = _decode_bytes_array(value)\n            new_columns.append(pl.Series(name=key_name, values=decoded_value))\n\n    df = variant_table.with_columns(new_columns)\n\n    if \"parameters\" in trait_hdf5:\n        params = trait_hdf5[\"parameters\"][\"parameters\"]\n        jk_params = trait_hdf5[\"parameters\"][\"jackknife_parameters\"]\n    else:\n        params, jk_params = None, None\n\n    # Create TraitData - it will compute exclude_cols and annot_names via properties\n    trait_data = TraitData(\n        df=df,\n        params=params,\n        jk_params=jk_params,\n        keys=keys,\n    )\n\n    # Compute annotation names using the property\n    trait_data.annot_names = [\n        col for col in df.columns if col not in trait_data.exclude_cols\n    ]\n\n    return trait_data\n</code></pre>"},{"location":"api/score_test/#score_test.score_test_io.load_annotations","title":"load_annotations","text":"<pre><code>load_annotations(annot_path: str, chromosome: Optional[int] = None, infer_schema_length: int = 100000, add_positions: bool = True) -&gt; pl.DataFrame\n</code></pre> <p>Load annotation data for specified chromosome(s).</p> <p>Parameters:</p> Name Type Description Default <code>annot_path</code> <code>str</code> <p>Path to directory containing annotation files</p> required <code>chromosome</code> <code>Optional[int]</code> <p>Specific chromosome number, or None for all chromosomes</p> <code>None</code> <code>infer_schema_length</code> <code>int</code> <p>Number of rows to infer schema from</p> <code>100000</code> <code>add_positions</code> <code>bool</code> <p>If True, rename BP to POS</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing annotations</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no matching annotation files are found</p> Source code in <code>src/score_test/score_test_io.py</code> <pre><code>def load_annotations(\n    annot_path: str,\n    chromosome: Optional[int] = None,\n    infer_schema_length: int = 100_000,\n    add_positions: bool = True,\n) -&gt; pl.DataFrame:\n    \"\"\"Load annotation data for specified chromosome(s).\n\n    Args:\n        annot_path: Path to directory containing annotation files\n        chromosome: Specific chromosome number, or None for all chromosomes\n        infer_schema_length: Number of rows to infer schema from\n        add_positions: If True, rename BP to POS\n\n    Returns:\n        DataFrame containing annotations\n\n    Raises:\n        ValueError: If no matching annotation files are found\n    \"\"\"\n    # Determine which chromosomes to process\n    if chromosome is not None:\n        chromosomes = [chromosome]\n    else:\n        chromosomes = range(1, 23)  # Assuming chromosomes 1-22\n\n    # Find matching files\n    annotations = []\n    for chromosome in chromosomes:\n        file_pattern = f\"*.{chromosome}.annot\"\n        matching_files = Path(annot_path).glob(file_pattern)\n\n        # Read all matching files for this chromosome\n        dfs = []\n        cols_found = set()\n        for file_path in matching_files:\n            df = pl.scan_csv(\n                file_path, separator=\"\\t\", infer_schema_length=infer_schema_length\n            )\n            df = df.select([col for col in df.columns if col not in cols_found])\n            cols_found.update(df.columns)\n            dfs.append(df)\n\n        # Horizontally concatenate all dataframes for this chromosome\n        if dfs:\n            combined_df = pl.concat(dfs, how=\"horizontal\")\n            combined_df = combined_df.select(sorted(combined_df.columns))\n            annotations.append(combined_df)\n\n    # Check if any files were found\n    if not annotations:\n        raise ValueError(\n            f\"No annotation files found in {annot_path} matching pattern *.{{chrom}}.annot\"\n        )\n\n    # Concatenate all chromosome dataframes vertically\n    annotations = pl.concat(annotations, how=\"vertical\").collect()\n\n    # Convert binary columns to boolean to save memory\n    numeric_types = (pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.Float32, pl.Float64)\n    binary_cols = []\n    for col in annotations.columns:\n        # Skip non-numeric columns\n        if not isinstance(annotations[col].dtype, numeric_types):\n            continue\n        # Check if column only contains 0, 1 and null values\n        unique_vals = set(annotations[col].unique().drop_nulls())\n        if unique_vals == {0, 1}:\n            binary_cols.append(col)\n\n    # Convert binary columns to boolean\n    if binary_cols:\n        bool_exprs = [pl.col(col).cast(pl.Boolean) for col in binary_cols]\n        annotations = annotations.with_columns(bool_exprs)\n\n    if add_positions:\n        annotations = annotations.rename({\"BP\": \"POS\"})\n\n    return annotations\n</code></pre>"},{"location":"api/score_test/#score_test.score_test_io.load_gene_table","title":"load_gene_table","text":"<pre><code>load_gene_table(gene_table_path: str, chromosomes: list[int] | None = None) -&gt; pl.DataFrame\n</code></pre> <p>Load gene table and optionally filter to specific chromosomes.</p> <p>Parameters:</p> Name Type Description Default <code>gene_table_path</code> <code>str</code> <p>Path to gene table TSV</p> required <code>chromosomes</code> <code>list[int] | None</code> <p>Optional list of chromosome numbers to filter to</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Gene table DataFrame</p> Source code in <code>src/score_test/score_test_io.py</code> <pre><code>def load_gene_table(\n    gene_table_path: str, chromosomes: list[int] | None = None\n) -&gt; pl.DataFrame:\n    \"\"\"Load gene table and optionally filter to specific chromosomes.\n\n    Args:\n        gene_table_path: Path to gene table TSV\n        chromosomes: Optional list of chromosome numbers to filter to\n\n    Returns:\n        Gene table DataFrame\n    \"\"\"\n    schema = {\n        \"gene_id\": pl.Utf8,\n        \"gene_id_version\": pl.Utf8,\n        \"gene_name\": pl.Utf8,\n        \"start\": pl.Int64,\n        \"end\": pl.Int64,\n        \"CHR\": pl.Utf8,\n    }\n    gene_table = (\n        pl.scan_csv(gene_table_path, schema=schema, separator=\"\\t\", has_header=True)\n        .filter(pl.col(\"CHR\").is_in([str(i) for i in range(1, 23)]))\n        .filter(pl.col(\"gene_id\").is_not_null())\n        .with_columns(pl.col(\"gene_name\").fill_null(\"NA\"))\n        .with_columns(((pl.col(\"start\") + pl.col(\"end\")) / 2).alias(\"midpoint\"))\n        .with_columns(pl.col(\"midpoint\").alias(\"POS\"))\n        .sort(pl.col(\"CHR\").cast(pl.Int64), \"midpoint\")\n        .collect()\n    )\n\n    if chromosomes:\n        # Convert chromosomes to integers if they're strings\n        if isinstance(chromosomes[0], str):\n            chromosomes = [int(c) for c in chromosomes if c.isdigit()]\n        gene_table = gene_table.filter(pl.col(\"CHR\").cast(pl.Int64).is_in(chromosomes))\n\n    # Add POS column (using midpoint) for compatibility with position-based functions\n    gene_table = gene_table.with_columns(pl.col(\"midpoint\").cast(pl.Int64).alias(\"POS\"))\n\n    return gene_table\n</code></pre>"},{"location":"api/score_test/#score_test.score_test_io.load_gene_sets_from_gmt","title":"load_gene_sets_from_gmt","text":"<pre><code>load_gene_sets_from_gmt(gene_annot_dir: str) -&gt; dict[str, list[str]]\n</code></pre> <p>Load gene sets from GMT files in a directory.</p> <p>GMT format: set_namedescriptiongene1gene2... <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>Dictionary mapping set names to lists of genes</p> Source code in <code>src/score_test/score_test_io.py</code> <pre><code>def load_gene_sets_from_gmt(gene_annot_dir: str) -&gt; dict[str, list[str]]:\n    \"\"\"Load gene sets from GMT files in a directory.\n\n    GMT format: set_name&lt;tab&gt;description&lt;tab&gt;gene1&lt;tab&gt;gene2&lt;tab&gt;...\n\n    Returns:\n        Dictionary mapping set names to lists of genes\n    \"\"\"\n    import glob\n    from pathlib import Path\n\n    gmt_files = glob.glob(str(Path(gene_annot_dir) / \"*.gmt\"))\n    if not gmt_files:\n        raise FileNotFoundError(f\"No .gmt files found in {gene_annot_dir}\")\n\n    gene_sets = {}\n    for gmt_file in gmt_files:\n        with open(gmt_file, \"r\") as f:\n            for line in f:\n                parts = line.strip().split(\"\\t\")\n                if len(parts) &gt;= 3:\n                    set_name = parts[0]\n                    genes = parts[2:]  # Skip description\n                    gene_sets[set_name] = genes\n\n    return gene_sets\n</code></pre>"},{"location":"api/score_test/#score_test.score_test_io.load_variant_annotations","title":"load_variant_annotations","text":"<pre><code>load_variant_annotations(annot_dir: str, annot_names: list[str] | None = None) -&gt; VariantAnnot\n</code></pre> <p>Load variant-level annotations from directory.</p> <p>Parameters:</p> Name Type Description Default <code>annot_dir</code> <code>str</code> <p>Directory containing annotation files</p> required <code>annot_names</code> <code>list[str] | None</code> <p>Optional list of specific annotation names to load</p> <code>None</code> <p>Returns:</p> Type Description <code>VariantAnnot</code> <p>VariantAnnot object</p> Source code in <code>src/score_test/score_test_io.py</code> <pre><code>def load_variant_annotations(\n    annot_dir: str, annot_names: list[str] | None = None\n) -&gt; \"VariantAnnot\":\n    \"\"\"Load variant-level annotations from directory.\n\n    Args:\n        annot_dir: Directory containing annotation files\n        annot_names: Optional list of specific annotation names to load\n\n    Returns:\n        VariantAnnot object\n    \"\"\"\n    # Import at runtime to avoid circular import\n    try:\n        from .score_test import VariantAnnot\n    except ImportError:\n        from score_test import VariantAnnot\n\n    df_annot = load_annotations(annot_dir, add_positions=False)\n\n    # Rename SNP to RSID for consistency\n    if \"SNP\" in df_annot.columns:\n        df_annot = df_annot.rename({\"SNP\": \"RSID\"})\n\n    # Exclude positional and identifier columns from annotations\n    exclude_cols = [\"CHR\", \"BP\", \"POS\", \"RSID\", \"CM\"]\n\n    if annot_names:\n        available = [col for col in df_annot.columns if col not in exclude_cols]\n        annot_names = [name for name in annot_names if name in available]\n    else:\n        annot_names = [col for col in df_annot.columns if col not in exclude_cols]\n\n    return VariantAnnot(df_annot, annot_names)\n</code></pre>"},{"location":"api/score_test/#score_test.score_test_io.load_gene_annotations","title":"load_gene_annotations","text":"<pre><code>load_gene_annotations(gene_annot_dir: str, variant_table: DataFrame, gene_table_path: str, nearest_weights: ndarray, annot_names: list[str] | None = None) -&gt; GeneAnnot\n</code></pre> <p>Load gene-level annotations as a GeneAnnot object.</p> <p>Parameters:</p> Name Type Description Default <code>gene_annot_dir</code> <code>str</code> <p>Directory containing GMT files with gene sets</p> required <code>variant_table</code> <code>DataFrame</code> <p>Variant table DataFrame (used to determine chromosomes)</p> required <code>gene_table_path</code> <code>str</code> <p>Path to gene table TSV file</p> required <code>nearest_weights</code> <code>ndarray</code> <p>Weights for k-nearest genes</p> required <code>annot_names</code> <code>list[str] | None</code> <p>Optional list of specific annotation names to load</p> <code>None</code> <p>Returns:</p> Type Description <code>GeneAnnot</code> <p>GeneAnnot object</p> Source code in <code>src/score_test/score_test_io.py</code> <pre><code>def load_gene_annotations(\n    gene_annot_dir: str,\n    variant_table: pl.DataFrame,\n    gene_table_path: str,\n    nearest_weights: np.ndarray,\n    annot_names: list[str] | None = None,\n) -&gt; \"GeneAnnot\":\n    \"\"\"Load gene-level annotations as a GeneAnnot object.\n\n    Args:\n        gene_annot_dir: Directory containing GMT files with gene sets\n        variant_table: Variant table DataFrame (used to determine chromosomes)\n        gene_table_path: Path to gene table TSV file\n        nearest_weights: Weights for k-nearest genes\n        annot_names: Optional list of specific annotation names to load\n\n    Returns:\n        GeneAnnot object\n    \"\"\"\n    # Import at runtime to avoid circular import\n    try:\n        from .score_test import GeneAnnot\n    except ImportError:\n        from score_test import GeneAnnot\n\n    chromosomes = variant_table[\"CHR\"].unique().sort().to_list()\n    _gene_table = load_gene_table(gene_table_path, chromosomes)  # TODO: unused, may need review\n    gene_sets = load_gene_sets_from_gmt(gene_annot_dir)\n\n    if annot_names:\n        gene_sets = {\n            name: genes for name, genes in gene_sets.items() if name in annot_names\n        }\n\n    return GeneAnnot(gene_sets)\n</code></pre>"},{"location":"api/score_test/#score_test.score_test_io.create_random_gene_annotations","title":"create_random_gene_annotations","text":"<pre><code>create_random_gene_annotations(data_table: DataFrame, gene_table_path: str, probs: List[float]) -&gt; GeneAnnot\n</code></pre> <p>Create random gene-level annotations as a GeneAnnot object.</p> <p>Parameters:</p> Name Type Description Default <code>data_table</code> <code>DataFrame</code> <p>Data table DataFrame (variants or genes)</p> required <code>gene_table_path</code> <code>str</code> <p>Path to gene table TSV file</p> required <code>probs</code> <code>List[float]</code> <p>List of probabilities for random gene selection</p> required <p>Returns:</p> Type Description <code>GeneAnnot</code> <p>GeneAnnot object with random gene sets</p> Source code in <code>src/score_test/score_test_io.py</code> <pre><code>def create_random_gene_annotations(\n    data_table: pl.DataFrame,\n    gene_table_path: str,\n    probs: List[float],\n) -&gt; \"GeneAnnot\":\n    \"\"\"Create random gene-level annotations as a GeneAnnot object.\n\n    Args:\n        data_table: Data table DataFrame (variants or genes)\n        gene_table_path: Path to gene table TSV file\n        probs: List of probabilities for random gene selection\n\n    Returns:\n        GeneAnnot object with random gene sets\n    \"\"\"\n    # Import at runtime to avoid circular import\n    try:\n        from .score_test import GeneAnnot\n    except ImportError:\n        from score_test import GeneAnnot\n\n    # Check if this is gene-level data (has gene_id column)\n    if \"gene_id\" in data_table.columns:\n        # For gene-level data, use genes directly from the data table\n        gene_names = data_table[\"gene_name\"].to_list()\n    else:\n        # For variant-level data, load genes from file\n        chromosomes = data_table[\"CHR\"].unique().sort().to_list()\n        gene_table = load_gene_table(gene_table_path, chromosomes)\n        gene_names = gene_table[\"gene_name\"].to_list()\n\n    # Create random gene sets\n    gene_sets = {}\n    for i, p in enumerate(probs):\n        set_name = f\"random_gene_{i}\"\n        n_genes = int(len(gene_names) * p)\n        gene_sets[set_name] = list(\n            np.random.choice(gene_names, size=n_genes, replace=False)\n        )\n\n    return GeneAnnot(gene_sets)\n</code></pre>"},{"location":"api/score_test/#score_test.score_test_io.create_random_variant_annotations","title":"create_random_variant_annotations","text":"<pre><code>create_random_variant_annotations(variant_table: DataFrame, probs: List[float]) -&gt; VariantAnnot\n</code></pre> <p>Create random variant-level annotations as a VariantAnnot object.</p> <p>Parameters:</p> Name Type Description Default <code>variant_table</code> <code>DataFrame</code> <p>Variant table DataFrame</p> required <code>probs</code> <code>List[float]</code> <p>List of probabilities for random annotation</p> required <p>Returns:</p> Type Description <code>VariantAnnot</code> <p>VariantAnnot object with random variant annotations</p> Source code in <code>src/score_test/score_test_io.py</code> <pre><code>def create_random_variant_annotations(\n    variant_table: pl.DataFrame,\n    probs: List[float],\n) -&gt; \"VariantAnnot\":\n    \"\"\"Create random variant-level annotations as a VariantAnnot object.\n\n    Args:\n        variant_table: Variant table DataFrame\n        probs: List of probabilities for random annotation\n\n    Returns:\n        VariantAnnot object with random variant annotations\n    \"\"\"\n    # Import at runtime to avoid circular import\n    try:\n        from .score_test import VariantAnnot\n    except ImportError:\n        from score_test import VariantAnnot\n\n    num_variants = len(variant_table)\n\n    # Create random annotations\n    variant_annots = {}\n    annot_names = []\n\n    for i, p in enumerate(probs):\n        col_name = f\"random_variant_{i}\"\n        annot_names.append(col_name)\n        variant_annots[col_name] = np.random.binomial(1, p, size=num_variants).astype(\n            np.float64\n        )\n\n    # Create output DataFrame\n    # Use RSID for variants\n    if \"RSID\" not in variant_table.columns:\n        raise ValueError(\n            \"variant_table must have 'RSID' column for variant annotations\"\n        )\n\n    df_annot = pl.DataFrame(\n        {\n            \"CHR\": variant_table[\"CHR\"],\n            \"BP\": variant_table[\"POS\"],\n            \"RSID\": variant_table[\"RSID\"],\n            \"CM\": pl.Series([0.0] * len(variant_table)),\n            **variant_annots,\n        }\n    )\n\n    return VariantAnnot(df_annot, annot_names)\n</code></pre>"},{"location":"api/score_test/#score_test.score_test_io.get_trait_groups","title":"get_trait_groups","text":"<pre><code>get_trait_groups(hdf5_path: str) -&gt; dict[str, list[str]]\n</code></pre> <p>Get trait groups for meta-analysis from HDF5 file.</p> <p>Parameters:</p> Name Type Description Default <code>hdf5_path</code> <code>str</code> <p>Path to HDF5 file containing trait groups</p> required <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>Dictionary mapping group names to lists of trait names.</p> <code>dict[str, list[str]]</code> <p>Returns empty dict if no groups are defined.</p> Source code in <code>src/score_test/score_test_io.py</code> <pre><code>def get_trait_groups(hdf5_path: str) -&gt; dict[str, list[str]]:\n    \"\"\"Get trait groups for meta-analysis from HDF5 file.\n\n    Args:\n        hdf5_path: Path to HDF5 file containing trait groups\n\n    Returns:\n        Dictionary mapping group names to lists of trait names.\n        Returns empty dict if no groups are defined.\n    \"\"\"\n    with h5py.File(hdf5_path, \"r\") as f:\n        if \"groups\" not in f:\n            return {}\n\n        groups = {}\n        groups_group = f[\"groups\"]\n        for group_name in groups_group.keys():\n            trait_list = groups_group[group_name][:]\n            # Convert bytes to strings if necessary\n            if trait_list.dtype.kind == \"S\":\n                trait_list = [t.decode(\"utf-8\") for t in trait_list]\n            else:\n                trait_list = trait_list.tolist()\n            groups[group_name] = trait_list\n\n        return groups\n</code></pre>"},{"location":"api/score_test/#score_test.score_test_io.save_trait_groups","title":"save_trait_groups","text":"<pre><code>save_trait_groups(hdf5_path: str, groups: dict[str, list[str]]) -&gt; None\n</code></pre> <p>Save trait groups for meta-analysis to HDF5 file.</p> <p>Parameters:</p> Name Type Description Default <code>hdf5_path</code> <code>str</code> <p>Path to HDF5 file to create/update</p> required <code>groups</code> <code>dict[str, list[str]]</code> <p>Dictionary mapping group names to lists of trait names</p> required Source code in <code>src/score_test/score_test_io.py</code> <pre><code>def save_trait_groups(hdf5_path: str, groups: dict[str, list[str]]) -&gt; None:\n    \"\"\"Save trait groups for meta-analysis to HDF5 file.\n\n    Args:\n        hdf5_path: Path to HDF5 file to create/update\n        groups: Dictionary mapping group names to lists of trait names\n    \"\"\"\n    with h5py.File(hdf5_path, \"a\") as f:\n        if \"groups\" in f:\n            del f[\"groups\"]\n\n        groups_group = f.create_group(\"groups\")\n\n        for group_name, trait_list in groups.items():\n            # Convert strings to bytes for HDF5 storage\n            trait_array = np.array(trait_list, dtype=\"S\")\n            groups_group.create_dataset(group_name, data=trait_array)\n</code></pre>"},{"location":"api/score_test/#score_test.score_test_io.save_trait_data","title":"save_trait_data","text":"<pre><code>save_trait_data(trait_data: TraitData, hdf5_path: str, trait_name: str) -&gt; None\n</code></pre> <p>Save trait data to HDF5 file in new format.</p> <p>Parameters:</p> Name Type Description Default <code>trait_data</code> <code>TraitData</code> <p>TraitData object to save</p> required <code>hdf5_path</code> <code>str</code> <p>Path to HDF5 file to create/update</p> required <code>trait_name</code> <code>str</code> <p>Name of the trait</p> required Source code in <code>src/score_test/score_test_io.py</code> <pre><code>def save_trait_data(\n    trait_data: \"TraitData\",\n    hdf5_path: str,\n    trait_name: str,\n) -&gt; None:\n    \"\"\"Save trait data to HDF5 file in new format.\n\n    Args:\n        trait_data: TraitData object to save\n        hdf5_path: Path to HDF5 file to create/update\n        trait_name: Name of the trait\n    \"\"\"\n    with h5py.File(hdf5_path, \"a\") as f:\n        # Create metadata attribute if it doesn't exist\n        if \"metadata\" not in f.attrs:\n            f.attrs[\"metadata\"] = \"\"\n\n        # Set data_type and keys attributes\n        if \"gene_id\" in trait_data.df.columns:\n            f.attrs[\"data_type\"] = \"gene\"\n            f.attrs[\"keys\"] = [\"gene_id\", \"gene_name\"]\n        else:\n            f.attrs[\"data_type\"] = \"variant\"\n            f.attrs[\"keys\"] = [\"RSID\", \"POS\"]\n\n        # Create row_data group if it doesn't exist, or recreate if size mismatch\n        if \"row_data\" in f:\n            existing_size = len(f[\"row_data\"][list(f[\"row_data\"].keys())[0]])\n            expected_size = len(trait_data.df)\n            if existing_size != expected_size:\n                # Size mismatch - delete and recreate\n                del f[\"row_data\"]\n\n        if \"row_data\" not in f:\n            data_group = f.create_group(\"row_data\")\n\n            # Save all columns except trait-specific ones (gradient, hessian, etc.)\n            # Use TraitData's exclude_cols property\n            row_data_cols = set(trait_data.df.columns) - trait_data.exclude_cols\n            # Add back standard columns that should be in row_data\n            row_data_cols.update({\"CHR\", \"POS\", \"jackknife_blocks\"})\n            if \"RSID\" in trait_data.df.columns:\n                row_data_cols.add(\"RSID\")\n            if \"gene_id\" in trait_data.df.columns:\n                row_data_cols.update({\"gene_id\", \"gene_name\"})\n\n            for col in trait_data.df.columns:\n                if col in row_data_cols:\n                    data = trait_data.df[col].to_numpy()\n                    # Handle string columns\n                    if data.dtype == object:\n                        data = data.astype(\"S\")\n                    data_group.create_dataset(col, data=data)\n\n        # Create traits group if it doesn't exist\n        if \"traits\" not in f:\n            f.create_group(\"traits\")\n\n        trait_path = f\"traits/{trait_name}\"\n        if trait_path in f:\n            del f[trait_path]\n\n        trait_group = f.create_group(trait_path)\n\n        # Save parameters in parameters/ subgroup\n        if trait_data.params is not None or trait_data.jk_params is not None:\n            params_group = trait_group.create_group(\"parameters\")\n            if trait_data.params is not None:\n                params_group.create_dataset(\"parameters\", data=trait_data.params)\n            if trait_data.jk_params is not None:\n                params_group.create_dataset(\n                    \"jackknife_parameters\", data=trait_data.jk_params\n                )\n\n        # Save gradient (required)\n        trait_group.create_dataset(\n            \"gradient\", data=trait_data.df[\"gradient\"].to_numpy()\n        )\n\n        # Save any other trait-specific datasets (e.g., hessian)\n        standard_cols = {\n            \"CHR\",\n            \"POS\",\n            \"jackknife_blocks\",\n            \"RSID\",\n            \"gene_id\",\n            \"gene_name\",\n            \"gradient\",\n        }\n        annot_cols = set(trait_data.annot_names) if trait_data.annot_names else set()\n        for col in trait_data.df.columns:\n            if col not in standard_cols and col not in annot_cols:\n                trait_group.create_dataset(col, data=trait_data.df[col].to_numpy())\n</code></pre>"},{"location":"api/simulate/","title":"graphld.simulate","text":"<p>Simulation of GWAS summary statistics.</p> <p>Summary statistics can be simulated from their asymptotic distribution without individual-level genotype data. Effect sizes are drawn from a flexible mixture distribution, with support for annotation-dependent and frequency-dependent architectures.</p>"},{"location":"api/simulate/#graphld.simulate","title":"simulate","text":"<p>Simulate GWAS summary statistics.</p>"},{"location":"api/simulate/#graphld.simulate.Simulate","title":"Simulate  <code>dataclass</code>","text":"<pre><code>Simulate(sample_size: int, heritability: float = 0.5, component_variance: Union[ndarray, List[float]] = None, component_weight: Union[ndarray, List[float]] = None, alpha_param: float = -1, annotation_dependent_polygenicity: bool = False, link_fn: Callable[[ndarray], ndarray] = _default_link_fn, random_seed: Optional[int] = None, annotation_columns: Optional[List[str]] = None)\n</code></pre> <p>               Bases: <code>ParallelProcessor</code>, <code>_SimulationSpecification</code></p> <p>Parallel processor for simulating GWAS summary statistics.</p>"},{"location":"api/simulate/#graphld.simulate.Simulate.create_shared_memory","title":"create_shared_memory  <code>staticmethod</code>","text":"<pre><code>create_shared_memory(metadata: DataFrame, block_data: list[tuple], **kwargs) -&gt; SharedData\n</code></pre> <p>Create shared memory arrays for simulation.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>DataFrame</code> <p>Metadata DataFrame containing block information</p> required <code>block_data</code> <code>list[tuple]</code> <p>List of tuples containing block-specific annotation DataFrames</p> required <code>**kwargs</code> <p>Additional keyword arguments</p> <code>{}</code> Source code in <code>src/graphld/simulate.py</code> <pre><code>@staticmethod\ndef create_shared_memory(\n    metadata: pl.DataFrame,\n    block_data: list[tuple],\n    **kwargs\n) -&gt; SharedData:\n    \"\"\"Create shared memory arrays for simulation.\n\n    Args:\n        metadata: Metadata DataFrame containing block information\n        block_data: List of tuples containing block-specific annotation DataFrames\n        **kwargs: Additional keyword arguments\n    \"\"\"\n    # Get total number of variants and indices\n    num_variants = np.array([len(df) for df, _ in block_data])\n    total_variants = int(sum(num_variants))  # Convert to Python int\n\n    # Create shared arrays sized according to metadata\n    shared = SharedData({\n        'beta': total_variants,    # Causal effect sizes (one per index)\n        'alpha': total_variants,   # Marginal effect sizes (one per index)\n        'h2': total_variants,      # Per-variant heritability\n        'noise': total_variants,   # Noise component\n        'scale_param': 1,         # Single value\n    })\n\n    # Initialize arrays with zeros\n    shared['beta'][:] = 0\n    shared['alpha'][:] = 0\n    shared['noise'][:] = 0\n    shared['h2'][:] = 0\n    shared['scale_param'][:] = 0\n\n    return shared\n</code></pre>"},{"location":"api/simulate/#graphld.simulate.Simulate.prepare_block_data","title":"prepare_block_data  <code>classmethod</code>","text":"<pre><code>prepare_block_data(metadata: DataFrame, **kwargs) -&gt; list[tuple]\n</code></pre> <p>Prepare block-specific data for processing.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>DataFrame</code> <p>DataFrame containing LDGM metadata</p> required <code>**kwargs</code> <p>Additional arguments from run(), including: annotations: Optional DataFrame containing variant annotations</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[tuple]</code> <p>List of block-specific annotation DataFrames, or None if no annotations</p> Source code in <code>src/graphld/simulate.py</code> <pre><code>@classmethod\ndef prepare_block_data(cls, metadata: pl.DataFrame, **kwargs) -&gt; list[tuple]:\n    \"\"\"Prepare block-specific data for processing.\n\n    Args:\n        metadata: DataFrame containing LDGM metadata\n        **kwargs: Additional arguments from run(), including:\n            annotations: Optional DataFrame containing variant annotations\n\n    Returns:\n        List of block-specific annotation DataFrames, or None if no annotations\n    \"\"\"\n    annotations = kwargs.get('annotations')\n    if annotations is None:\n        block_annotations = _create_block_annotations(\n            metadata,\n            kwargs.get('ldgm_metadata_path_duplicate')\n        )\n    else:\n        block_annotations = partition_variants(metadata, annotations)\n\n    cumulative_num_variants = np.cumsum(np.array([len(df) for df in block_annotations]))\n    cumulative_num_variants = [0] + list(cumulative_num_variants[:-1])\n\n    return list(zip(block_annotations, cumulative_num_variants, strict=False))\n</code></pre>"},{"location":"api/simulate/#graphld.simulate.Simulate.process_block","title":"process_block  <code>classmethod</code>","text":"<pre><code>process_block(ldgm: PrecisionOperator, flag: Value, shared_data: SharedData, block_offset: int, block_data: Optional[tuple] = None, worker_params: Optional[Dict] = None) -&gt; None\n</code></pre> <p>Process a single block.</p> Source code in <code>src/graphld/simulate.py</code> <pre><code>@classmethod\ndef process_block(cls, ldgm: PrecisionOperator, flag: Value,\n                 shared_data: SharedData, block_offset: int,\n                 block_data: Optional[tuple] = None,\n                 worker_params: Optional[Dict] = None) -&gt; None:\n    \"\"\"Process a single block.\"\"\"\n    # If we have block_data, merge it with LDGM variant info\n    if block_data is not None:\n        assert isinstance(block_data, tuple), \"block_data must be a tuple\"\n        annotations, variant_offset = block_data\n        num_variants = len(annotations)\n\n        # Merge annotations with LDGM variant info and get indices of merged variants\n        from .io import merge_snplists\n        ldgm, sumstat_indices = merge_snplists(\n            ldgm, annotations,\n            match_by_position=True,\n            pos_col='BP',\n            ref_allele_col='REF',\n            alt_allele_col='ALT'\n        )\n    else:\n        variant_offset = block_offset\n        num_variants = ldgm.shape[0]\n        sumstat_indices = range(num_variants)\n\n    # Get block slice using the number of indices\n    block_slice = slice(variant_offset, variant_offset + num_variants)\n\n    # Simulate effect sizes using the merged data\n    beta, alpha = _simulate_beta_block(ldgm, worker_params)\n\n    block_random_seed = None if worker_params.random_seed is None \\\n        else worker_params.random_seed + variant_offset\n    noise = _simulate_noise_block(ldgm, random_seed=block_random_seed)\n\n    # Create zero-filled arrays for all variants in sumstats\n    beta_reshaped = np.zeros((num_variants, 1))\n    alpha_reshaped = np.zeros((num_variants, 1))\n    noise_reshaped = np.zeros((num_variants, 1))\n\n    # Fill in values for successfully merged variants\n    beta_reshaped[sumstat_indices, 0] = beta.flatten()\n    alpha_reshaped[sumstat_indices, 0] = alpha.flatten()\n    noise_reshaped[sumstat_indices, 0] = noise.flatten()\n\n    # Update the shared memory arrays\n    block_slice = slice(variant_offset, variant_offset + num_variants)\n    shared_data['beta', block_slice] = beta_reshaped\n    shared_data['alpha', block_slice] = alpha_reshaped\n    shared_data['noise', block_slice] = noise_reshaped\n</code></pre>"},{"location":"api/simulate/#graphld.simulate.Simulate.supervise","title":"supervise  <code>classmethod</code>","text":"<pre><code>supervise(manager: WorkerManager, shared_data: Dict[str, Any], block_data: list, **kwargs) -&gt; pl.DataFrame\n</code></pre> <p>Supervise worker processes and collect results.</p> <p>Parameters:</p> Name Type Description Default <code>manager</code> <code>WorkerManager</code> <p>Worker manager</p> required <code>shared_data</code> <code>Dict[str, Any]</code> <p>Dictionary of shared memory arrays</p> required <code>**kwargs</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing simulated summary statistics</p> Source code in <code>src/graphld/simulate.py</code> <pre><code>@classmethod\ndef supervise(\n    cls,\n    manager: WorkerManager,\n    shared_data: Dict[str, Any],\n    block_data: list,\n    **kwargs\n) -&gt; pl.DataFrame:\n    \"\"\"Supervise worker processes and collect results.\n\n    Args:\n        manager: Worker manager\n        shared_data: Dictionary of shared memory arrays\n        **kwargs: Additional arguments\n\n    Returns:\n        DataFrame containing simulated summary statistics\n    \"\"\"\n    manager.start_workers()\n    manager.await_workers()\n    beta, alpha, noise = shared_data['beta'], shared_data['alpha'], shared_data['noise']\n\n    # Compute scaling parameter to achieve desired heritability\n    spec = kwargs['spec']\n    h2 = spec.heritability\n    current_h2 = np.dot(beta,alpha)\n    assert current_h2 &gt;= 0, \"beta'*R*beta should be non-negative\"\n    scaling_param = np.sqrt(h2 / current_h2) if current_h2 &gt; 0 else 1\n    beta *= scaling_param\n    alpha *= scaling_param\n\n    # Concatenate block annotations and add simulation results\n    result = pl.concat([\n        df.select(['CHR', 'SNP', 'POS', 'A1', 'A2'])\n        for df, _ in block_data\n    ])\n\n    return result.with_columns([\n        pl.Series('Z', noise + np.sqrt(spec.sample_size) * alpha),\n        pl.Series('beta', beta),\n        pl.Series('beta_marginal', alpha),\n        pl.Series('N', np.full(len(result), spec.sample_size)),\n    ])\n</code></pre>"},{"location":"api/simulate/#graphld.simulate.Simulate.simulate","title":"simulate","text":"<pre><code>simulate(ldgm_metadata_path: str = 'data/ldgms/metadata.csv', populations: Optional[Union[str, List[str]]] = 'EUR', chromosomes: Optional[Union[int, List[int]]] = None, run_in_serial: bool = False, num_processes: Optional[int] = None, annotations: Optional[DataFrame] = None, verbose: bool = False) -&gt; pl.DataFrame\n</code></pre> <p>Simulate genetic data.</p> <p>Parameters:</p> Name Type Description Default <code>ldgm_metadata_path</code> <code>str</code> <p>Path to LDGM metadata file</p> <code>'data/ldgms/metadata.csv'</code> <code>populations</code> <code>Optional[Union[str, List[str]]]</code> <p>Population(s) to filter</p> <code>'EUR'</code> <code>chromosomes</code> <code>Optional[Union[int, List[int]]]</code> <p>Chromosome(s) to filter</p> <code>None</code> <code>run_in_serial</code> <code>bool</code> <p>Whether to run in serial mode</p> <code>False</code> <code>annotations</code> <code>Optional[DataFrame]</code> <p>Optional variant annotations</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Simulated genetic data DataFrame</p> Source code in <code>src/graphld/simulate.py</code> <pre><code>def simulate(\n    self,\n    ldgm_metadata_path: str = 'data/ldgms/metadata.csv',\n    populations: Optional[Union[str, List[str]]] = 'EUR',\n    chromosomes: Optional[Union[int, List[int]]] = None,\n    run_in_serial: bool = False,\n    num_processes: Optional[int] = None,\n    annotations: Optional[pl.DataFrame] = None,\n    verbose: bool = False,\n) -&gt; pl.DataFrame:\n    \"\"\"Simulate genetic data.\n\n    Args:\n        ldgm_metadata_path: Path to LDGM metadata file\n        populations: Population(s) to filter\n        chromosomes: Chromosome(s) to filter\n        run_in_serial: Whether to run in serial mode\n        annotations: Optional variant annotations\n\n    Returns:\n        Simulated genetic data DataFrame\n    \"\"\"\n    # Check if link_fn is picklable when using multiprocessing\n    if not run_in_serial and self.link_fn is not _default_link_fn:\n        import pickle\n        try:\n            pickle.dumps(self.link_fn)\n        except (pickle.PicklingError, AttributeError, TypeError) as e:\n            raise ValueError(\n                f\"link_fn must be picklable to work with multiprocessing. \"\n                f\"Lambda functions and nested functions cannot be pickled. \"\n                f\"Define your link function at module level or use run_in_serial=True. \"\n                f\"Original error: {e}\"\n            ) from e\n\n    run_fn = self.run_serial if run_in_serial else self.run\n    result = run_fn(\n        ldgm_metadata_path=ldgm_metadata_path,\n        populations=populations,\n        chromosomes=chromosomes,\n        worker_params=self,  # Use instance itself as spec\n        spec=self,\n        annotations=annotations,\n        # Pass path to prepare_block_data\n        ldgm_metadata_path_duplicate=ldgm_metadata_path,\n        num_processes=num_processes,\n    )\n\n    if verbose:\n        print(f\"Number of variants in summary statistics: {len(result)}\")\n        nonzero_count = (result['beta'] != 0).sum()\n        print(f\"Number of variants with nonzero beta: {nonzero_count}\")\n\n    return result\n</code></pre>"},{"location":"api/simulate/#graphld.simulate.run_simulate","title":"run_simulate","text":"<pre><code>run_simulate(sample_size: int, heritability: float = 0.5, component_variance: Optional[Union[ndarray, List[float]]] = None, component_weight: Optional[Union[ndarray, List[float]]] = None, alpha_param: float = -1, annotation_dependent_polygenicity: bool = False, link_fn: Callable[[ndarray], ndarray] = _default_link_fn, random_seed: Optional[int] = None, annotation_columns: Optional[List[str]] = None, ldgm_metadata_path: str = 'data/ldgms/metadata.csv', populations: Optional[Union[str, List[str]]] = 'EUR', chromosomes: Optional[Union[int, List[int]]] = None, run_in_serial: bool = False, num_processes: Optional[int] = None, annotations: Optional[DataFrame] = None, verbose: bool = False) -&gt; pl.DataFrame\n</code></pre> <p>Run GWAS summary statistics simulation with specified parameters.</p> <p>Parameters:</p> Name Type Description Default <code>sample_size</code> <code>int</code> <p>Sample size for the population</p> required <code>heritability</code> <code>float</code> <p>Total heritability (h2) for the trait</p> <code>0.5</code> <code>component_variance</code> <code>Optional[Union[ndarray, List[float]]]</code> <p>Per-allele effect size variance for each mixture component</p> <code>None</code> <code>component_weight</code> <code>Optional[Union[ndarray, List[float]]]</code> <p>Mixture weight for each component (must sum to \u2264 1)</p> <code>None</code> <code>alpha_param</code> <code>float</code> <p>Alpha parameter for allele frequency-dependent architecture</p> <code>-1</code> <code>annotation_dependent_polygenicity</code> <code>bool</code> <p>If True, use annotations to modify proportion of causal variants instead of effect size magnitude</p> <code>False</code> <code>link_fn</code> <code>Callable[[ndarray], ndarray]</code> <p>Function mapping annotation vector to relative per-variant heritability. Default is softmax: x -&gt; log(1 + exp(sum(x))). Must be defined at module level (not as lambda or nested function) to work with multiprocessing</p> <code>_default_link_fn</code> <code>random_seed</code> <code>Optional[int]</code> <p>Random seed for reproducibility</p> <code>None</code> <code>annotation_columns</code> <code>Optional[List[str]]</code> <p>List of column names to use as annotations</p> <code>None</code> <code>ldgm_metadata_path</code> <code>str</code> <p>Path to LDGM metadata file</p> <code>'data/ldgms/metadata.csv'</code> <code>populations</code> <code>Optional[Union[str, List[str]]]</code> <p>Population(s) to filter</p> <code>'EUR'</code> <code>chromosomes</code> <code>Optional[Union[int, List[int]]]</code> <p>Chromosome(s) to filter</p> <code>None</code> <code>run_in_serial</code> <code>bool</code> <p>Whether to run in serial mode</p> <code>False</code> <code>num_processes</code> <code>Optional[int]</code> <p>Number of processes for parallel execution</p> <code>None</code> <code>annotations</code> <code>Optional[DataFrame]</code> <p>Optional variant annotations DataFrame</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print progress information</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing simulated summary statistics</p> Source code in <code>src/graphld/simulate.py</code> <pre><code>def run_simulate(\n    sample_size: int,\n    heritability: float = 0.5,\n    component_variance: Optional[Union[np.ndarray, List[float]]] = None,\n    component_weight: Optional[Union[np.ndarray, List[float]]] = None,\n    alpha_param: float = -1,\n    annotation_dependent_polygenicity: bool = False,\n    link_fn: Callable[[np.ndarray], np.ndarray] = _default_link_fn,\n    random_seed: Optional[int] = None,\n    annotation_columns: Optional[List[str]] = None,\n    ldgm_metadata_path: str = 'data/ldgms/metadata.csv',\n    populations: Optional[Union[str, List[str]]] = 'EUR',\n    chromosomes: Optional[Union[int, List[int]]] = None,\n    run_in_serial: bool = False,\n    num_processes: Optional[int] = None,\n    annotations: Optional[pl.DataFrame] = None,\n    verbose: bool = False,\n) -&gt; pl.DataFrame:\n    \"\"\"Run GWAS summary statistics simulation with specified parameters.\n\n    Args:\n        sample_size: Sample size for the population\n        heritability: Total heritability (h2) for the trait\n        component_variance: Per-allele effect size variance for each mixture component\n        component_weight: Mixture weight for each component (must sum to \u2264 1)\n        alpha_param: Alpha parameter for allele frequency-dependent architecture\n        annotation_dependent_polygenicity: If True, use annotations to modify proportion\n            of causal variants instead of effect size magnitude\n        link_fn: Function mapping annotation vector to relative per-variant heritability.\n            Default is softmax: x -&gt; log(1 + exp(sum(x))). Must be defined at module level\n            (not as lambda or nested function) to work with multiprocessing\n        random_seed: Random seed for reproducibility\n        annotation_columns: List of column names to use as annotations\n        ldgm_metadata_path: Path to LDGM metadata file\n        populations: Population(s) to filter\n        chromosomes: Chromosome(s) to filter\n        run_in_serial: Whether to run in serial mode\n        num_processes: Number of processes for parallel execution\n        annotations: Optional variant annotations DataFrame\n        verbose: Whether to print progress information\n\n    Returns:\n        DataFrame containing simulated summary statistics\n    \"\"\"\n    sim = Simulate(\n        sample_size=sample_size,\n        heritability=heritability,\n        component_variance=component_variance,\n        component_weight=component_weight,\n        alpha_param=alpha_param,\n        annotation_dependent_polygenicity=annotation_dependent_polygenicity,\n        link_fn=link_fn,\n        random_seed=random_seed,\n        annotation_columns=annotation_columns,\n    )\n\n    return sim.simulate(\n        ldgm_metadata_path=ldgm_metadata_path,\n        populations=populations,\n        chromosomes=chromosomes,\n        run_in_serial=run_in_serial,\n        num_processes=num_processes,\n        annotations=annotations,\n        verbose=verbose,\n    )\n</code></pre>"},{"location":"api/surrogates/","title":"graphld.surrogates","text":"<p>Surrogate marker assignment for missing variants.</p> <p>When variants are missing from GWAS summary statistics, graphREML can assign surrogate markers in high LD with those variants. This is particularly useful when working with HapMap3 SNPs only (~1.1M SNPs).</p>"},{"location":"api/surrogates/#graphld.surrogates","title":"surrogates","text":"<p>Generate surrogate-marker mapping files for LD graphical models.</p> <p>Each output is an HDF5 file per population.  Within the file there is one HDF5 group per LD block.  Every group stores three 1-D datasets with a common length equal to the number of variants in the LDGM block:</p> <pre><code>variant_id       \u2014 UTF-8 string     (snplist ``site_ids`` column)\nvariant_index    \u2014 int32            (row index inside the LDGM)\nsurrogate_index  \u2014 int32            (row index of best surrogate)\n</code></pre> <p>For variants that are already non-missing in the training sum-stats the surrogate index equals the variant index.</p> <p>The heavy (and slow) search for a surrogate marker therefore needs to be performed only once per population and block; subsequent GraphREML runs can simply look up the stored mapping.</p>"},{"location":"api/surrogates/#graphld.surrogates.Surrogates","title":"Surrogates","text":"<p>               Bases: <code>ParallelProcessor</code></p>"},{"location":"api/surrogates/#graphld.surrogates.Surrogates.prepare_block_data","title":"prepare_block_data  <code>classmethod</code>","text":"<pre><code>prepare_block_data(metadata: DataFrame, **kwargs) -&gt; list[dict]\n</code></pre> <p>Partition dataframe with nonmissing variants among blocks and attach block names.</p> Source code in <code>src/graphld/surrogates.py</code> <pre><code>@classmethod\ndef prepare_block_data(cls, metadata: pl.DataFrame, **kwargs) -&gt; list[dict]:\n    \"\"\"Partition dataframe with nonmissing variants among blocks and attach block names.\"\"\"\n    nonmissing_variant_ids: pl.DataFrame = kwargs.get('nonmissing_variant_ids')\n    dfs = partition_variants(metadata, nonmissing_variant_ids)\n    names = metadata.get_column('name').to_numpy().tolist()\n    return [\n        {'df': df, 'block_name': name}\n        for df, name in zip(dfs, names, strict=False)\n    ]\n</code></pre>"},{"location":"api/surrogates/#graphld.surrogates.Surrogates.create_shared_memory","title":"create_shared_memory  <code>classmethod</code>","text":"<pre><code>create_shared_memory(metadata, block_data, **kwargs) -&gt; SharedData\n</code></pre> <p>No shared memory needed; return empty container for interface compliance.</p> Source code in <code>src/graphld/surrogates.py</code> <pre><code>@classmethod\ndef create_shared_memory(cls, metadata, block_data, **kwargs) -&gt; SharedData:\n    \"\"\"No shared memory needed; return empty container for interface compliance.\"\"\"\n    return SharedData({})\n</code></pre>"},{"location":"api/surrogates/#graphld.surrogates.Surrogates.supervise","title":"supervise  <code>classmethod</code>","text":"<pre><code>supervise(manager: WorkerManager, shared_data: SharedData, block_data: list[dict], **kwargs) -&gt; np.ndarray\n</code></pre> <p>Wait for all workers to finish and return output path.</p> Source code in <code>src/graphld/surrogates.py</code> <pre><code>@classmethod\ndef supervise(\n    cls,\n    manager: WorkerManager,\n    shared_data: SharedData,\n    block_data: list[dict],\n    **kwargs\n) -&gt; np.ndarray:\n    \"\"\"Wait for all workers to finish and return output path.\"\"\"\n    manager.start_workers()\n    manager.await_workers()\n    return kwargs.get('output_path')\n</code></pre>"},{"location":"api/surrogates/#graphld.surrogates.Surrogates.process_block","title":"process_block  <code>classmethod</code>","text":"<pre><code>process_block(ldgm, flag, shared_data: SharedData, block_offset: int, block_data: dict, worker_params: dict)\n</code></pre> <p>Compute surrogate markers and save to file.</p> Source code in <code>src/graphld/surrogates.py</code> <pre><code>@classmethod\ndef process_block(\n    cls,\n    ldgm,\n    flag,\n    shared_data: SharedData,\n    block_offset: int,\n    block_data: dict,\n    worker_params: dict,\n):\n    \"\"\"Compute surrogate markers and save to file.\"\"\"\n    # Use merge_snplists to match the merge logic in heritability.py\n    # This ensures allele matching is applied consistently\n    sumstats_df = block_data['df']\n    match_by_position = worker_params.get('match_by_position', False)\n\n    # Don't modify ldgm in place - we need to preserve original indices\n    merged_ldgm, _ = merge_snplists(\n        ldgm, sumstats_df,\n        match_by_position=match_by_position,\n        pos_col='POS',\n        ref_allele_col='REF',\n        alt_allele_col='ALT',\n        modify_in_place=False\n    )\n\n    # After merge_snplists, merged_ldgm.variant_info contains only variants that\n    # matched sumstats (with allele checking). Get their unique indices.\n    # Work at index-level: length equals number of unique LDGM indices in this block\n    num_indices = ldgm.shape[0]\n    mapping = np.arange(num_indices, dtype=np.int32)\n\n    # Candidates are unique indices among non-missing rows (those that survived the merge)\n    candidates = (\n        merged_ldgm.variant_info\n          .select('index')\n          .unique()\n          .with_row_index(name='surrogate_nr')\n    )\n\n    if len(candidates) &gt; 0:\n        missing_indices = np.setdiff1d(np.arange(num_indices), candidates['index'].to_numpy())\n        for mi in missing_indices:\n            surrogate = _surrogate_marker(ldgm, mi, candidates)\n            mapping[mi] = int(surrogate['index'])\n    else:\n        print(f\"No non-missing variants found in block {block_data['block_name']}. Skipping.\")\n\n    # Persist to HDF5: one dataset per block named by metadata 'name'\n    out_path = worker_params['output_path']\n\n    # Ensure directory exists\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Serialize writes across processes\n    lock = FileLock(str(out_path) + \".lock\")\n    with lock:\n        with h5py.File(out_path, 'a') as h5:\n            dset_name = str(block_data['block_name'])\n            if dset_name in h5:\n                raise ValueError(f\"Dataset {dset_name} already exists in {out_path}\")\n            h5.create_dataset(dset_name, data=mapping, compression='lzf', chunks=True)\n</code></pre>"},{"location":"api/surrogates/#graphld.surrogates.get_surrogate_markers","title":"get_surrogate_markers","text":"<pre><code>get_surrogate_markers(metadata_path: Union[str, PathLike], nonmissing_variant_ids: DataFrame, *, population: str, run_serial: bool = False, num_processes: Optional[int] = None, output_path: Optional[Union[str, PathLike]] = None, chromosomes: Optional[int] = None) -&gt; Path\n</code></pre> <p>Create an HDF5 file with one dataset per LD block containing index-level surrogates.</p> <p>Returns the path to the HDF5 file.</p> Source code in <code>src/graphld/surrogates.py</code> <pre><code>def get_surrogate_markers(\n    metadata_path: Union[str, os.PathLike],\n    nonmissing_variant_ids: pl.DataFrame,\n    *,\n    population: str,\n    run_serial: bool = False,\n    num_processes: Optional[int] = None,\n    output_path: Optional[Union[str, os.PathLike]] = None,\n    chromosomes: Optional[int] = None,\n) -&gt; Path:\n    \"\"\"Create an HDF5 file with one dataset per LD block containing index-level surrogates.\n\n    Returns the path to the HDF5 file.\n    \"\"\"\n    run_fn = Surrogates.run_serial if run_serial else Surrogates.run\n\n    # Default output path: alongside metadata file\n    if output_path is None:\n        output_path = Path(metadata_path).parent / f\"surrogates.{population}.h5\"\n\n    result_path = run_fn(\n        ldgm_metadata_path=metadata_path,\n        populations=population,\n        chromosomes=chromosomes,\n        nonmissing_variant_ids=nonmissing_variant_ids,\n        num_processes=num_processes,\n        worker_params={'output_path': Path(output_path)},\n        output_path=Path(output_path),\n    )\n\n    return result_path\n</code></pre>"},{"location":"api/vcf_io/","title":"graphld.vcf_io","text":"<p>I/O functions for GWAS-VCF format summary statistics.</p>"},{"location":"api/vcf_io/#graphld.vcf_io","title":"vcf_io","text":""},{"location":"api/vcf_io/#graphld.vcf_io.validate_vcf_format_columns","title":"validate_vcf_format_columns","text":"<pre><code>validate_vcf_format_columns(columns: list, verbose: bool = False) -&gt; dict\n</code></pre> <p>Validate and describe VCF FORMAT columns according to GWAS-VCF specification.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>list</code> <p>List of column names from VCF FORMAT</p> required <code>verbose</code> <code>bool</code> <p>If True, print detailed information about columns. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary of validated columns with their descriptions and requirements</p> Source code in <code>src/graphld/vcf_io.py</code> <pre><code>def validate_vcf_format_columns(columns: list, verbose: bool = False) -&gt; dict:\n    \"\"\"\n    Validate and describe VCF FORMAT columns according to GWAS-VCF specification.\n\n    Args:\n        columns (list): List of column names from VCF FORMAT\n        verbose (bool, optional): If True, print detailed information about columns.\n            Defaults to False.\n\n    Returns:\n        dict: Dictionary of validated columns with their descriptions and requirements\n    \"\"\"\n    # Define the VCF FORMAT column specifications\n    vcf_format_spec = {\n        'NS': {\n            'description': 'Variant-specific number of samples/individuals with called '\n                         'genotypes used to test association with specified trait',\n            'required': False\n        },\n        'EZ': {\n            'description': 'Z-score provided if it was used to derive the ES and SE fields',\n            'required': False\n        },\n        'SI': {\n            'description': 'Accuracy score of association statistics imputation',\n            'required': False\n        },\n        'NC': {\n            'description': 'Variant-specific number of cases used to estimate genetic '\n                         'effect (binary traits only)',\n            'required': False\n        },\n        'ES': {\n            'description': 'Effect size estimate relative to the alternative allele',\n            'required': True\n        },\n        'SE': {\n            'description': 'Standard error of effect size estimate',\n            'required': True\n        },\n        'LP': {\n            'description': '-log10 p-value for effect estimate',\n            'required': True\n        },\n        'AF': {\n            'description': 'Alternative allele frequency in trait subset',\n            'required': False\n        },\n        'AC': {\n            'description': 'Alternative allele count in the trait subset',\n            'required': False\n        }\n    }\n\n    # Check for missing required columns\n    missing_required = [\n        col for col, spec in vcf_format_spec.items()\n        if spec['required'] and col not in columns\n    ]\n    if missing_required:\n        raise ValueError(f\"Missing required columns: {missing_required}\")\n\n    # Check for extra columns not in specification\n    extra_columns = [col for col in columns if col not in vcf_format_spec]\n    if extra_columns:\n        raise ValueError(\n            f\"Extra columns not in VCF FORMAT specification: {extra_columns}\"\n        )\n\n    # If verbose, print detailed information\n    if verbose:\n        print(\"GWAS-VCF keys and descriptions:\")\n        for col in columns:\n            spec = vcf_format_spec[col]\n            req_status = 'Required' if spec['required'] else 'Optional'\n            print(f\"- {col}: {spec['description']} ({req_status})\")\n\n    # Return the specification for the found columns\n    return {col: vcf_format_spec[col] for col in columns}\n</code></pre>"},{"location":"api/vcf_io/#graphld.vcf_io.read_gwas_vcf","title":"read_gwas_vcf","text":"<pre><code>read_gwas_vcf(file_path: str, num_rows: Optional[int] = None, maximum_missingness: float = 0.1, verbose: bool = False) -&gt; pl.DataFrame\n</code></pre> <p>Reads a GWAS-VCF file using Polars and returns a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the VCF file.</p> required <code>num_rows</code> <code>Optional[int]</code> <p>Number of rows to read. Defaults to None.</p> <code>None</code> <code>maximum_missingness</code> <code>float</code> <p>Maximum fraction of missing samples allowed. Defaults to 0.1.</p> <code>0.1</code> <code>verbose</code> <code>bool</code> <p>Print detailed information about FORMAT columns. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: DataFrame containing the VCF data.</p> Source code in <code>src/graphld/vcf_io.py</code> <pre><code>def read_gwas_vcf(\n    file_path: str,\n    num_rows: Optional[int] = None,\n    maximum_missingness: float = 0.1,\n    verbose: bool = False\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Reads a GWAS-VCF file using Polars and returns a DataFrame.\n\n    Args:\n        file_path (str): Path to the VCF file.\n        num_rows (Optional[int], optional): Number of rows to read. Defaults to None.\n        maximum_missingness (float, optional): Maximum fraction of missing samples\n            allowed. Defaults to 0.1.\n        verbose (bool, optional): Print detailed information about FORMAT columns.\n            Defaults to False.\n\n    Returns:\n        pl.DataFrame: DataFrame containing the VCF data.\n    \"\"\"\n    # Read the VCF file, skipping the header lines\n    df = pl.read_csv(\n        file_path,\n        separator='\\t',\n        comment_prefix='##',\n        has_header=True,\n        n_rows=num_rows\n    )\n\n    df = split_sample_column(df)\n    df = process_chromosome_column(df)\n\n    # Filter based on missingness using NS (number of samples)\n    if 'NS' in df.columns:\n        min_samples = (1 - maximum_missingness) * pl.col('NS').max()\n        df = df.filter(pl.col('NS') &gt;= min_samples)\n\n    return df\n</code></pre>"}]}